{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BrainTumorDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1TmH26aNqrbqSE25B-FSn9gZ9PON9j1MQ",
      "authorship_tag": "ABX9TyO9avi9XD5rgLxSqxfCMPjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BitanSarkar/BrainTumorDetect/blob/main/BrainTumorDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1sn0CDWPfJ"
      },
      "source": [
        "# Taking data from drive. \n",
        "# Pre-processing of data using sklearn.preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MJCys9IKslQ",
        "outputId": "bc8c6684-c9d6-45b6-ca97-6f00fd0071fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import cv2 as cv \n",
        "from google.colab.patches import cv2_imshow # for image display\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DIRECTORY = r\"/content/drive/My Drive/brain_tumor_dataset\"\n",
        "CATEGORIES = [\"yes\", \"no\"]\n",
        "\n",
        "print(\"[INFO] loading images...\")\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "ctr=0\n",
        "for category in CATEGORIES:\n",
        "  path = os.path.join(DIRECTORY, category)\n",
        "  for img in os.listdir(path):\n",
        "    ctr+=1\n",
        "    img_path = os.path.join(path, img)\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = preprocess_input(image)\n",
        "    data.append(image)\n",
        "    labels.append(category)\n",
        "    print(ctr)\n",
        "    if ctr%500==0:\n",
        "      print(\"Data loaded upto = \" + str(ctr))\n",
        "    if ctr == 3000:\n",
        "      print(\"Data Loading COMPLETED\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "Data loaded upto = 500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "Data loaded upto = 1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "Data loaded upto = 1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "Data loaded upto = 2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "Data loaded upto = 2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n",
            "2670\n",
            "2671\n",
            "2672\n",
            "2673\n",
            "2674\n",
            "2675\n",
            "2676\n",
            "2677\n",
            "2678\n",
            "2679\n",
            "2680\n",
            "2681\n",
            "2682\n",
            "2683\n",
            "2684\n",
            "2685\n",
            "2686\n",
            "2687\n",
            "2688\n",
            "2689\n",
            "2690\n",
            "2691\n",
            "2692\n",
            "2693\n",
            "2694\n",
            "2695\n",
            "2696\n",
            "2697\n",
            "2698\n",
            "2699\n",
            "2700\n",
            "2701\n",
            "2702\n",
            "2703\n",
            "2704\n",
            "2705\n",
            "2706\n",
            "2707\n",
            "2708\n",
            "2709\n",
            "2710\n",
            "2711\n",
            "2712\n",
            "2713\n",
            "2714\n",
            "2715\n",
            "2716\n",
            "2717\n",
            "2718\n",
            "2719\n",
            "2720\n",
            "2721\n",
            "2722\n",
            "2723\n",
            "2724\n",
            "2725\n",
            "2726\n",
            "2727\n",
            "2728\n",
            "2729\n",
            "2730\n",
            "2731\n",
            "2732\n",
            "2733\n",
            "2734\n",
            "2735\n",
            "2736\n",
            "2737\n",
            "2738\n",
            "2739\n",
            "2740\n",
            "2741\n",
            "2742\n",
            "2743\n",
            "2744\n",
            "2745\n",
            "2746\n",
            "2747\n",
            "2748\n",
            "2749\n",
            "2750\n",
            "2751\n",
            "2752\n",
            "2753\n",
            "2754\n",
            "2755\n",
            "2756\n",
            "2757\n",
            "2758\n",
            "2759\n",
            "2760\n",
            "2761\n",
            "2762\n",
            "2763\n",
            "2764\n",
            "2765\n",
            "2766\n",
            "2767\n",
            "2768\n",
            "2769\n",
            "2770\n",
            "2771\n",
            "2772\n",
            "2773\n",
            "2774\n",
            "2775\n",
            "2776\n",
            "2777\n",
            "2778\n",
            "2779\n",
            "2780\n",
            "2781\n",
            "2782\n",
            "2783\n",
            "2784\n",
            "2785\n",
            "2786\n",
            "2787\n",
            "2788\n",
            "2789\n",
            "2790\n",
            "2791\n",
            "2792\n",
            "2793\n",
            "2794\n",
            "2795\n",
            "2796\n",
            "2797\n",
            "2798\n",
            "2799\n",
            "2800\n",
            "2801\n",
            "2802\n",
            "2803\n",
            "2804\n",
            "2805\n",
            "2806\n",
            "2807\n",
            "2808\n",
            "2809\n",
            "2810\n",
            "2811\n",
            "2812\n",
            "2813\n",
            "2814\n",
            "2815\n",
            "2816\n",
            "2817\n",
            "2818\n",
            "2819\n",
            "2820\n",
            "2821\n",
            "2822\n",
            "2823\n",
            "2824\n",
            "2825\n",
            "2826\n",
            "2827\n",
            "2828\n",
            "2829\n",
            "2830\n",
            "2831\n",
            "2832\n",
            "2833\n",
            "2834\n",
            "2835\n",
            "2836\n",
            "2837\n",
            "2838\n",
            "2839\n",
            "2840\n",
            "2841\n",
            "2842\n",
            "2843\n",
            "2844\n",
            "2845\n",
            "2846\n",
            "2847\n",
            "2848\n",
            "2849\n",
            "2850\n",
            "2851\n",
            "2852\n",
            "2853\n",
            "2854\n",
            "2855\n",
            "2856\n",
            "2857\n",
            "2858\n",
            "2859\n",
            "2860\n",
            "2861\n",
            "2862\n",
            "2863\n",
            "2864\n",
            "2865\n",
            "2866\n",
            "2867\n",
            "2868\n",
            "2869\n",
            "2870\n",
            "2871\n",
            "2872\n",
            "2873\n",
            "2874\n",
            "2875\n",
            "2876\n",
            "2877\n",
            "2878\n",
            "2879\n",
            "2880\n",
            "2881\n",
            "2882\n",
            "2883\n",
            "2884\n",
            "2885\n",
            "2886\n",
            "2887\n",
            "2888\n",
            "2889\n",
            "2890\n",
            "2891\n",
            "2892\n",
            "2893\n",
            "2894\n",
            "2895\n",
            "2896\n",
            "2897\n",
            "2898\n",
            "2899\n",
            "2900\n",
            "2901\n",
            "2902\n",
            "2903\n",
            "2904\n",
            "2905\n",
            "2906\n",
            "2907\n",
            "2908\n",
            "2909\n",
            "2910\n",
            "2911\n",
            "2912\n",
            "2913\n",
            "2914\n",
            "2915\n",
            "2916\n",
            "2917\n",
            "2918\n",
            "2919\n",
            "2920\n",
            "2921\n",
            "2922\n",
            "2923\n",
            "2924\n",
            "2925\n",
            "2926\n",
            "2927\n",
            "2928\n",
            "2929\n",
            "2930\n",
            "2931\n",
            "2932\n",
            "2933\n",
            "2934\n",
            "2935\n",
            "2936\n",
            "2937\n",
            "2938\n",
            "2939\n",
            "2940\n",
            "2941\n",
            "2942\n",
            "2943\n",
            "2944\n",
            "2945\n",
            "2946\n",
            "2947\n",
            "2948\n",
            "2949\n",
            "2950\n",
            "2951\n",
            "2952\n",
            "2953\n",
            "2954\n",
            "2955\n",
            "2956\n",
            "2957\n",
            "2958\n",
            "2959\n",
            "2960\n",
            "2961\n",
            "2962\n",
            "2963\n",
            "2964\n",
            "2965\n",
            "2966\n",
            "2967\n",
            "2968\n",
            "2969\n",
            "2970\n",
            "2971\n",
            "2972\n",
            "2973\n",
            "2974\n",
            "2975\n",
            "2976\n",
            "2977\n",
            "2978\n",
            "2979\n",
            "2980\n",
            "2981\n",
            "2982\n",
            "2983\n",
            "2984\n",
            "2985\n",
            "2986\n",
            "2987\n",
            "2988\n",
            "2989\n",
            "2990\n",
            "2991\n",
            "2992\n",
            "2993\n",
            "2994\n",
            "2995\n",
            "2996\n",
            "2997\n",
            "2998\n",
            "2999\n",
            "3000\n",
            "Data loaded upto = 3000\n",
            "Data Loading COMPLETED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXuULYJVckCo"
      },
      "source": [
        "# Label Binarizer to change \"yes\", \"no\" to 1s and 0s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hb1pjSLZor8"
      },
      "source": [
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBa66OGYZSRR"
      },
      "source": [
        "# Splitting of dataset into training and testing datas and generate image augmentation as the number of images in dataset is less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyewR2YjXDvq"
      },
      "source": [
        "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.10, stratify=labels, random_state=42)\n",
        "aug = ImageDataGenerator(rotation_range=20,zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,horizontal_flip=True,fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn_DrcUNc5ck"
      },
      "source": [
        "# Using Neurons from MobileNetV2 to create layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfFr-QAFc42t",
        "outputId": "86d6d583-5f77-40d6-cf50-45958556be1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224,3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "\n",
        "headModel = Dense(87, activation=\"relu\")(headModel) #layer 1\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "\n",
        "headModel = Dense(34, activation=\"relu\")(headModel) #layer 2\n",
        "headModel = Dropout(0.4)(headModel)\n",
        "\n",
        "headModel = Dense(13, activation=\"relu\")(headModel) #layer 3\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "\n",
        "headModel = Dense(5, activation=\"relu\")(headModel)  #layer 4\n",
        "headModel = Dropout(0.2)(headModel)\n",
        "\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKeQ8bIn4gz"
      },
      "source": [
        "#Traing on CNN using the data-set \n",
        "# Learning rate = 0.0001\n",
        "# Epochs = 1000\n",
        "# Batch size = 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg98pDuhogB9",
        "outputId": "2c516cf4-9166-4064-b01c-e37f77805020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 1000\n",
        "BS = 50\n",
        "\n",
        "\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(aug.flow(trainX, trainY, batch_size=BS), steps_per_epoch=len(trainX) // BS, validation_data=(testX, testY), validation_steps=len(testX) // BS, epochs=EPOCHS)\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "[INFO] training head...\n",
            "Epoch 1/1000\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.5115WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_test_batch_end` time: 0.0541s). Check your callbacks.\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.8014 - accuracy: 0.5115 - val_loss: 0.6864 - val_accuracy: 0.5467\n",
            "Epoch 2/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.7136 - accuracy: 0.5352 - val_loss: 0.6649 - val_accuracy: 0.6067\n",
            "Epoch 3/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.6887 - accuracy: 0.5574 - val_loss: 0.6524 - val_accuracy: 0.6233\n",
            "Epoch 4/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.6663 - accuracy: 0.5956 - val_loss: 0.6366 - val_accuracy: 0.7167\n",
            "Epoch 5/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.6500 - accuracy: 0.6163 - val_loss: 0.6114 - val_accuracy: 0.7400\n",
            "Epoch 6/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.6481 - accuracy: 0.6341 - val_loss: 0.5681 - val_accuracy: 0.7767\n",
            "Epoch 7/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.6139 - accuracy: 0.6567 - val_loss: 0.5524 - val_accuracy: 0.7700\n",
            "Epoch 8/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.5966 - accuracy: 0.6630 - val_loss: 0.5100 - val_accuracy: 0.8433\n",
            "Epoch 9/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.5713 - accuracy: 0.6926 - val_loss: 0.5086 - val_accuracy: 0.8100\n",
            "Epoch 10/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.5630 - accuracy: 0.7048 - val_loss: 0.4720 - val_accuracy: 0.8500\n",
            "Epoch 11/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.5312 - accuracy: 0.7337 - val_loss: 0.4463 - val_accuracy: 0.8300\n",
            "Epoch 12/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.5340 - accuracy: 0.7319 - val_loss: 0.4219 - val_accuracy: 0.8767\n",
            "Epoch 13/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.5345 - accuracy: 0.7304 - val_loss: 0.4177 - val_accuracy: 0.8867\n",
            "Epoch 14/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.5048 - accuracy: 0.7433 - val_loss: 0.3972 - val_accuracy: 0.8900\n",
            "Epoch 15/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.4848 - accuracy: 0.7648 - val_loss: 0.3739 - val_accuracy: 0.8900\n",
            "Epoch 16/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.4832 - accuracy: 0.7678 - val_loss: 0.3639 - val_accuracy: 0.8967\n",
            "Epoch 17/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.4602 - accuracy: 0.7822 - val_loss: 0.3502 - val_accuracy: 0.9033\n",
            "Epoch 18/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.4684 - accuracy: 0.7815 - val_loss: 0.3332 - val_accuracy: 0.9000\n",
            "Epoch 19/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.4411 - accuracy: 0.8011 - val_loss: 0.3233 - val_accuracy: 0.9000\n",
            "Epoch 20/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.4397 - accuracy: 0.8037 - val_loss: 0.3131 - val_accuracy: 0.9067\n",
            "Epoch 21/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.4223 - accuracy: 0.8107 - val_loss: 0.3010 - val_accuracy: 0.9133\n",
            "Epoch 22/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.4246 - accuracy: 0.8033 - val_loss: 0.2910 - val_accuracy: 0.9233\n",
            "Epoch 23/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.4116 - accuracy: 0.8193 - val_loss: 0.2681 - val_accuracy: 0.9300\n",
            "Epoch 24/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.4129 - accuracy: 0.8259 - val_loss: 0.2630 - val_accuracy: 0.9233\n",
            "Epoch 25/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.3862 - accuracy: 0.8367 - val_loss: 0.2552 - val_accuracy: 0.9267\n",
            "Epoch 26/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.3805 - accuracy: 0.8352 - val_loss: 0.2418 - val_accuracy: 0.9267\n",
            "Epoch 27/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.3700 - accuracy: 0.8422 - val_loss: 0.2476 - val_accuracy: 0.9233\n",
            "Epoch 28/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.3600 - accuracy: 0.8637 - val_loss: 0.2325 - val_accuracy: 0.9367\n",
            "Epoch 29/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.3598 - accuracy: 0.8504 - val_loss: 0.2264 - val_accuracy: 0.9433\n",
            "Epoch 30/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.3622 - accuracy: 0.8507 - val_loss: 0.2352 - val_accuracy: 0.9433\n",
            "Epoch 31/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.3422 - accuracy: 0.8570 - val_loss: 0.2252 - val_accuracy: 0.9367\n",
            "Epoch 32/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.3392 - accuracy: 0.8596 - val_loss: 0.2212 - val_accuracy: 0.9467\n",
            "Epoch 33/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.3377 - accuracy: 0.8715 - val_loss: 0.2151 - val_accuracy: 0.9367\n",
            "Epoch 34/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.3318 - accuracy: 0.8707 - val_loss: 0.2207 - val_accuracy: 0.9500\n",
            "Epoch 35/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.3122 - accuracy: 0.8759 - val_loss: 0.2042 - val_accuracy: 0.9433\n",
            "Epoch 36/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.3015 - accuracy: 0.8800 - val_loss: 0.1930 - val_accuracy: 0.9533\n",
            "Epoch 37/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2941 - accuracy: 0.8870 - val_loss: 0.1983 - val_accuracy: 0.9467\n",
            "Epoch 38/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2990 - accuracy: 0.8811 - val_loss: 0.1995 - val_accuracy: 0.9400\n",
            "Epoch 39/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.3135 - accuracy: 0.8819 - val_loss: 0.1933 - val_accuracy: 0.9533\n",
            "Epoch 40/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2998 - accuracy: 0.8848 - val_loss: 0.1854 - val_accuracy: 0.9467\n",
            "Epoch 41/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.3189 - accuracy: 0.8789 - val_loss: 0.1840 - val_accuracy: 0.9567\n",
            "Epoch 42/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2829 - accuracy: 0.8867 - val_loss: 0.1850 - val_accuracy: 0.9533\n",
            "Epoch 43/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2841 - accuracy: 0.8907 - val_loss: 0.1860 - val_accuracy: 0.9567\n",
            "Epoch 44/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2806 - accuracy: 0.8996 - val_loss: 0.1780 - val_accuracy: 0.9600\n",
            "Epoch 45/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.2754 - accuracy: 0.8941 - val_loss: 0.1703 - val_accuracy: 0.9567\n",
            "Epoch 46/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2811 - accuracy: 0.8948 - val_loss: 0.1738 - val_accuracy: 0.9467\n",
            "Epoch 47/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2749 - accuracy: 0.8926 - val_loss: 0.2023 - val_accuracy: 0.9467\n",
            "Epoch 48/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2747 - accuracy: 0.8993 - val_loss: 0.1665 - val_accuracy: 0.9533\n",
            "Epoch 49/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.2741 - accuracy: 0.8963 - val_loss: 0.1616 - val_accuracy: 0.9600\n",
            "Epoch 50/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2689 - accuracy: 0.9015 - val_loss: 0.1721 - val_accuracy: 0.9533\n",
            "Epoch 51/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2472 - accuracy: 0.9100 - val_loss: 0.1553 - val_accuracy: 0.9667\n",
            "Epoch 52/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2584 - accuracy: 0.9041 - val_loss: 0.1554 - val_accuracy: 0.9633\n",
            "Epoch 53/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2408 - accuracy: 0.9085 - val_loss: 0.1619 - val_accuracy: 0.9633\n",
            "Epoch 54/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2575 - accuracy: 0.9070 - val_loss: 0.1588 - val_accuracy: 0.9600\n",
            "Epoch 55/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2206 - accuracy: 0.9219 - val_loss: 0.1680 - val_accuracy: 0.9567\n",
            "Epoch 56/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2579 - accuracy: 0.9089 - val_loss: 0.1732 - val_accuracy: 0.9567\n",
            "Epoch 57/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.2271 - accuracy: 0.9170 - val_loss: 0.1716 - val_accuracy: 0.9567\n",
            "Epoch 58/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2318 - accuracy: 0.9185 - val_loss: 0.1635 - val_accuracy: 0.9667\n",
            "Epoch 59/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2355 - accuracy: 0.9285 - val_loss: 0.1688 - val_accuracy: 0.9600\n",
            "Epoch 60/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2237 - accuracy: 0.9207 - val_loss: 0.1623 - val_accuracy: 0.9600\n",
            "Epoch 61/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.2402 - accuracy: 0.9215 - val_loss: 0.1813 - val_accuracy: 0.9600\n",
            "Epoch 62/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.2243 - accuracy: 0.9281 - val_loss: 0.1582 - val_accuracy: 0.9600\n",
            "Epoch 63/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2245 - accuracy: 0.9237 - val_loss: 0.1609 - val_accuracy: 0.9633\n",
            "Epoch 64/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.2081 - accuracy: 0.9289 - val_loss: 0.1651 - val_accuracy: 0.9600\n",
            "Epoch 65/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.2052 - accuracy: 0.9270 - val_loss: 0.1558 - val_accuracy: 0.9633\n",
            "Epoch 66/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2164 - accuracy: 0.9274 - val_loss: 0.1653 - val_accuracy: 0.9633\n",
            "Epoch 67/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.2047 - accuracy: 0.9244 - val_loss: 0.1562 - val_accuracy: 0.9600\n",
            "Epoch 68/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1900 - accuracy: 0.9267 - val_loss: 0.1719 - val_accuracy: 0.9600\n",
            "Epoch 69/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2093 - accuracy: 0.9385 - val_loss: 0.1477 - val_accuracy: 0.9633\n",
            "Epoch 70/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.2003 - accuracy: 0.9296 - val_loss: 0.1613 - val_accuracy: 0.9633\n",
            "Epoch 71/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.2033 - accuracy: 0.9341 - val_loss: 0.1840 - val_accuracy: 0.9500\n",
            "Epoch 72/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1977 - accuracy: 0.9330 - val_loss: 0.1833 - val_accuracy: 0.9533\n",
            "Epoch 73/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.1929 - accuracy: 0.9333 - val_loss: 0.1291 - val_accuracy: 0.9700\n",
            "Epoch 74/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1999 - accuracy: 0.9304 - val_loss: 0.1522 - val_accuracy: 0.9700\n",
            "Epoch 75/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1804 - accuracy: 0.9367 - val_loss: 0.1669 - val_accuracy: 0.9633\n",
            "Epoch 76/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.1900 - accuracy: 0.9333 - val_loss: 0.1495 - val_accuracy: 0.9667\n",
            "Epoch 77/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1746 - accuracy: 0.9459 - val_loss: 0.1521 - val_accuracy: 0.9667\n",
            "Epoch 78/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1852 - accuracy: 0.9393 - val_loss: 0.1360 - val_accuracy: 0.9767\n",
            "Epoch 79/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.1885 - accuracy: 0.9389 - val_loss: 0.1608 - val_accuracy: 0.9633\n",
            "Epoch 80/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1975 - accuracy: 0.9319 - val_loss: 0.1487 - val_accuracy: 0.9633\n",
            "Epoch 81/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1628 - accuracy: 0.9504 - val_loss: 0.1832 - val_accuracy: 0.9600\n",
            "Epoch 82/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.1641 - accuracy: 0.9485 - val_loss: 0.1413 - val_accuracy: 0.9667\n",
            "Epoch 83/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1932 - accuracy: 0.9300 - val_loss: 0.1487 - val_accuracy: 0.9633\n",
            "Epoch 84/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1636 - accuracy: 0.9463 - val_loss: 0.1509 - val_accuracy: 0.9700\n",
            "Epoch 85/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.1691 - accuracy: 0.9430 - val_loss: 0.1538 - val_accuracy: 0.9667\n",
            "Epoch 86/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1748 - accuracy: 0.9400 - val_loss: 0.1768 - val_accuracy: 0.9633\n",
            "Epoch 87/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1556 - accuracy: 0.9507 - val_loss: 0.1796 - val_accuracy: 0.9600\n",
            "Epoch 88/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1752 - accuracy: 0.9437 - val_loss: 0.1439 - val_accuracy: 0.9667\n",
            "Epoch 89/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1791 - accuracy: 0.9407 - val_loss: 0.1733 - val_accuracy: 0.9600\n",
            "Epoch 90/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1610 - accuracy: 0.9493 - val_loss: 0.2030 - val_accuracy: 0.9567\n",
            "Epoch 91/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1694 - accuracy: 0.9456 - val_loss: 0.1507 - val_accuracy: 0.9633\n",
            "Epoch 92/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1752 - accuracy: 0.9456 - val_loss: 0.1442 - val_accuracy: 0.9600\n",
            "Epoch 93/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1741 - accuracy: 0.9470 - val_loss: 0.1334 - val_accuracy: 0.9667\n",
            "Epoch 94/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.1712 - accuracy: 0.9430 - val_loss: 0.1655 - val_accuracy: 0.9600\n",
            "Epoch 95/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1432 - accuracy: 0.9548 - val_loss: 0.1655 - val_accuracy: 0.9567\n",
            "Epoch 96/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1719 - accuracy: 0.9463 - val_loss: 0.1383 - val_accuracy: 0.9633\n",
            "Epoch 97/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1552 - accuracy: 0.9478 - val_loss: 0.1296 - val_accuracy: 0.9700\n",
            "Epoch 98/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1562 - accuracy: 0.9500 - val_loss: 0.1479 - val_accuracy: 0.9633\n",
            "Epoch 99/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1561 - accuracy: 0.9489 - val_loss: 0.1741 - val_accuracy: 0.9533\n",
            "Epoch 100/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1513 - accuracy: 0.9567 - val_loss: 0.1862 - val_accuracy: 0.9533\n",
            "Epoch 101/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1624 - accuracy: 0.9441 - val_loss: 0.1458 - val_accuracy: 0.9633\n",
            "Epoch 102/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1673 - accuracy: 0.9456 - val_loss: 0.1805 - val_accuracy: 0.9600\n",
            "Epoch 103/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1470 - accuracy: 0.9467 - val_loss: 0.1550 - val_accuracy: 0.9667\n",
            "Epoch 104/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1432 - accuracy: 0.9574 - val_loss: 0.1320 - val_accuracy: 0.9767\n",
            "Epoch 105/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1454 - accuracy: 0.9537 - val_loss: 0.1505 - val_accuracy: 0.9600\n",
            "Epoch 106/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1550 - accuracy: 0.9511 - val_loss: 0.1419 - val_accuracy: 0.9600\n",
            "Epoch 107/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1462 - accuracy: 0.9519 - val_loss: 0.1592 - val_accuracy: 0.9667\n",
            "Epoch 108/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.1409 - accuracy: 0.9522 - val_loss: 0.1579 - val_accuracy: 0.9733\n",
            "Epoch 109/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1440 - accuracy: 0.9496 - val_loss: 0.1632 - val_accuracy: 0.9600\n",
            "Epoch 110/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1456 - accuracy: 0.9530 - val_loss: 0.1447 - val_accuracy: 0.9667\n",
            "Epoch 111/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1551 - accuracy: 0.9485 - val_loss: 0.1478 - val_accuracy: 0.9667\n",
            "Epoch 112/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1563 - accuracy: 0.9478 - val_loss: 0.1669 - val_accuracy: 0.9633\n",
            "Epoch 113/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.1448 - accuracy: 0.9544 - val_loss: 0.1511 - val_accuracy: 0.9633\n",
            "Epoch 114/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1473 - accuracy: 0.9504 - val_loss: 0.1586 - val_accuracy: 0.9633\n",
            "Epoch 115/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1214 - accuracy: 0.9596 - val_loss: 0.1614 - val_accuracy: 0.9633\n",
            "Epoch 116/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.1445 - accuracy: 0.9519 - val_loss: 0.1558 - val_accuracy: 0.9667\n",
            "Epoch 117/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1571 - accuracy: 0.9526 - val_loss: 0.1591 - val_accuracy: 0.9667\n",
            "Epoch 118/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1185 - accuracy: 0.9589 - val_loss: 0.1584 - val_accuracy: 0.9633\n",
            "Epoch 119/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.1313 - accuracy: 0.9596 - val_loss: 0.1486 - val_accuracy: 0.9667\n",
            "Epoch 120/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.1385 - accuracy: 0.9507 - val_loss: 0.1532 - val_accuracy: 0.9667\n",
            "Epoch 121/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1328 - accuracy: 0.9615 - val_loss: 0.1715 - val_accuracy: 0.9600\n",
            "Epoch 122/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1334 - accuracy: 0.9581 - val_loss: 0.1503 - val_accuracy: 0.9633\n",
            "Epoch 123/1000\n",
            "54/54 [==============================] - 25s 472ms/step - loss: 0.1277 - accuracy: 0.9581 - val_loss: 0.1302 - val_accuracy: 0.9700\n",
            "Epoch 124/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.1307 - accuracy: 0.9570 - val_loss: 0.1311 - val_accuracy: 0.9667\n",
            "Epoch 125/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1161 - accuracy: 0.9656 - val_loss: 0.1286 - val_accuracy: 0.9733\n",
            "Epoch 126/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.1255 - accuracy: 0.9674 - val_loss: 0.1504 - val_accuracy: 0.9633\n",
            "Epoch 127/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.1379 - accuracy: 0.9570 - val_loss: 0.1329 - val_accuracy: 0.9733\n",
            "Epoch 128/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1304 - accuracy: 0.9563 - val_loss: 0.1257 - val_accuracy: 0.9767\n",
            "Epoch 129/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1295 - accuracy: 0.9607 - val_loss: 0.1489 - val_accuracy: 0.9633\n",
            "Epoch 130/1000\n",
            "54/54 [==============================] - 26s 472ms/step - loss: 0.1373 - accuracy: 0.9511 - val_loss: 0.1479 - val_accuracy: 0.9667\n",
            "Epoch 131/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1308 - accuracy: 0.9567 - val_loss: 0.1592 - val_accuracy: 0.9600\n",
            "Epoch 132/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.1272 - accuracy: 0.9589 - val_loss: 0.1332 - val_accuracy: 0.9700\n",
            "Epoch 133/1000\n",
            "54/54 [==============================] - 25s 472ms/step - loss: 0.1126 - accuracy: 0.9644 - val_loss: 0.1593 - val_accuracy: 0.9633\n",
            "Epoch 134/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1265 - accuracy: 0.9593 - val_loss: 0.1501 - val_accuracy: 0.9700\n",
            "Epoch 135/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.1235 - accuracy: 0.9607 - val_loss: 0.1475 - val_accuracy: 0.9667\n",
            "Epoch 136/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.1125 - accuracy: 0.9667 - val_loss: 0.1994 - val_accuracy: 0.9600\n",
            "Epoch 137/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1278 - accuracy: 0.9600 - val_loss: 0.1747 - val_accuracy: 0.9567\n",
            "Epoch 138/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1399 - accuracy: 0.9537 - val_loss: 0.1292 - val_accuracy: 0.9767\n",
            "Epoch 139/1000\n",
            "54/54 [==============================] - 26s 472ms/step - loss: 0.1026 - accuracy: 0.9656 - val_loss: 0.1809 - val_accuracy: 0.9633\n",
            "Epoch 140/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1130 - accuracy: 0.9615 - val_loss: 0.1574 - val_accuracy: 0.9667\n",
            "Epoch 141/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1235 - accuracy: 0.9619 - val_loss: 0.1278 - val_accuracy: 0.9767\n",
            "Epoch 142/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.1013 - accuracy: 0.9659 - val_loss: 0.1507 - val_accuracy: 0.9700\n",
            "Epoch 143/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1217 - accuracy: 0.9574 - val_loss: 0.1363 - val_accuracy: 0.9667\n",
            "Epoch 144/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.1040 - accuracy: 0.9707 - val_loss: 0.1434 - val_accuracy: 0.9733\n",
            "Epoch 145/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.1183 - accuracy: 0.9637 - val_loss: 0.1519 - val_accuracy: 0.9633\n",
            "Epoch 146/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1118 - accuracy: 0.9626 - val_loss: 0.1612 - val_accuracy: 0.9700\n",
            "Epoch 147/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1325 - accuracy: 0.9530 - val_loss: 0.1406 - val_accuracy: 0.9667\n",
            "Epoch 148/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1174 - accuracy: 0.9644 - val_loss: 0.1366 - val_accuracy: 0.9700\n",
            "Epoch 149/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.1123 - accuracy: 0.9667 - val_loss: 0.1746 - val_accuracy: 0.9633\n",
            "Epoch 150/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.1029 - accuracy: 0.9659 - val_loss: 0.1675 - val_accuracy: 0.9633\n",
            "Epoch 151/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.1077 - accuracy: 0.9663 - val_loss: 0.1597 - val_accuracy: 0.9667\n",
            "Epoch 152/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.1243 - accuracy: 0.9641 - val_loss: 0.1533 - val_accuracy: 0.9667\n",
            "Epoch 153/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0946 - accuracy: 0.9715 - val_loss: 0.1522 - val_accuracy: 0.9700\n",
            "Epoch 154/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.1109 - accuracy: 0.9615 - val_loss: 0.1320 - val_accuracy: 0.9733\n",
            "Epoch 155/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.1276 - accuracy: 0.9596 - val_loss: 0.1160 - val_accuracy: 0.9733\n",
            "Epoch 156/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0995 - accuracy: 0.9685 - val_loss: 0.1294 - val_accuracy: 0.9733\n",
            "Epoch 157/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.1135 - accuracy: 0.9637 - val_loss: 0.1211 - val_accuracy: 0.9700\n",
            "Epoch 158/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0987 - accuracy: 0.9678 - val_loss: 0.1510 - val_accuracy: 0.9733\n",
            "Epoch 159/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.1087 - accuracy: 0.9674 - val_loss: 0.1793 - val_accuracy: 0.9600\n",
            "Epoch 160/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0994 - accuracy: 0.9733 - val_loss: 0.1514 - val_accuracy: 0.9633\n",
            "Epoch 161/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.1021 - accuracy: 0.9648 - val_loss: 0.1350 - val_accuracy: 0.9733\n",
            "Epoch 162/1000\n",
            "54/54 [==============================] - 27s 495ms/step - loss: 0.0993 - accuracy: 0.9678 - val_loss: 0.1382 - val_accuracy: 0.9667\n",
            "Epoch 163/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.1179 - accuracy: 0.9630 - val_loss: 0.1310 - val_accuracy: 0.9667\n",
            "Epoch 164/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.1155 - accuracy: 0.9633 - val_loss: 0.1249 - val_accuracy: 0.9733\n",
            "Epoch 165/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0957 - accuracy: 0.9670 - val_loss: 0.1261 - val_accuracy: 0.9733\n",
            "Epoch 166/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0974 - accuracy: 0.9704 - val_loss: 0.1790 - val_accuracy: 0.9567\n",
            "Epoch 167/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.1105 - accuracy: 0.9663 - val_loss: 0.1284 - val_accuracy: 0.9700\n",
            "Epoch 168/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.1096 - accuracy: 0.9633 - val_loss: 0.1432 - val_accuracy: 0.9733\n",
            "Epoch 169/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.1192 - accuracy: 0.9604 - val_loss: 0.1067 - val_accuracy: 0.9767\n",
            "Epoch 170/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.1116 - accuracy: 0.9674 - val_loss: 0.1481 - val_accuracy: 0.9633\n",
            "Epoch 171/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0965 - accuracy: 0.9681 - val_loss: 0.1460 - val_accuracy: 0.9667\n",
            "Epoch 172/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0988 - accuracy: 0.9674 - val_loss: 0.1562 - val_accuracy: 0.9600\n",
            "Epoch 173/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.1009 - accuracy: 0.9678 - val_loss: 0.1186 - val_accuracy: 0.9733\n",
            "Epoch 174/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.1087 - accuracy: 0.9637 - val_loss: 0.1493 - val_accuracy: 0.9633\n",
            "Epoch 175/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0988 - accuracy: 0.9693 - val_loss: 0.1415 - val_accuracy: 0.9733\n",
            "Epoch 176/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.1065 - accuracy: 0.9704 - val_loss: 0.1358 - val_accuracy: 0.9700\n",
            "Epoch 177/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.1062 - accuracy: 0.9663 - val_loss: 0.1271 - val_accuracy: 0.9733\n",
            "Epoch 178/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.1112 - accuracy: 0.9652 - val_loss: 0.1018 - val_accuracy: 0.9767\n",
            "Epoch 179/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.1033 - accuracy: 0.9674 - val_loss: 0.1047 - val_accuracy: 0.9767\n",
            "Epoch 180/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.1011 - accuracy: 0.9648 - val_loss: 0.1147 - val_accuracy: 0.9800\n",
            "Epoch 181/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0924 - accuracy: 0.9681 - val_loss: 0.1122 - val_accuracy: 0.9767\n",
            "Epoch 182/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0970 - accuracy: 0.9693 - val_loss: 0.1182 - val_accuracy: 0.9767\n",
            "Epoch 183/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0871 - accuracy: 0.9678 - val_loss: 0.1245 - val_accuracy: 0.9800\n",
            "Epoch 184/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0999 - accuracy: 0.9704 - val_loss: 0.1132 - val_accuracy: 0.9800\n",
            "Epoch 185/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.1013 - accuracy: 0.9693 - val_loss: 0.1189 - val_accuracy: 0.9700\n",
            "Epoch 186/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.1082 - accuracy: 0.9678 - val_loss: 0.1322 - val_accuracy: 0.9667\n",
            "Epoch 187/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.1037 - accuracy: 0.9630 - val_loss: 0.1576 - val_accuracy: 0.9600\n",
            "Epoch 188/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.1081 - accuracy: 0.9656 - val_loss: 0.1277 - val_accuracy: 0.9733\n",
            "Epoch 189/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0986 - accuracy: 0.9663 - val_loss: 0.1285 - val_accuracy: 0.9733\n",
            "Epoch 190/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0950 - accuracy: 0.9737 - val_loss: 0.1175 - val_accuracy: 0.9767\n",
            "Epoch 191/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0977 - accuracy: 0.9689 - val_loss: 0.1289 - val_accuracy: 0.9667\n",
            "Epoch 192/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0915 - accuracy: 0.9719 - val_loss: 0.1248 - val_accuracy: 0.9733\n",
            "Epoch 193/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0956 - accuracy: 0.9715 - val_loss: 0.1350 - val_accuracy: 0.9700\n",
            "Epoch 194/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0983 - accuracy: 0.9707 - val_loss: 0.1607 - val_accuracy: 0.9633\n",
            "Epoch 195/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0873 - accuracy: 0.9726 - val_loss: 0.1069 - val_accuracy: 0.9767\n",
            "Epoch 196/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.1011 - accuracy: 0.9663 - val_loss: 0.1168 - val_accuracy: 0.9767\n",
            "Epoch 197/1000\n",
            "54/54 [==============================] - 28s 509ms/step - loss: 0.0928 - accuracy: 0.9719 - val_loss: 0.1283 - val_accuracy: 0.9733\n",
            "Epoch 198/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0989 - accuracy: 0.9700 - val_loss: 0.1447 - val_accuracy: 0.9700\n",
            "Epoch 199/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0854 - accuracy: 0.9689 - val_loss: 0.1557 - val_accuracy: 0.9667\n",
            "Epoch 200/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.1034 - accuracy: 0.9641 - val_loss: 0.1358 - val_accuracy: 0.9733\n",
            "Epoch 201/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.1066 - accuracy: 0.9652 - val_loss: 0.1121 - val_accuracy: 0.9733\n",
            "Epoch 202/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0865 - accuracy: 0.9741 - val_loss: 0.1593 - val_accuracy: 0.9667\n",
            "Epoch 203/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0754 - accuracy: 0.9752 - val_loss: 0.1380 - val_accuracy: 0.9733\n",
            "Epoch 204/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0932 - accuracy: 0.9685 - val_loss: 0.1524 - val_accuracy: 0.9667\n",
            "Epoch 205/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0972 - accuracy: 0.9711 - val_loss: 0.1423 - val_accuracy: 0.9733\n",
            "Epoch 206/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0952 - accuracy: 0.9700 - val_loss: 0.1561 - val_accuracy: 0.9600\n",
            "Epoch 207/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0877 - accuracy: 0.9722 - val_loss: 0.1256 - val_accuracy: 0.9733\n",
            "Epoch 208/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0874 - accuracy: 0.9748 - val_loss: 0.1644 - val_accuracy: 0.9600\n",
            "Epoch 209/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.1057 - accuracy: 0.9715 - val_loss: 0.1029 - val_accuracy: 0.9767\n",
            "Epoch 210/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0978 - accuracy: 0.9674 - val_loss: 0.0979 - val_accuracy: 0.9833\n",
            "Epoch 211/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0975 - accuracy: 0.9678 - val_loss: 0.1156 - val_accuracy: 0.9700\n",
            "Epoch 212/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0898 - accuracy: 0.9711 - val_loss: 0.1171 - val_accuracy: 0.9733\n",
            "Epoch 213/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0804 - accuracy: 0.9756 - val_loss: 0.1265 - val_accuracy: 0.9633\n",
            "Epoch 214/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0834 - accuracy: 0.9681 - val_loss: 0.1485 - val_accuracy: 0.9633\n",
            "Epoch 215/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0919 - accuracy: 0.9730 - val_loss: 0.1143 - val_accuracy: 0.9767\n",
            "Epoch 216/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0900 - accuracy: 0.9707 - val_loss: 0.1089 - val_accuracy: 0.9767\n",
            "Epoch 217/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0857 - accuracy: 0.9722 - val_loss: 0.1726 - val_accuracy: 0.9600\n",
            "Epoch 218/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.1004 - accuracy: 0.9730 - val_loss: 0.1202 - val_accuracy: 0.9733\n",
            "Epoch 219/1000\n",
            "54/54 [==============================] - 27s 494ms/step - loss: 0.0806 - accuracy: 0.9763 - val_loss: 0.1289 - val_accuracy: 0.9733\n",
            "Epoch 220/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0757 - accuracy: 0.9796 - val_loss: 0.1837 - val_accuracy: 0.9633\n",
            "Epoch 221/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.1024 - accuracy: 0.9719 - val_loss: 0.1061 - val_accuracy: 0.9767\n",
            "Epoch 222/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0934 - accuracy: 0.9733 - val_loss: 0.1403 - val_accuracy: 0.9633\n",
            "Epoch 223/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0764 - accuracy: 0.9737 - val_loss: 0.1368 - val_accuracy: 0.9733\n",
            "Epoch 224/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0925 - accuracy: 0.9700 - val_loss: 0.1667 - val_accuracy: 0.9667\n",
            "Epoch 225/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0887 - accuracy: 0.9726 - val_loss: 0.1362 - val_accuracy: 0.9633\n",
            "Epoch 226/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0818 - accuracy: 0.9722 - val_loss: 0.1068 - val_accuracy: 0.9800\n",
            "Epoch 227/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0946 - accuracy: 0.9685 - val_loss: 0.1134 - val_accuracy: 0.9800\n",
            "Epoch 228/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0735 - accuracy: 0.9741 - val_loss: 0.1504 - val_accuracy: 0.9667\n",
            "Epoch 229/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0841 - accuracy: 0.9741 - val_loss: 0.1229 - val_accuracy: 0.9733\n",
            "Epoch 230/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0796 - accuracy: 0.9770 - val_loss: 0.1504 - val_accuracy: 0.9700\n",
            "Epoch 231/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.0886 - accuracy: 0.9681 - val_loss: 0.1286 - val_accuracy: 0.9733\n",
            "Epoch 232/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0937 - accuracy: 0.9689 - val_loss: 0.1469 - val_accuracy: 0.9600\n",
            "Epoch 233/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0975 - accuracy: 0.9678 - val_loss: 0.0922 - val_accuracy: 0.9833\n",
            "Epoch 234/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0876 - accuracy: 0.9730 - val_loss: 0.1074 - val_accuracy: 0.9767\n",
            "Epoch 235/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0909 - accuracy: 0.9741 - val_loss: 0.1182 - val_accuracy: 0.9767\n",
            "Epoch 236/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.0792 - accuracy: 0.9774 - val_loss: 0.1282 - val_accuracy: 0.9733\n",
            "Epoch 237/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.0882 - accuracy: 0.9733 - val_loss: 0.0981 - val_accuracy: 0.9867\n",
            "Epoch 238/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0905 - accuracy: 0.9752 - val_loss: 0.1062 - val_accuracy: 0.9800\n",
            "Epoch 239/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0858 - accuracy: 0.9700 - val_loss: 0.0936 - val_accuracy: 0.9900\n",
            "Epoch 240/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0757 - accuracy: 0.9759 - val_loss: 0.0939 - val_accuracy: 0.9900\n",
            "Epoch 241/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0988 - accuracy: 0.9689 - val_loss: 0.1241 - val_accuracy: 0.9800\n",
            "Epoch 242/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0991 - accuracy: 0.9644 - val_loss: 0.1086 - val_accuracy: 0.9800\n",
            "Epoch 243/1000\n",
            "54/54 [==============================] - 26s 473ms/step - loss: 0.0867 - accuracy: 0.9711 - val_loss: 0.1115 - val_accuracy: 0.9800\n",
            "Epoch 244/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0894 - accuracy: 0.9693 - val_loss: 0.1086 - val_accuracy: 0.9800\n",
            "Epoch 245/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.0808 - accuracy: 0.9741 - val_loss: 0.1090 - val_accuracy: 0.9800\n",
            "Epoch 246/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0885 - accuracy: 0.9733 - val_loss: 0.1282 - val_accuracy: 0.9767\n",
            "Epoch 247/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0798 - accuracy: 0.9726 - val_loss: 0.1051 - val_accuracy: 0.9833\n",
            "Epoch 248/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0872 - accuracy: 0.9722 - val_loss: 0.1059 - val_accuracy: 0.9800\n",
            "Epoch 249/1000\n",
            "54/54 [==============================] - 26s 474ms/step - loss: 0.0716 - accuracy: 0.9785 - val_loss: 0.1371 - val_accuracy: 0.9733\n",
            "Epoch 250/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0851 - accuracy: 0.9730 - val_loss: 0.1133 - val_accuracy: 0.9800\n",
            "Epoch 251/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0644 - accuracy: 0.9778 - val_loss: 0.1306 - val_accuracy: 0.9767\n",
            "Epoch 252/1000\n",
            "54/54 [==============================] - 26s 475ms/step - loss: 0.0805 - accuracy: 0.9759 - val_loss: 0.1150 - val_accuracy: 0.9767\n",
            "Epoch 253/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0807 - accuracy: 0.9730 - val_loss: 0.1094 - val_accuracy: 0.9800\n",
            "Epoch 254/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0724 - accuracy: 0.9759 - val_loss: 0.1024 - val_accuracy: 0.9833\n",
            "Epoch 255/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0779 - accuracy: 0.9733 - val_loss: 0.1014 - val_accuracy: 0.9833\n",
            "Epoch 256/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0714 - accuracy: 0.9752 - val_loss: 0.1194 - val_accuracy: 0.9767\n",
            "Epoch 257/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0843 - accuracy: 0.9719 - val_loss: 0.1351 - val_accuracy: 0.9733\n",
            "Epoch 258/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0669 - accuracy: 0.9774 - val_loss: 0.1167 - val_accuracy: 0.9833\n",
            "Epoch 259/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0708 - accuracy: 0.9807 - val_loss: 0.1105 - val_accuracy: 0.9833\n",
            "Epoch 260/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0695 - accuracy: 0.9781 - val_loss: 0.0943 - val_accuracy: 0.9867\n",
            "Epoch 261/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0812 - accuracy: 0.9730 - val_loss: 0.1125 - val_accuracy: 0.9800\n",
            "Epoch 262/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0752 - accuracy: 0.9785 - val_loss: 0.1210 - val_accuracy: 0.9800\n",
            "Epoch 263/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0805 - accuracy: 0.9722 - val_loss: 0.1036 - val_accuracy: 0.9867\n",
            "Epoch 264/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0857 - accuracy: 0.9707 - val_loss: 0.1124 - val_accuracy: 0.9833\n",
            "Epoch 265/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0891 - accuracy: 0.9685 - val_loss: 0.1434 - val_accuracy: 0.9733\n",
            "Epoch 266/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0904 - accuracy: 0.9681 - val_loss: 0.1539 - val_accuracy: 0.9633\n",
            "Epoch 267/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0760 - accuracy: 0.9752 - val_loss: 0.1047 - val_accuracy: 0.9833\n",
            "Epoch 268/1000\n",
            "54/54 [==============================] - 27s 494ms/step - loss: 0.0650 - accuracy: 0.9752 - val_loss: 0.1044 - val_accuracy: 0.9833\n",
            "Epoch 269/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0829 - accuracy: 0.9733 - val_loss: 0.1087 - val_accuracy: 0.9800\n",
            "Epoch 270/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0709 - accuracy: 0.9770 - val_loss: 0.1064 - val_accuracy: 0.9800\n",
            "Epoch 271/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0724 - accuracy: 0.9767 - val_loss: 0.1086 - val_accuracy: 0.9800\n",
            "Epoch 272/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0807 - accuracy: 0.9711 - val_loss: 0.1080 - val_accuracy: 0.9833\n",
            "Epoch 273/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0824 - accuracy: 0.9719 - val_loss: 0.0917 - val_accuracy: 0.9833\n",
            "Epoch 274/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0716 - accuracy: 0.9778 - val_loss: 0.1264 - val_accuracy: 0.9767\n",
            "Epoch 275/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0730 - accuracy: 0.9759 - val_loss: 0.1315 - val_accuracy: 0.9800\n",
            "Epoch 276/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0938 - accuracy: 0.9696 - val_loss: 0.0998 - val_accuracy: 0.9800\n",
            "Epoch 277/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0688 - accuracy: 0.9778 - val_loss: 0.1042 - val_accuracy: 0.9833\n",
            "Epoch 278/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0773 - accuracy: 0.9707 - val_loss: 0.1383 - val_accuracy: 0.9667\n",
            "Epoch 279/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0806 - accuracy: 0.9774 - val_loss: 0.1360 - val_accuracy: 0.9733\n",
            "Epoch 280/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0830 - accuracy: 0.9730 - val_loss: 0.1163 - val_accuracy: 0.9833\n",
            "Epoch 281/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0696 - accuracy: 0.9785 - val_loss: 0.1060 - val_accuracy: 0.9867\n",
            "Epoch 282/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0807 - accuracy: 0.9711 - val_loss: 0.1004 - val_accuracy: 0.9833\n",
            "Epoch 283/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0772 - accuracy: 0.9756 - val_loss: 0.1094 - val_accuracy: 0.9800\n",
            "Epoch 284/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0667 - accuracy: 0.9807 - val_loss: 0.1105 - val_accuracy: 0.9800\n",
            "Epoch 285/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0827 - accuracy: 0.9756 - val_loss: 0.1295 - val_accuracy: 0.9767\n",
            "Epoch 286/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0713 - accuracy: 0.9774 - val_loss: 0.1245 - val_accuracy: 0.9767\n",
            "Epoch 287/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0761 - accuracy: 0.9748 - val_loss: 0.1057 - val_accuracy: 0.9833\n",
            "Epoch 288/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0727 - accuracy: 0.9774 - val_loss: 0.1065 - val_accuracy: 0.9833\n",
            "Epoch 289/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0732 - accuracy: 0.9785 - val_loss: 0.1011 - val_accuracy: 0.9833\n",
            "Epoch 290/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0773 - accuracy: 0.9733 - val_loss: 0.1236 - val_accuracy: 0.9767\n",
            "Epoch 291/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0851 - accuracy: 0.9719 - val_loss: 0.0980 - val_accuracy: 0.9867\n",
            "Epoch 292/1000\n",
            "54/54 [==============================] - 28s 524ms/step - loss: 0.0753 - accuracy: 0.9752 - val_loss: 0.1044 - val_accuracy: 0.9767\n",
            "Epoch 293/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0767 - accuracy: 0.9756 - val_loss: 0.1140 - val_accuracy: 0.9833\n",
            "Epoch 294/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0643 - accuracy: 0.9796 - val_loss: 0.1409 - val_accuracy: 0.9767\n",
            "Epoch 295/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0868 - accuracy: 0.9700 - val_loss: 0.1137 - val_accuracy: 0.9800\n",
            "Epoch 296/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0620 - accuracy: 0.9800 - val_loss: 0.1488 - val_accuracy: 0.9767\n",
            "Epoch 297/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0752 - accuracy: 0.9796 - val_loss: 0.1490 - val_accuracy: 0.9733\n",
            "Epoch 298/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0646 - accuracy: 0.9789 - val_loss: 0.1246 - val_accuracy: 0.9800\n",
            "Epoch 299/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0700 - accuracy: 0.9759 - val_loss: 0.1489 - val_accuracy: 0.9700\n",
            "Epoch 300/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0628 - accuracy: 0.9778 - val_loss: 0.1096 - val_accuracy: 0.9733\n",
            "Epoch 301/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0663 - accuracy: 0.9789 - val_loss: 0.1056 - val_accuracy: 0.9800\n",
            "Epoch 302/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0760 - accuracy: 0.9759 - val_loss: 0.1232 - val_accuracy: 0.9767\n",
            "Epoch 303/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0743 - accuracy: 0.9752 - val_loss: 0.1077 - val_accuracy: 0.9867\n",
            "Epoch 304/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0771 - accuracy: 0.9748 - val_loss: 0.1027 - val_accuracy: 0.9833\n",
            "Epoch 305/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0664 - accuracy: 0.9778 - val_loss: 0.1059 - val_accuracy: 0.9833\n",
            "Epoch 306/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0570 - accuracy: 0.9826 - val_loss: 0.1006 - val_accuracy: 0.9867\n",
            "Epoch 307/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0530 - accuracy: 0.9819 - val_loss: 0.0993 - val_accuracy: 0.9867\n",
            "Epoch 308/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0789 - accuracy: 0.9763 - val_loss: 0.1060 - val_accuracy: 0.9833\n",
            "Epoch 309/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0739 - accuracy: 0.9733 - val_loss: 0.0967 - val_accuracy: 0.9867\n",
            "Epoch 310/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0783 - accuracy: 0.9767 - val_loss: 0.1181 - val_accuracy: 0.9733\n",
            "Epoch 311/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0633 - accuracy: 0.9815 - val_loss: 0.1208 - val_accuracy: 0.9733\n",
            "Epoch 312/1000\n",
            "54/54 [==============================] - 28s 509ms/step - loss: 0.0695 - accuracy: 0.9759 - val_loss: 0.0963 - val_accuracy: 0.9800\n",
            "Epoch 313/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0654 - accuracy: 0.9789 - val_loss: 0.1063 - val_accuracy: 0.9800\n",
            "Epoch 314/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0734 - accuracy: 0.9767 - val_loss: 0.1064 - val_accuracy: 0.9800\n",
            "Epoch 315/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0707 - accuracy: 0.9774 - val_loss: 0.1102 - val_accuracy: 0.9767\n",
            "Epoch 316/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0727 - accuracy: 0.9767 - val_loss: 0.1208 - val_accuracy: 0.9800\n",
            "Epoch 317/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0749 - accuracy: 0.9726 - val_loss: 0.1244 - val_accuracy: 0.9733\n",
            "Epoch 318/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0722 - accuracy: 0.9737 - val_loss: 0.1028 - val_accuracy: 0.9733\n",
            "Epoch 319/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0708 - accuracy: 0.9785 - val_loss: 0.1076 - val_accuracy: 0.9767\n",
            "Epoch 320/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 0.1017 - val_accuracy: 0.9767\n",
            "Epoch 321/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0776 - accuracy: 0.9770 - val_loss: 0.1180 - val_accuracy: 0.9733\n",
            "Epoch 322/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0705 - accuracy: 0.9759 - val_loss: 0.1156 - val_accuracy: 0.9733\n",
            "Epoch 323/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0690 - accuracy: 0.9811 - val_loss: 0.1252 - val_accuracy: 0.9800\n",
            "Epoch 324/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0618 - accuracy: 0.9804 - val_loss: 0.1199 - val_accuracy: 0.9733\n",
            "Epoch 325/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0708 - accuracy: 0.9756 - val_loss: 0.0999 - val_accuracy: 0.9800\n",
            "Epoch 326/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0601 - accuracy: 0.9819 - val_loss: 0.1218 - val_accuracy: 0.9800\n",
            "Epoch 327/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0671 - accuracy: 0.9785 - val_loss: 0.1103 - val_accuracy: 0.9733\n",
            "Epoch 328/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0693 - accuracy: 0.9789 - val_loss: 0.1089 - val_accuracy: 0.9767\n",
            "Epoch 329/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0658 - accuracy: 0.9785 - val_loss: 0.1064 - val_accuracy: 0.9800\n",
            "Epoch 330/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0629 - accuracy: 0.9796 - val_loss: 0.1292 - val_accuracy: 0.9767\n",
            "Epoch 331/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0594 - accuracy: 0.9811 - val_loss: 0.1172 - val_accuracy: 0.9800\n",
            "Epoch 332/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0646 - accuracy: 0.9807 - val_loss: 0.0970 - val_accuracy: 0.9833\n",
            "Epoch 333/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0567 - accuracy: 0.9815 - val_loss: 0.1112 - val_accuracy: 0.9767\n",
            "Epoch 334/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0615 - accuracy: 0.9811 - val_loss: 0.1515 - val_accuracy: 0.9667\n",
            "Epoch 335/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0857 - accuracy: 0.9756 - val_loss: 0.1090 - val_accuracy: 0.9767\n",
            "Epoch 336/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0656 - accuracy: 0.9719 - val_loss: 0.1250 - val_accuracy: 0.9700\n",
            "Epoch 337/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0665 - accuracy: 0.9774 - val_loss: 0.1005 - val_accuracy: 0.9800\n",
            "Epoch 338/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0598 - accuracy: 0.9804 - val_loss: 0.1055 - val_accuracy: 0.9833\n",
            "Epoch 339/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0788 - accuracy: 0.9741 - val_loss: 0.1380 - val_accuracy: 0.9700\n",
            "Epoch 340/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0611 - accuracy: 0.9778 - val_loss: 0.1329 - val_accuracy: 0.9733\n",
            "Epoch 341/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0666 - accuracy: 0.9796 - val_loss: 0.1104 - val_accuracy: 0.9733\n",
            "Epoch 342/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0527 - accuracy: 0.9863 - val_loss: 0.1506 - val_accuracy: 0.9667\n",
            "Epoch 343/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0597 - accuracy: 0.9800 - val_loss: 0.1498 - val_accuracy: 0.9667\n",
            "Epoch 344/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0758 - accuracy: 0.9774 - val_loss: 0.0877 - val_accuracy: 0.9900\n",
            "Epoch 345/1000\n",
            "54/54 [==============================] - 27s 494ms/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.1073 - val_accuracy: 0.9767\n",
            "Epoch 346/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0689 - accuracy: 0.9796 - val_loss: 0.1230 - val_accuracy: 0.9767\n",
            "Epoch 347/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0698 - accuracy: 0.9800 - val_loss: 0.1350 - val_accuracy: 0.9667\n",
            "Epoch 348/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0614 - accuracy: 0.9811 - val_loss: 0.1025 - val_accuracy: 0.9833\n",
            "Epoch 349/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0777 - accuracy: 0.9770 - val_loss: 0.1510 - val_accuracy: 0.9700\n",
            "Epoch 350/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0798 - accuracy: 0.9763 - val_loss: 0.1270 - val_accuracy: 0.9733\n",
            "Epoch 351/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0707 - accuracy: 0.9793 - val_loss: 0.1024 - val_accuracy: 0.9767\n",
            "Epoch 352/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0604 - accuracy: 0.9793 - val_loss: 0.1439 - val_accuracy: 0.9700\n",
            "Epoch 353/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0629 - accuracy: 0.9781 - val_loss: 0.0943 - val_accuracy: 0.9867\n",
            "Epoch 354/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0766 - accuracy: 0.9774 - val_loss: 0.1020 - val_accuracy: 0.9800\n",
            "Epoch 355/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0616 - accuracy: 0.9804 - val_loss: 0.1440 - val_accuracy: 0.9700\n",
            "Epoch 356/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0733 - accuracy: 0.9756 - val_loss: 0.1384 - val_accuracy: 0.9700\n",
            "Epoch 357/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0527 - accuracy: 0.9833 - val_loss: 0.1161 - val_accuracy: 0.9767\n",
            "Epoch 358/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0749 - accuracy: 0.9744 - val_loss: 0.1053 - val_accuracy: 0.9833\n",
            "Epoch 359/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.1354 - val_accuracy: 0.9767\n",
            "Epoch 360/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0654 - accuracy: 0.9800 - val_loss: 0.1087 - val_accuracy: 0.9767\n",
            "Epoch 361/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0715 - accuracy: 0.9759 - val_loss: 0.1078 - val_accuracy: 0.9800\n",
            "Epoch 362/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0544 - accuracy: 0.9830 - val_loss: 0.1034 - val_accuracy: 0.9800\n",
            "Epoch 363/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0691 - accuracy: 0.9752 - val_loss: 0.1659 - val_accuracy: 0.9700\n",
            "Epoch 364/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0572 - accuracy: 0.9826 - val_loss: 0.1015 - val_accuracy: 0.9833\n",
            "Epoch 365/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0686 - accuracy: 0.9785 - val_loss: 0.1209 - val_accuracy: 0.9767\n",
            "Epoch 366/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0746 - accuracy: 0.9781 - val_loss: 0.1006 - val_accuracy: 0.9800\n",
            "Epoch 367/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0762 - accuracy: 0.9785 - val_loss: 0.1443 - val_accuracy: 0.9667\n",
            "Epoch 368/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0540 - accuracy: 0.9822 - val_loss: 0.1257 - val_accuracy: 0.9733\n",
            "Epoch 369/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0666 - accuracy: 0.9793 - val_loss: 0.0845 - val_accuracy: 0.9867\n",
            "Epoch 370/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0734 - accuracy: 0.9752 - val_loss: 0.0966 - val_accuracy: 0.9867\n",
            "Epoch 371/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0537 - accuracy: 0.9822 - val_loss: 0.1106 - val_accuracy: 0.9833\n",
            "Epoch 372/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.0945 - val_accuracy: 0.9867\n",
            "Epoch 373/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0663 - accuracy: 0.9793 - val_loss: 0.1152 - val_accuracy: 0.9833\n",
            "Epoch 374/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.1242 - val_accuracy: 0.9800\n",
            "Epoch 375/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0639 - accuracy: 0.9804 - val_loss: 0.1112 - val_accuracy: 0.9833\n",
            "Epoch 376/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0549 - accuracy: 0.9793 - val_loss: 0.1171 - val_accuracy: 0.9833\n",
            "Epoch 377/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0651 - accuracy: 0.9830 - val_loss: 0.1128 - val_accuracy: 0.9833\n",
            "Epoch 378/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0675 - accuracy: 0.9759 - val_loss: 0.0922 - val_accuracy: 0.9867\n",
            "Epoch 379/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0679 - accuracy: 0.9774 - val_loss: 0.0870 - val_accuracy: 0.9867\n",
            "Epoch 380/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0630 - accuracy: 0.9796 - val_loss: 0.0976 - val_accuracy: 0.9833\n",
            "Epoch 381/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0545 - accuracy: 0.9815 - val_loss: 0.0930 - val_accuracy: 0.9867\n",
            "Epoch 382/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0705 - accuracy: 0.9763 - val_loss: 0.1255 - val_accuracy: 0.9767\n",
            "Epoch 383/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0543 - accuracy: 0.9819 - val_loss: 0.1347 - val_accuracy: 0.9733\n",
            "Epoch 384/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0521 - accuracy: 0.9822 - val_loss: 0.1257 - val_accuracy: 0.9800\n",
            "Epoch 385/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0565 - accuracy: 0.9837 - val_loss: 0.1180 - val_accuracy: 0.9800\n",
            "Epoch 386/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0650 - accuracy: 0.9789 - val_loss: 0.1031 - val_accuracy: 0.9800\n",
            "Epoch 387/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0676 - accuracy: 0.9778 - val_loss: 0.1005 - val_accuracy: 0.9833\n",
            "Epoch 388/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0627 - accuracy: 0.9781 - val_loss: 0.1095 - val_accuracy: 0.9767\n",
            "Epoch 389/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0666 - accuracy: 0.9778 - val_loss: 0.1248 - val_accuracy: 0.9733\n",
            "Epoch 390/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0614 - accuracy: 0.9781 - val_loss: 0.1009 - val_accuracy: 0.9833\n",
            "Epoch 391/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0528 - accuracy: 0.9804 - val_loss: 0.1050 - val_accuracy: 0.9800\n",
            "Epoch 392/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0632 - accuracy: 0.9789 - val_loss: 0.0964 - val_accuracy: 0.9833\n",
            "Epoch 393/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0578 - accuracy: 0.9819 - val_loss: 0.1343 - val_accuracy: 0.9700\n",
            "Epoch 394/1000\n",
            "54/54 [==============================] - 27s 495ms/step - loss: 0.0620 - accuracy: 0.9807 - val_loss: 0.1086 - val_accuracy: 0.9867\n",
            "Epoch 395/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0761 - accuracy: 0.9763 - val_loss: 0.1126 - val_accuracy: 0.9767\n",
            "Epoch 396/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0643 - accuracy: 0.9811 - val_loss: 0.1412 - val_accuracy: 0.9733\n",
            "Epoch 397/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.1399 - val_accuracy: 0.9667\n",
            "Epoch 398/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0661 - accuracy: 0.9785 - val_loss: 0.0936 - val_accuracy: 0.9900\n",
            "Epoch 399/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0665 - accuracy: 0.9763 - val_loss: 0.0915 - val_accuracy: 0.9867\n",
            "Epoch 400/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0505 - accuracy: 0.9833 - val_loss: 0.0865 - val_accuracy: 0.9867\n",
            "Epoch 401/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0622 - accuracy: 0.9793 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
            "Epoch 402/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0639 - accuracy: 0.9815 - val_loss: 0.1130 - val_accuracy: 0.9833\n",
            "Epoch 403/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0635 - accuracy: 0.9826 - val_loss: 0.1313 - val_accuracy: 0.9733\n",
            "Epoch 404/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0668 - accuracy: 0.9781 - val_loss: 0.0978 - val_accuracy: 0.9867\n",
            "Epoch 405/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0654 - accuracy: 0.9781 - val_loss: 0.1452 - val_accuracy: 0.9667\n",
            "Epoch 406/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0679 - accuracy: 0.9763 - val_loss: 0.0980 - val_accuracy: 0.9833\n",
            "Epoch 407/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0552 - accuracy: 0.9804 - val_loss: 0.0828 - val_accuracy: 0.9867\n",
            "Epoch 408/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0612 - accuracy: 0.9796 - val_loss: 0.1059 - val_accuracy: 0.9800\n",
            "Epoch 409/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.0959 - val_accuracy: 0.9833\n",
            "Epoch 410/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0637 - accuracy: 0.9800 - val_loss: 0.1080 - val_accuracy: 0.9800\n",
            "Epoch 411/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0581 - accuracy: 0.9826 - val_loss: 0.1029 - val_accuracy: 0.9800\n",
            "Epoch 412/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0650 - accuracy: 0.9833 - val_loss: 0.1359 - val_accuracy: 0.9700\n",
            "Epoch 413/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0642 - accuracy: 0.9793 - val_loss: 0.1315 - val_accuracy: 0.9767\n",
            "Epoch 414/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0552 - accuracy: 0.9822 - val_loss: 0.1465 - val_accuracy: 0.9700\n",
            "Epoch 415/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0603 - accuracy: 0.9785 - val_loss: 0.1032 - val_accuracy: 0.9833\n",
            "Epoch 416/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0619 - accuracy: 0.9811 - val_loss: 0.0804 - val_accuracy: 0.9867\n",
            "Epoch 417/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0594 - accuracy: 0.9822 - val_loss: 0.1056 - val_accuracy: 0.9833\n",
            "Epoch 418/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0644 - accuracy: 0.9796 - val_loss: 0.0858 - val_accuracy: 0.9900\n",
            "Epoch 419/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0498 - accuracy: 0.9815 - val_loss: 0.1200 - val_accuracy: 0.9767\n",
            "Epoch 420/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0549 - accuracy: 0.9815 - val_loss: 0.1073 - val_accuracy: 0.9867\n",
            "Epoch 421/1000\n",
            "54/54 [==============================] - 27s 495ms/step - loss: 0.0602 - accuracy: 0.9819 - val_loss: 0.1284 - val_accuracy: 0.9767\n",
            "Epoch 422/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0600 - accuracy: 0.9807 - val_loss: 0.1129 - val_accuracy: 0.9767\n",
            "Epoch 423/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0734 - accuracy: 0.9796 - val_loss: 0.1062 - val_accuracy: 0.9867\n",
            "Epoch 424/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.1397 - val_accuracy: 0.9733\n",
            "Epoch 425/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0576 - accuracy: 0.9819 - val_loss: 0.1362 - val_accuracy: 0.9700\n",
            "Epoch 426/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0448 - accuracy: 0.9852 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
            "Epoch 427/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0480 - accuracy: 0.9811 - val_loss: 0.0974 - val_accuracy: 0.9833\n",
            "Epoch 428/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0596 - accuracy: 0.9815 - val_loss: 0.1140 - val_accuracy: 0.9833\n",
            "Epoch 429/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0564 - accuracy: 0.9819 - val_loss: 0.0854 - val_accuracy: 0.9900\n",
            "Epoch 430/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0548 - accuracy: 0.9815 - val_loss: 0.1137 - val_accuracy: 0.9800\n",
            "Epoch 431/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0481 - accuracy: 0.9878 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
            "Epoch 432/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0623 - accuracy: 0.9778 - val_loss: 0.1110 - val_accuracy: 0.9833\n",
            "Epoch 433/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0411 - accuracy: 0.9863 - val_loss: 0.1370 - val_accuracy: 0.9767\n",
            "Epoch 434/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.1096 - val_accuracy: 0.9800\n",
            "Epoch 435/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0596 - accuracy: 0.9796 - val_loss: 0.0922 - val_accuracy: 0.9833\n",
            "Epoch 436/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0561 - accuracy: 0.9844 - val_loss: 0.1098 - val_accuracy: 0.9833\n",
            "Epoch 437/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0678 - accuracy: 0.9756 - val_loss: 0.0914 - val_accuracy: 0.9900\n",
            "Epoch 438/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0580 - accuracy: 0.9789 - val_loss: 0.1398 - val_accuracy: 0.9700\n",
            "Epoch 439/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0767 - accuracy: 0.9767 - val_loss: 0.0915 - val_accuracy: 0.9833\n",
            "Epoch 440/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0492 - accuracy: 0.9859 - val_loss: 0.1166 - val_accuracy: 0.9800\n",
            "Epoch 441/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0521 - accuracy: 0.9830 - val_loss: 0.1144 - val_accuracy: 0.9800\n",
            "Epoch 442/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0455 - accuracy: 0.9867 - val_loss: 0.0741 - val_accuracy: 0.9900\n",
            "Epoch 443/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0569 - accuracy: 0.9789 - val_loss: 0.0702 - val_accuracy: 0.9867\n",
            "Epoch 444/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0549 - accuracy: 0.9822 - val_loss: 0.1108 - val_accuracy: 0.9800\n",
            "Epoch 445/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0589 - accuracy: 0.9804 - val_loss: 0.0949 - val_accuracy: 0.9800\n",
            "Epoch 446/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0503 - accuracy: 0.9874 - val_loss: 0.1285 - val_accuracy: 0.9767\n",
            "Epoch 447/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0475 - accuracy: 0.9848 - val_loss: 0.1034 - val_accuracy: 0.9767\n",
            "Epoch 448/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0604 - accuracy: 0.9804 - val_loss: 0.0920 - val_accuracy: 0.9867\n",
            "Epoch 449/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0650 - accuracy: 0.9744 - val_loss: 0.0937 - val_accuracy: 0.9833\n",
            "Epoch 450/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0445 - accuracy: 0.9881 - val_loss: 0.1076 - val_accuracy: 0.9800\n",
            "Epoch 451/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0477 - accuracy: 0.9848 - val_loss: 0.1341 - val_accuracy: 0.9800\n",
            "Epoch 452/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0743 - accuracy: 0.9774 - val_loss: 0.1154 - val_accuracy: 0.9833\n",
            "Epoch 453/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0418 - accuracy: 0.9852 - val_loss: 0.1154 - val_accuracy: 0.9833\n",
            "Epoch 454/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0445 - accuracy: 0.9841 - val_loss: 0.1121 - val_accuracy: 0.9800\n",
            "Epoch 455/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0667 - accuracy: 0.9811 - val_loss: 0.1061 - val_accuracy: 0.9833\n",
            "Epoch 456/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0622 - accuracy: 0.9781 - val_loss: 0.0768 - val_accuracy: 0.9833\n",
            "Epoch 457/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0526 - accuracy: 0.9819 - val_loss: 0.0960 - val_accuracy: 0.9800\n",
            "Epoch 458/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0563 - accuracy: 0.9822 - val_loss: 0.1112 - val_accuracy: 0.9767\n",
            "Epoch 459/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0582 - accuracy: 0.9815 - val_loss: 0.1100 - val_accuracy: 0.9800\n",
            "Epoch 460/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0568 - accuracy: 0.9811 - val_loss: 0.1151 - val_accuracy: 0.9767\n",
            "Epoch 461/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0582 - accuracy: 0.9852 - val_loss: 0.1033 - val_accuracy: 0.9800\n",
            "Epoch 462/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0567 - accuracy: 0.9826 - val_loss: 0.1312 - val_accuracy: 0.9733\n",
            "Epoch 463/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0580 - accuracy: 0.9815 - val_loss: 0.1159 - val_accuracy: 0.9733\n",
            "Epoch 464/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0568 - accuracy: 0.9807 - val_loss: 0.0867 - val_accuracy: 0.9800\n",
            "Epoch 465/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0500 - accuracy: 0.9804 - val_loss: 0.1285 - val_accuracy: 0.9767\n",
            "Epoch 466/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0551 - accuracy: 0.9793 - val_loss: 0.1085 - val_accuracy: 0.9767\n",
            "Epoch 467/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 0.1197 - val_accuracy: 0.9767\n",
            "Epoch 468/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0634 - accuracy: 0.9756 - val_loss: 0.0939 - val_accuracy: 0.9800\n",
            "Epoch 469/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0596 - accuracy: 0.9793 - val_loss: 0.1031 - val_accuracy: 0.9800\n",
            "Epoch 470/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0545 - accuracy: 0.9822 - val_loss: 0.0933 - val_accuracy: 0.9867\n",
            "Epoch 471/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0525 - accuracy: 0.9848 - val_loss: 0.1338 - val_accuracy: 0.9733\n",
            "Epoch 472/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 0.0865 - val_accuracy: 0.9900\n",
            "Epoch 473/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0553 - accuracy: 0.9811 - val_loss: 0.0851 - val_accuracy: 0.9900\n",
            "Epoch 474/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0623 - accuracy: 0.9800 - val_loss: 0.0859 - val_accuracy: 0.9867\n",
            "Epoch 475/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0583 - accuracy: 0.9819 - val_loss: 0.1051 - val_accuracy: 0.9767\n",
            "Epoch 476/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0508 - accuracy: 0.9841 - val_loss: 0.1144 - val_accuracy: 0.9767\n",
            "Epoch 477/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0498 - accuracy: 0.9830 - val_loss: 0.0888 - val_accuracy: 0.9900\n",
            "Epoch 478/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0498 - accuracy: 0.9815 - val_loss: 0.0974 - val_accuracy: 0.9867\n",
            "Epoch 479/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0520 - accuracy: 0.9837 - val_loss: 0.0980 - val_accuracy: 0.9867\n",
            "Epoch 480/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0609 - accuracy: 0.9796 - val_loss: 0.1016 - val_accuracy: 0.9833\n",
            "Epoch 481/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0502 - accuracy: 0.9826 - val_loss: 0.1012 - val_accuracy: 0.9833\n",
            "Epoch 482/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0543 - accuracy: 0.9841 - val_loss: 0.0934 - val_accuracy: 0.9833\n",
            "Epoch 483/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0573 - accuracy: 0.9830 - val_loss: 0.0839 - val_accuracy: 0.9833\n",
            "Epoch 484/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0464 - accuracy: 0.9859 - val_loss: 0.1094 - val_accuracy: 0.9800\n",
            "Epoch 485/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0604 - accuracy: 0.9856 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
            "Epoch 486/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0407 - accuracy: 0.9874 - val_loss: 0.0812 - val_accuracy: 0.9867\n",
            "Epoch 487/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0487 - accuracy: 0.9822 - val_loss: 0.0932 - val_accuracy: 0.9867\n",
            "Epoch 488/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0475 - accuracy: 0.9848 - val_loss: 0.0919 - val_accuracy: 0.9833\n",
            "Epoch 489/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0536 - accuracy: 0.9804 - val_loss: 0.1087 - val_accuracy: 0.9767\n",
            "Epoch 490/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0505 - accuracy: 0.9837 - val_loss: 0.0865 - val_accuracy: 0.9867\n",
            "Epoch 491/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0558 - accuracy: 0.9826 - val_loss: 0.0943 - val_accuracy: 0.9800\n",
            "Epoch 492/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0558 - accuracy: 0.9785 - val_loss: 0.0971 - val_accuracy: 0.9833\n",
            "Epoch 493/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.1060 - val_accuracy: 0.9833\n",
            "Epoch 494/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0553 - accuracy: 0.9830 - val_loss: 0.0915 - val_accuracy: 0.9867\n",
            "Epoch 495/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0500 - accuracy: 0.9811 - val_loss: 0.1075 - val_accuracy: 0.9800\n",
            "Epoch 496/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0600 - accuracy: 0.9800 - val_loss: 0.1106 - val_accuracy: 0.9767\n",
            "Epoch 497/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.1085 - val_accuracy: 0.9767\n",
            "Epoch 498/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0617 - accuracy: 0.9796 - val_loss: 0.1144 - val_accuracy: 0.9833\n",
            "Epoch 499/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0475 - accuracy: 0.9830 - val_loss: 0.1104 - val_accuracy: 0.9733\n",
            "Epoch 500/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0480 - accuracy: 0.9844 - val_loss: 0.0864 - val_accuracy: 0.9867\n",
            "Epoch 501/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0615 - accuracy: 0.9774 - val_loss: 0.0876 - val_accuracy: 0.9867\n",
            "Epoch 502/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0601 - accuracy: 0.9789 - val_loss: 0.1244 - val_accuracy: 0.9800\n",
            "Epoch 503/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0581 - accuracy: 0.9822 - val_loss: 0.1038 - val_accuracy: 0.9867\n",
            "Epoch 504/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0502 - accuracy: 0.9837 - val_loss: 0.1045 - val_accuracy: 0.9833\n",
            "Epoch 505/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0457 - accuracy: 0.9870 - val_loss: 0.1001 - val_accuracy: 0.9800\n",
            "Epoch 506/1000\n",
            "54/54 [==============================] - 28s 509ms/step - loss: 0.0456 - accuracy: 0.9859 - val_loss: 0.1198 - val_accuracy: 0.9800\n",
            "Epoch 507/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0583 - accuracy: 0.9807 - val_loss: 0.0869 - val_accuracy: 0.9833\n",
            "Epoch 508/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0458 - accuracy: 0.9856 - val_loss: 0.0927 - val_accuracy: 0.9867\n",
            "Epoch 509/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0406 - accuracy: 0.9867 - val_loss: 0.1184 - val_accuracy: 0.9800\n",
            "Epoch 510/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0433 - accuracy: 0.9863 - val_loss: 0.0952 - val_accuracy: 0.9867\n",
            "Epoch 511/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0494 - accuracy: 0.9830 - val_loss: 0.0848 - val_accuracy: 0.9867\n",
            "Epoch 512/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0445 - accuracy: 0.9826 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 513/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0667 - accuracy: 0.9774 - val_loss: 0.0864 - val_accuracy: 0.9867\n",
            "Epoch 514/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0435 - accuracy: 0.9841 - val_loss: 0.0948 - val_accuracy: 0.9867\n",
            "Epoch 515/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0457 - accuracy: 0.9870 - val_loss: 0.0861 - val_accuracy: 0.9867\n",
            "Epoch 516/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0410 - accuracy: 0.9878 - val_loss: 0.1081 - val_accuracy: 0.9833\n",
            "Epoch 517/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0594 - accuracy: 0.9826 - val_loss: 0.1045 - val_accuracy: 0.9800\n",
            "Epoch 518/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0469 - accuracy: 0.9815 - val_loss: 0.0867 - val_accuracy: 0.9867\n",
            "Epoch 519/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0478 - accuracy: 0.9815 - val_loss: 0.0880 - val_accuracy: 0.9867\n",
            "Epoch 520/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0507 - accuracy: 0.9822 - val_loss: 0.0947 - val_accuracy: 0.9867\n",
            "Epoch 521/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0518 - accuracy: 0.9848 - val_loss: 0.1092 - val_accuracy: 0.9867\n",
            "Epoch 522/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0500 - accuracy: 0.9804 - val_loss: 0.1110 - val_accuracy: 0.9800\n",
            "Epoch 523/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0486 - accuracy: 0.9867 - val_loss: 0.1030 - val_accuracy: 0.9800\n",
            "Epoch 524/1000\n",
            "54/54 [==============================] - 28s 528ms/step - loss: 0.0609 - accuracy: 0.9815 - val_loss: 0.0650 - val_accuracy: 0.9900\n",
            "Epoch 525/1000\n",
            "54/54 [==============================] - 28s 523ms/step - loss: 0.0525 - accuracy: 0.9841 - val_loss: 0.0958 - val_accuracy: 0.9833\n",
            "Epoch 526/1000\n",
            "54/54 [==============================] - 28s 524ms/step - loss: 0.0465 - accuracy: 0.9837 - val_loss: 0.0759 - val_accuracy: 0.9900\n",
            "Epoch 527/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0692 - accuracy: 0.9748 - val_loss: 0.0961 - val_accuracy: 0.9867\n",
            "Epoch 528/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0480 - accuracy: 0.9815 - val_loss: 0.1154 - val_accuracy: 0.9800\n",
            "Epoch 529/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0545 - accuracy: 0.9822 - val_loss: 0.1022 - val_accuracy: 0.9867\n",
            "Epoch 530/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 0.0952 - val_accuracy: 0.9867\n",
            "Epoch 531/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0581 - accuracy: 0.9807 - val_loss: 0.0817 - val_accuracy: 0.9900\n",
            "Epoch 532/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0384 - accuracy: 0.9848 - val_loss: 0.1009 - val_accuracy: 0.9833\n",
            "Epoch 533/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0566 - accuracy: 0.9830 - val_loss: 0.0717 - val_accuracy: 0.9833\n",
            "Epoch 534/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0387 - accuracy: 0.9878 - val_loss: 0.0742 - val_accuracy: 0.9867\n",
            "Epoch 535/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0417 - accuracy: 0.9848 - val_loss: 0.1026 - val_accuracy: 0.9867\n",
            "Epoch 536/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0523 - accuracy: 0.9833 - val_loss: 0.0952 - val_accuracy: 0.9867\n",
            "Epoch 537/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0462 - accuracy: 0.9859 - val_loss: 0.0659 - val_accuracy: 0.9900\n",
            "Epoch 538/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0523 - accuracy: 0.9848 - val_loss: 0.0740 - val_accuracy: 0.9900\n",
            "Epoch 539/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0540 - accuracy: 0.9826 - val_loss: 0.0701 - val_accuracy: 0.9900\n",
            "Epoch 540/1000\n",
            "54/54 [==============================] - 28s 522ms/step - loss: 0.0477 - accuracy: 0.9844 - val_loss: 0.0868 - val_accuracy: 0.9800\n",
            "Epoch 541/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0500 - accuracy: 0.9867 - val_loss: 0.0913 - val_accuracy: 0.9900\n",
            "Epoch 542/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0484 - accuracy: 0.9867 - val_loss: 0.0940 - val_accuracy: 0.9867\n",
            "Epoch 543/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0421 - accuracy: 0.9844 - val_loss: 0.0944 - val_accuracy: 0.9833\n",
            "Epoch 544/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0418 - accuracy: 0.9874 - val_loss: 0.0848 - val_accuracy: 0.9833\n",
            "Epoch 545/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0444 - accuracy: 0.9874 - val_loss: 0.0769 - val_accuracy: 0.9900\n",
            "Epoch 546/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0491 - accuracy: 0.9837 - val_loss: 0.0970 - val_accuracy: 0.9867\n",
            "Epoch 547/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0434 - accuracy: 0.9844 - val_loss: 0.1040 - val_accuracy: 0.9833\n",
            "Epoch 548/1000\n",
            "54/54 [==============================] - 28s 523ms/step - loss: 0.0550 - accuracy: 0.9822 - val_loss: 0.1369 - val_accuracy: 0.9733\n",
            "Epoch 549/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0486 - accuracy: 0.9848 - val_loss: 0.0933 - val_accuracy: 0.9867\n",
            "Epoch 550/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0479 - accuracy: 0.9841 - val_loss: 0.1165 - val_accuracy: 0.9833\n",
            "Epoch 551/1000\n",
            "54/54 [==============================] - 28s 524ms/step - loss: 0.0415 - accuracy: 0.9856 - val_loss: 0.1277 - val_accuracy: 0.9833\n",
            "Epoch 552/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0431 - accuracy: 0.9848 - val_loss: 0.0987 - val_accuracy: 0.9867\n",
            "Epoch 553/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0485 - accuracy: 0.9856 - val_loss: 0.1013 - val_accuracy: 0.9867\n",
            "Epoch 554/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0554 - accuracy: 0.9848 - val_loss: 0.1137 - val_accuracy: 0.9833\n",
            "Epoch 555/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0494 - accuracy: 0.9833 - val_loss: 0.1010 - val_accuracy: 0.9900\n",
            "Epoch 556/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0435 - accuracy: 0.9848 - val_loss: 0.1074 - val_accuracy: 0.9867\n",
            "Epoch 557/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0561 - accuracy: 0.9811 - val_loss: 0.1033 - val_accuracy: 0.9833\n",
            "Epoch 558/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0440 - accuracy: 0.9830 - val_loss: 0.1074 - val_accuracy: 0.9767\n",
            "Epoch 559/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0466 - accuracy: 0.9856 - val_loss: 0.0874 - val_accuracy: 0.9900\n",
            "Epoch 560/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0317 - accuracy: 0.9893 - val_loss: 0.1171 - val_accuracy: 0.9867\n",
            "Epoch 561/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0551 - accuracy: 0.9837 - val_loss: 0.0824 - val_accuracy: 0.9833\n",
            "Epoch 562/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0429 - accuracy: 0.9844 - val_loss: 0.1152 - val_accuracy: 0.9833\n",
            "Epoch 563/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0533 - accuracy: 0.9848 - val_loss: 0.0850 - val_accuracy: 0.9900\n",
            "Epoch 564/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0421 - accuracy: 0.9878 - val_loss: 0.0875 - val_accuracy: 0.9833\n",
            "Epoch 565/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0353 - accuracy: 0.9900 - val_loss: 0.0943 - val_accuracy: 0.9867\n",
            "Epoch 566/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0442 - accuracy: 0.9867 - val_loss: 0.1094 - val_accuracy: 0.9900\n",
            "Epoch 567/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0370 - accuracy: 0.9870 - val_loss: 0.0936 - val_accuracy: 0.9867\n",
            "Epoch 568/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0514 - accuracy: 0.9881 - val_loss: 0.0896 - val_accuracy: 0.9867\n",
            "Epoch 569/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0588 - accuracy: 0.9781 - val_loss: 0.0974 - val_accuracy: 0.9900\n",
            "Epoch 570/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0412 - accuracy: 0.9867 - val_loss: 0.0935 - val_accuracy: 0.9900\n",
            "Epoch 571/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0431 - accuracy: 0.9852 - val_loss: 0.1030 - val_accuracy: 0.9867\n",
            "Epoch 572/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0575 - accuracy: 0.9804 - val_loss: 0.0969 - val_accuracy: 0.9867\n",
            "Epoch 573/1000\n",
            "54/54 [==============================] - 28s 523ms/step - loss: 0.0519 - accuracy: 0.9841 - val_loss: 0.1178 - val_accuracy: 0.9833\n",
            "Epoch 574/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0396 - accuracy: 0.9863 - val_loss: 0.1421 - val_accuracy: 0.9767\n",
            "Epoch 575/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0499 - accuracy: 0.9837 - val_loss: 0.1075 - val_accuracy: 0.9833\n",
            "Epoch 576/1000\n",
            "54/54 [==============================] - 28s 521ms/step - loss: 0.0444 - accuracy: 0.9874 - val_loss: 0.1107 - val_accuracy: 0.9900\n",
            "Epoch 577/1000\n",
            "54/54 [==============================] - 28s 522ms/step - loss: 0.0603 - accuracy: 0.9848 - val_loss: 0.0791 - val_accuracy: 0.9867\n",
            "Epoch 578/1000\n",
            "54/54 [==============================] - 28s 522ms/step - loss: 0.0387 - accuracy: 0.9859 - val_loss: 0.0928 - val_accuracy: 0.9867\n",
            "Epoch 579/1000\n",
            "54/54 [==============================] - 28s 523ms/step - loss: 0.0439 - accuracy: 0.9833 - val_loss: 0.0864 - val_accuracy: 0.9867\n",
            "Epoch 580/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0655 - accuracy: 0.9819 - val_loss: 0.0792 - val_accuracy: 0.9867\n",
            "Epoch 581/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0436 - accuracy: 0.9852 - val_loss: 0.0801 - val_accuracy: 0.9867\n",
            "Epoch 582/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0480 - accuracy: 0.9819 - val_loss: 0.0907 - val_accuracy: 0.9867\n",
            "Epoch 583/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0531 - accuracy: 0.9852 - val_loss: 0.0792 - val_accuracy: 0.9900\n",
            "Epoch 584/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0464 - accuracy: 0.9826 - val_loss: 0.0827 - val_accuracy: 0.9867\n",
            "Epoch 585/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0441 - accuracy: 0.9844 - val_loss: 0.1212 - val_accuracy: 0.9800\n",
            "Epoch 586/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.1163 - val_accuracy: 0.9800\n",
            "Epoch 587/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0495 - accuracy: 0.9856 - val_loss: 0.0982 - val_accuracy: 0.9867\n",
            "Epoch 588/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0505 - accuracy: 0.9819 - val_loss: 0.0855 - val_accuracy: 0.9867\n",
            "Epoch 589/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0520 - accuracy: 0.9874 - val_loss: 0.1063 - val_accuracy: 0.9867\n",
            "Epoch 590/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0448 - accuracy: 0.9856 - val_loss: 0.0971 - val_accuracy: 0.9833\n",
            "Epoch 591/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0515 - accuracy: 0.9826 - val_loss: 0.0856 - val_accuracy: 0.9900\n",
            "Epoch 592/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0343 - accuracy: 0.9881 - val_loss: 0.1252 - val_accuracy: 0.9767\n",
            "Epoch 593/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0522 - accuracy: 0.9844 - val_loss: 0.0794 - val_accuracy: 0.9867\n",
            "Epoch 594/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0531 - accuracy: 0.9837 - val_loss: 0.0823 - val_accuracy: 0.9900\n",
            "Epoch 595/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 0.0869 - val_accuracy: 0.9900\n",
            "Epoch 596/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0460 - accuracy: 0.9833 - val_loss: 0.0757 - val_accuracy: 0.9900\n",
            "Epoch 597/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0511 - accuracy: 0.9852 - val_loss: 0.1057 - val_accuracy: 0.9833\n",
            "Epoch 598/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0491 - accuracy: 0.9826 - val_loss: 0.0814 - val_accuracy: 0.9867\n",
            "Epoch 599/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0391 - accuracy: 0.9870 - val_loss: 0.0837 - val_accuracy: 0.9867\n",
            "Epoch 600/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0458 - accuracy: 0.9863 - val_loss: 0.1004 - val_accuracy: 0.9833\n",
            "Epoch 601/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.0674 - val_accuracy: 0.9867\n",
            "Epoch 602/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0451 - accuracy: 0.9867 - val_loss: 0.0676 - val_accuracy: 0.9867\n",
            "Epoch 603/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0371 - accuracy: 0.9881 - val_loss: 0.0695 - val_accuracy: 0.9833\n",
            "Epoch 604/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0400 - accuracy: 0.9881 - val_loss: 0.0808 - val_accuracy: 0.9867\n",
            "Epoch 605/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0541 - accuracy: 0.9837 - val_loss: 0.0661 - val_accuracy: 0.9900\n",
            "Epoch 606/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0460 - accuracy: 0.9867 - val_loss: 0.0816 - val_accuracy: 0.9867\n",
            "Epoch 607/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0413 - accuracy: 0.9856 - val_loss: 0.0866 - val_accuracy: 0.9900\n",
            "Epoch 608/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0492 - accuracy: 0.9863 - val_loss: 0.1032 - val_accuracy: 0.9867\n",
            "Epoch 609/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0612 - accuracy: 0.9819 - val_loss: 0.0800 - val_accuracy: 0.9900\n",
            "Epoch 610/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0552 - accuracy: 0.9800 - val_loss: 0.0819 - val_accuracy: 0.9900\n",
            "Epoch 611/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.0940 - val_accuracy: 0.9900\n",
            "Epoch 612/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0530 - accuracy: 0.9837 - val_loss: 0.0755 - val_accuracy: 0.9833\n",
            "Epoch 613/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0483 - accuracy: 0.9822 - val_loss: 0.1149 - val_accuracy: 0.9833\n",
            "Epoch 614/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0426 - accuracy: 0.9874 - val_loss: 0.1244 - val_accuracy: 0.9833\n",
            "Epoch 615/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0577 - accuracy: 0.9826 - val_loss: 0.0892 - val_accuracy: 0.9767\n",
            "Epoch 616/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0515 - accuracy: 0.9844 - val_loss: 0.0808 - val_accuracy: 0.9867\n",
            "Epoch 617/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0383 - accuracy: 0.9859 - val_loss: 0.0943 - val_accuracy: 0.9900\n",
            "Epoch 618/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0427 - accuracy: 0.9852 - val_loss: 0.0909 - val_accuracy: 0.9900\n",
            "Epoch 619/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0485 - accuracy: 0.9844 - val_loss: 0.0861 - val_accuracy: 0.9867\n",
            "Epoch 620/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0532 - accuracy: 0.9819 - val_loss: 0.0800 - val_accuracy: 0.9900\n",
            "Epoch 621/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0374 - accuracy: 0.9867 - val_loss: 0.0881 - val_accuracy: 0.9833\n",
            "Epoch 622/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0544 - accuracy: 0.9844 - val_loss: 0.0914 - val_accuracy: 0.9900\n",
            "Epoch 623/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 0.0859 - val_accuracy: 0.9867\n",
            "Epoch 624/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0533 - accuracy: 0.9822 - val_loss: 0.0653 - val_accuracy: 0.9900\n",
            "Epoch 625/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0397 - accuracy: 0.9859 - val_loss: 0.0953 - val_accuracy: 0.9867\n",
            "Epoch 626/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0431 - accuracy: 0.9830 - val_loss: 0.0785 - val_accuracy: 0.9867\n",
            "Epoch 627/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0429 - accuracy: 0.9863 - val_loss: 0.0722 - val_accuracy: 0.9833\n",
            "Epoch 628/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0513 - accuracy: 0.9833 - val_loss: 0.0786 - val_accuracy: 0.9867\n",
            "Epoch 629/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.0972 - val_accuracy: 0.9833\n",
            "Epoch 630/1000\n",
            "54/54 [==============================] - 27s 495ms/step - loss: 0.0425 - accuracy: 0.9841 - val_loss: 0.0733 - val_accuracy: 0.9900\n",
            "Epoch 631/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0469 - accuracy: 0.9841 - val_loss: 0.0889 - val_accuracy: 0.9867\n",
            "Epoch 632/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0446 - accuracy: 0.9863 - val_loss: 0.1013 - val_accuracy: 0.9900\n",
            "Epoch 633/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0523 - accuracy: 0.9874 - val_loss: 0.0919 - val_accuracy: 0.9867\n",
            "Epoch 634/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.0955 - val_accuracy: 0.9867\n",
            "Epoch 635/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0400 - accuracy: 0.9878 - val_loss: 0.1046 - val_accuracy: 0.9867\n",
            "Epoch 636/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0539 - accuracy: 0.9819 - val_loss: 0.0981 - val_accuracy: 0.9900\n",
            "Epoch 637/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0396 - accuracy: 0.9885 - val_loss: 0.0983 - val_accuracy: 0.9900\n",
            "Epoch 638/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 0.1304 - val_accuracy: 0.9767\n",
            "Epoch 639/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0450 - accuracy: 0.9856 - val_loss: 0.0791 - val_accuracy: 0.9900\n",
            "Epoch 640/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0510 - accuracy: 0.9811 - val_loss: 0.1014 - val_accuracy: 0.9833\n",
            "Epoch 641/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0381 - accuracy: 0.9878 - val_loss: 0.1113 - val_accuracy: 0.9867\n",
            "Epoch 642/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0976 - val_accuracy: 0.9867\n",
            "Epoch 643/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0385 - accuracy: 0.9852 - val_loss: 0.0763 - val_accuracy: 0.9900\n",
            "Epoch 644/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0313 - accuracy: 0.9889 - val_loss: 0.1164 - val_accuracy: 0.9867\n",
            "Epoch 645/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0506 - accuracy: 0.9826 - val_loss: 0.1094 - val_accuracy: 0.9867\n",
            "Epoch 646/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0425 - accuracy: 0.9863 - val_loss: 0.0687 - val_accuracy: 0.9900\n",
            "Epoch 647/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0442 - accuracy: 0.9841 - val_loss: 0.0827 - val_accuracy: 0.9867\n",
            "Epoch 648/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0379 - accuracy: 0.9881 - val_loss: 0.0891 - val_accuracy: 0.9900\n",
            "Epoch 649/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0360 - accuracy: 0.9885 - val_loss: 0.0855 - val_accuracy: 0.9900\n",
            "Epoch 650/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0444 - accuracy: 0.9874 - val_loss: 0.1002 - val_accuracy: 0.9900\n",
            "Epoch 651/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0422 - accuracy: 0.9885 - val_loss: 0.0913 - val_accuracy: 0.9833\n",
            "Epoch 652/1000\n",
            "54/54 [==============================] - 27s 495ms/step - loss: 0.0335 - accuracy: 0.9870 - val_loss: 0.0900 - val_accuracy: 0.9833\n",
            "Epoch 653/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0432 - accuracy: 0.9852 - val_loss: 0.1093 - val_accuracy: 0.9833\n",
            "Epoch 654/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0525 - accuracy: 0.9822 - val_loss: 0.1145 - val_accuracy: 0.9800\n",
            "Epoch 655/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0467 - accuracy: 0.9859 - val_loss: 0.0919 - val_accuracy: 0.9833\n",
            "Epoch 656/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0618 - accuracy: 0.9800 - val_loss: 0.1034 - val_accuracy: 0.9867\n",
            "Epoch 657/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0337 - accuracy: 0.9885 - val_loss: 0.0997 - val_accuracy: 0.9900\n",
            "Epoch 658/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0479 - accuracy: 0.9856 - val_loss: 0.1129 - val_accuracy: 0.9833\n",
            "Epoch 659/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0369 - accuracy: 0.9896 - val_loss: 0.0901 - val_accuracy: 0.9867\n",
            "Epoch 660/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0486 - accuracy: 0.9837 - val_loss: 0.1139 - val_accuracy: 0.9767\n",
            "Epoch 661/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0459 - accuracy: 0.9844 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 662/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0462 - accuracy: 0.9833 - val_loss: 0.1215 - val_accuracy: 0.9800\n",
            "Epoch 663/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0443 - accuracy: 0.9837 - val_loss: 0.0915 - val_accuracy: 0.9867\n",
            "Epoch 664/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0418 - accuracy: 0.9848 - val_loss: 0.0959 - val_accuracy: 0.9867\n",
            "Epoch 665/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0362 - accuracy: 0.9896 - val_loss: 0.0915 - val_accuracy: 0.9800\n",
            "Epoch 666/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0425 - accuracy: 0.9889 - val_loss: 0.0991 - val_accuracy: 0.9900\n",
            "Epoch 667/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0336 - accuracy: 0.9863 - val_loss: 0.0993 - val_accuracy: 0.9867\n",
            "Epoch 668/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0408 - accuracy: 0.9867 - val_loss: 0.0820 - val_accuracy: 0.9833\n",
            "Epoch 669/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0505 - accuracy: 0.9822 - val_loss: 0.0709 - val_accuracy: 0.9867\n",
            "Epoch 670/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0418 - accuracy: 0.9863 - val_loss: 0.0753 - val_accuracy: 0.9900\n",
            "Epoch 671/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0324 - accuracy: 0.9870 - val_loss: 0.1163 - val_accuracy: 0.9833\n",
            "Epoch 672/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0426 - accuracy: 0.9863 - val_loss: 0.0979 - val_accuracy: 0.9867\n",
            "Epoch 673/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0412 - accuracy: 0.9859 - val_loss: 0.0985 - val_accuracy: 0.9867\n",
            "Epoch 674/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0410 - accuracy: 0.9841 - val_loss: 0.1293 - val_accuracy: 0.9700\n",
            "Epoch 675/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0497 - accuracy: 0.9881 - val_loss: 0.1025 - val_accuracy: 0.9767\n",
            "Epoch 676/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0539 - accuracy: 0.9848 - val_loss: 0.0859 - val_accuracy: 0.9800\n",
            "Epoch 677/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0445 - accuracy: 0.9841 - val_loss: 0.0945 - val_accuracy: 0.9833\n",
            "Epoch 678/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0467 - accuracy: 0.9848 - val_loss: 0.1072 - val_accuracy: 0.9867\n",
            "Epoch 679/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0431 - accuracy: 0.9848 - val_loss: 0.0830 - val_accuracy: 0.9867\n",
            "Epoch 680/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0486 - accuracy: 0.9822 - val_loss: 0.0731 - val_accuracy: 0.9867\n",
            "Epoch 681/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0370 - accuracy: 0.9844 - val_loss: 0.0959 - val_accuracy: 0.9867\n",
            "Epoch 682/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0425 - accuracy: 0.9859 - val_loss: 0.0826 - val_accuracy: 0.9867\n",
            "Epoch 683/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0498 - accuracy: 0.9830 - val_loss: 0.0977 - val_accuracy: 0.9867\n",
            "Epoch 684/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0431 - accuracy: 0.9863 - val_loss: 0.0816 - val_accuracy: 0.9900\n",
            "Epoch 685/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0378 - accuracy: 0.9848 - val_loss: 0.1134 - val_accuracy: 0.9833\n",
            "Epoch 686/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0514 - accuracy: 0.9804 - val_loss: 0.1003 - val_accuracy: 0.9867\n",
            "Epoch 687/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0508 - accuracy: 0.9848 - val_loss: 0.0925 - val_accuracy: 0.9900\n",
            "Epoch 688/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0535 - accuracy: 0.9878 - val_loss: 0.1158 - val_accuracy: 0.9867\n",
            "Epoch 689/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0499 - accuracy: 0.9852 - val_loss: 0.0739 - val_accuracy: 0.9867\n",
            "Epoch 690/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0393 - accuracy: 0.9863 - val_loss: 0.0987 - val_accuracy: 0.9900\n",
            "Epoch 691/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0360 - accuracy: 0.9874 - val_loss: 0.1276 - val_accuracy: 0.9800\n",
            "Epoch 692/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0566 - accuracy: 0.9800 - val_loss: 0.0879 - val_accuracy: 0.9800\n",
            "Epoch 693/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0476 - accuracy: 0.9837 - val_loss: 0.1166 - val_accuracy: 0.9833\n",
            "Epoch 694/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0567 - accuracy: 0.9804 - val_loss: 0.0941 - val_accuracy: 0.9867\n",
            "Epoch 695/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0509 - accuracy: 0.9796 - val_loss: 0.0881 - val_accuracy: 0.9867\n",
            "Epoch 696/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.1045 - val_accuracy: 0.9867\n",
            "Epoch 697/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0488 - accuracy: 0.9807 - val_loss: 0.0800 - val_accuracy: 0.9867\n",
            "Epoch 698/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.0699 - val_accuracy: 0.9833\n",
            "Epoch 699/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0505 - accuracy: 0.9822 - val_loss: 0.0969 - val_accuracy: 0.9800\n",
            "Epoch 700/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0422 - accuracy: 0.9870 - val_loss: 0.1172 - val_accuracy: 0.9833\n",
            "Epoch 701/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.1066 - val_accuracy: 0.9867\n",
            "Epoch 702/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0393 - accuracy: 0.9848 - val_loss: 0.0866 - val_accuracy: 0.9867\n",
            "Epoch 703/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0400 - accuracy: 0.9859 - val_loss: 0.1315 - val_accuracy: 0.9800\n",
            "Epoch 704/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0426 - accuracy: 0.9844 - val_loss: 0.1020 - val_accuracy: 0.9833\n",
            "Epoch 705/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0424 - accuracy: 0.9856 - val_loss: 0.1362 - val_accuracy: 0.9833\n",
            "Epoch 706/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0468 - accuracy: 0.9848 - val_loss: 0.1094 - val_accuracy: 0.9833\n",
            "Epoch 707/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.0787 - val_accuracy: 0.9833\n",
            "Epoch 708/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0494 - accuracy: 0.9841 - val_loss: 0.0812 - val_accuracy: 0.9867\n",
            "Epoch 709/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0524 - accuracy: 0.9841 - val_loss: 0.0829 - val_accuracy: 0.9833\n",
            "Epoch 710/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0380 - accuracy: 0.9874 - val_loss: 0.1502 - val_accuracy: 0.9767\n",
            "Epoch 711/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0397 - accuracy: 0.9852 - val_loss: 0.0877 - val_accuracy: 0.9800\n",
            "Epoch 712/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0555 - accuracy: 0.9815 - val_loss: 0.0706 - val_accuracy: 0.9867\n",
            "Epoch 713/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0489 - accuracy: 0.9859 - val_loss: 0.0794 - val_accuracy: 0.9867\n",
            "Epoch 714/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0432 - accuracy: 0.9848 - val_loss: 0.1056 - val_accuracy: 0.9867\n",
            "Epoch 715/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0424 - accuracy: 0.9848 - val_loss: 0.1164 - val_accuracy: 0.9867\n",
            "Epoch 716/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0541 - accuracy: 0.9852 - val_loss: 0.1146 - val_accuracy: 0.9867\n",
            "Epoch 717/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 0.1015 - val_accuracy: 0.9833\n",
            "Epoch 718/1000\n",
            "54/54 [==============================] - 27s 508ms/step - loss: 0.0531 - accuracy: 0.9830 - val_loss: 0.1233 - val_accuracy: 0.9833\n",
            "Epoch 719/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 0.1238 - val_accuracy: 0.9867\n",
            "Epoch 720/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0362 - accuracy: 0.9893 - val_loss: 0.0959 - val_accuracy: 0.9900\n",
            "Epoch 721/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0469 - accuracy: 0.9837 - val_loss: 0.0823 - val_accuracy: 0.9833\n",
            "Epoch 722/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0501 - accuracy: 0.9826 - val_loss: 0.0947 - val_accuracy: 0.9833\n",
            "Epoch 723/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0393 - accuracy: 0.9878 - val_loss: 0.1009 - val_accuracy: 0.9800\n",
            "Epoch 724/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0399 - accuracy: 0.9863 - val_loss: 0.0921 - val_accuracy: 0.9900\n",
            "Epoch 725/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0456 - accuracy: 0.9815 - val_loss: 0.0979 - val_accuracy: 0.9900\n",
            "Epoch 726/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0472 - accuracy: 0.9830 - val_loss: 0.1038 - val_accuracy: 0.9900\n",
            "Epoch 727/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0314 - accuracy: 0.9878 - val_loss: 0.1137 - val_accuracy: 0.9867\n",
            "Epoch 728/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0339 - accuracy: 0.9889 - val_loss: 0.1030 - val_accuracy: 0.9900\n",
            "Epoch 729/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0417 - accuracy: 0.9885 - val_loss: 0.0732 - val_accuracy: 0.9833\n",
            "Epoch 730/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0512 - accuracy: 0.9844 - val_loss: 0.0934 - val_accuracy: 0.9833\n",
            "Epoch 731/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.1054 - val_accuracy: 0.9867\n",
            "Epoch 732/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0488 - accuracy: 0.9841 - val_loss: 0.0942 - val_accuracy: 0.9833\n",
            "Epoch 733/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0390 - accuracy: 0.9881 - val_loss: 0.0801 - val_accuracy: 0.9900\n",
            "Epoch 734/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0427 - accuracy: 0.9863 - val_loss: 0.1063 - val_accuracy: 0.9900\n",
            "Epoch 735/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0378 - accuracy: 0.9867 - val_loss: 0.0894 - val_accuracy: 0.9867\n",
            "Epoch 736/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0447 - accuracy: 0.9837 - val_loss: 0.1109 - val_accuracy: 0.9900\n",
            "Epoch 737/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0389 - accuracy: 0.9885 - val_loss: 0.0985 - val_accuracy: 0.9867\n",
            "Epoch 738/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0501 - accuracy: 0.9837 - val_loss: 0.0905 - val_accuracy: 0.9867\n",
            "Epoch 739/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0387 - accuracy: 0.9889 - val_loss: 0.0908 - val_accuracy: 0.9833\n",
            "Epoch 740/1000\n",
            "54/54 [==============================] - 28s 525ms/step - loss: 0.0390 - accuracy: 0.9896 - val_loss: 0.0941 - val_accuracy: 0.9867\n",
            "Epoch 741/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0491 - accuracy: 0.9852 - val_loss: 0.0684 - val_accuracy: 0.9833\n",
            "Epoch 742/1000\n",
            "54/54 [==============================] - 28s 521ms/step - loss: 0.0393 - accuracy: 0.9878 - val_loss: 0.0976 - val_accuracy: 0.9867\n",
            "Epoch 743/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0449 - accuracy: 0.9867 - val_loss: 0.0760 - val_accuracy: 0.9833\n",
            "Epoch 744/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0479 - accuracy: 0.9844 - val_loss: 0.1077 - val_accuracy: 0.9900\n",
            "Epoch 745/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.0961 - val_accuracy: 0.9867\n",
            "Epoch 746/1000\n",
            "54/54 [==============================] - 28s 515ms/step - loss: 0.0417 - accuracy: 0.9852 - val_loss: 0.1154 - val_accuracy: 0.9867\n",
            "Epoch 747/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0423 - accuracy: 0.9859 - val_loss: 0.1002 - val_accuracy: 0.9867\n",
            "Epoch 748/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0434 - accuracy: 0.9863 - val_loss: 0.1038 - val_accuracy: 0.9867\n",
            "Epoch 749/1000\n",
            "54/54 [==============================] - 28s 518ms/step - loss: 0.0417 - accuracy: 0.9848 - val_loss: 0.1098 - val_accuracy: 0.9867\n",
            "Epoch 750/1000\n",
            "54/54 [==============================] - 28s 521ms/step - loss: 0.0409 - accuracy: 0.9867 - val_loss: 0.0933 - val_accuracy: 0.9833\n",
            "Epoch 751/1000\n",
            "54/54 [==============================] - 28s 523ms/step - loss: 0.0493 - accuracy: 0.9852 - val_loss: 0.0996 - val_accuracy: 0.9867\n",
            "Epoch 752/1000\n",
            "54/54 [==============================] - 28s 517ms/step - loss: 0.0475 - accuracy: 0.9856 - val_loss: 0.0810 - val_accuracy: 0.9867\n",
            "Epoch 753/1000\n",
            "54/54 [==============================] - 28s 521ms/step - loss: 0.0408 - accuracy: 0.9859 - val_loss: 0.1348 - val_accuracy: 0.9833\n",
            "Epoch 754/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0458 - accuracy: 0.9841 - val_loss: 0.0857 - val_accuracy: 0.9900\n",
            "Epoch 755/1000\n",
            "54/54 [==============================] - 28s 520ms/step - loss: 0.0407 - accuracy: 0.9852 - val_loss: 0.1251 - val_accuracy: 0.9833\n",
            "Epoch 756/1000\n",
            "54/54 [==============================] - 28s 519ms/step - loss: 0.0368 - accuracy: 0.9889 - val_loss: 0.1011 - val_accuracy: 0.9867\n",
            "Epoch 757/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.0977 - val_accuracy: 0.9867\n",
            "Epoch 758/1000\n",
            "54/54 [==============================] - 28s 512ms/step - loss: 0.0427 - accuracy: 0.9867 - val_loss: 0.0925 - val_accuracy: 0.9800\n",
            "Epoch 759/1000\n",
            "54/54 [==============================] - 28s 516ms/step - loss: 0.0461 - accuracy: 0.9841 - val_loss: 0.1242 - val_accuracy: 0.9833\n",
            "Epoch 760/1000\n",
            "54/54 [==============================] - 28s 510ms/step - loss: 0.0394 - accuracy: 0.9878 - val_loss: 0.0956 - val_accuracy: 0.9833\n",
            "Epoch 761/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0317 - accuracy: 0.9874 - val_loss: 0.1106 - val_accuracy: 0.9833\n",
            "Epoch 762/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0437 - accuracy: 0.9885 - val_loss: 0.0971 - val_accuracy: 0.9900\n",
            "Epoch 763/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0397 - accuracy: 0.9848 - val_loss: 0.0746 - val_accuracy: 0.9900\n",
            "Epoch 764/1000\n",
            "54/54 [==============================] - 28s 511ms/step - loss: 0.0388 - accuracy: 0.9867 - val_loss: 0.0825 - val_accuracy: 0.9867\n",
            "Epoch 765/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0460 - accuracy: 0.9852 - val_loss: 0.0937 - val_accuracy: 0.9867\n",
            "Epoch 766/1000\n",
            "54/54 [==============================] - 28s 513ms/step - loss: 0.0385 - accuracy: 0.9852 - val_loss: 0.1100 - val_accuracy: 0.9833\n",
            "Epoch 767/1000\n",
            "54/54 [==============================] - 28s 514ms/step - loss: 0.0392 - accuracy: 0.9863 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
            "Epoch 768/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0380 - accuracy: 0.9844 - val_loss: 0.0814 - val_accuracy: 0.9867\n",
            "Epoch 769/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0373 - accuracy: 0.9863 - val_loss: 0.1149 - val_accuracy: 0.9867\n",
            "Epoch 770/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0401 - accuracy: 0.9859 - val_loss: 0.1161 - val_accuracy: 0.9833\n",
            "Epoch 771/1000\n",
            "54/54 [==============================] - 27s 507ms/step - loss: 0.0406 - accuracy: 0.9874 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
            "Epoch 772/1000\n",
            "54/54 [==============================] - 27s 503ms/step - loss: 0.0579 - accuracy: 0.9844 - val_loss: 0.0752 - val_accuracy: 0.9833\n",
            "Epoch 773/1000\n",
            "54/54 [==============================] - 27s 509ms/step - loss: 0.0352 - accuracy: 0.9878 - val_loss: 0.0926 - val_accuracy: 0.9867\n",
            "Epoch 774/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0419 - accuracy: 0.9856 - val_loss: 0.1042 - val_accuracy: 0.9800\n",
            "Epoch 775/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0478 - accuracy: 0.9852 - val_loss: 0.0809 - val_accuracy: 0.9833\n",
            "Epoch 776/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0366 - accuracy: 0.9859 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
            "Epoch 777/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0441 - accuracy: 0.9863 - val_loss: 0.1049 - val_accuracy: 0.9900\n",
            "Epoch 778/1000\n",
            "54/54 [==============================] - 26s 490ms/step - loss: 0.0359 - accuracy: 0.9859 - val_loss: 0.1097 - val_accuracy: 0.9800\n",
            "Epoch 779/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.1225 - val_accuracy: 0.9833\n",
            "Epoch 780/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0323 - accuracy: 0.9878 - val_loss: 0.1109 - val_accuracy: 0.9867\n",
            "Epoch 781/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0392 - accuracy: 0.9852 - val_loss: 0.1026 - val_accuracy: 0.9833\n",
            "Epoch 782/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0415 - accuracy: 0.9856 - val_loss: 0.1113 - val_accuracy: 0.9867\n",
            "Epoch 783/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0363 - accuracy: 0.9870 - val_loss: 0.1131 - val_accuracy: 0.9833\n",
            "Epoch 784/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0389 - accuracy: 0.9859 - val_loss: 0.1148 - val_accuracy: 0.9833\n",
            "Epoch 785/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.0892 - val_accuracy: 0.9867\n",
            "Epoch 786/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.1140 - val_accuracy: 0.9867\n",
            "Epoch 787/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0480 - accuracy: 0.9811 - val_loss: 0.0979 - val_accuracy: 0.9867\n",
            "Epoch 788/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0403 - accuracy: 0.9878 - val_loss: 0.0847 - val_accuracy: 0.9900\n",
            "Epoch 789/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0411 - accuracy: 0.9844 - val_loss: 0.0902 - val_accuracy: 0.9867\n",
            "Epoch 790/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0386 - accuracy: 0.9867 - val_loss: 0.1062 - val_accuracy: 0.9867\n",
            "Epoch 791/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0423 - accuracy: 0.9881 - val_loss: 0.1095 - val_accuracy: 0.9833\n",
            "Epoch 792/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0356 - accuracy: 0.9878 - val_loss: 0.0986 - val_accuracy: 0.9900\n",
            "Epoch 793/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0449 - accuracy: 0.9856 - val_loss: 0.0915 - val_accuracy: 0.9867\n",
            "Epoch 794/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0413 - accuracy: 0.9878 - val_loss: 0.1012 - val_accuracy: 0.9867\n",
            "Epoch 795/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0359 - accuracy: 0.9867 - val_loss: 0.1149 - val_accuracy: 0.9833\n",
            "Epoch 796/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0367 - accuracy: 0.9848 - val_loss: 0.1182 - val_accuracy: 0.9800\n",
            "Epoch 797/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0379 - accuracy: 0.9878 - val_loss: 0.1431 - val_accuracy: 0.9800\n",
            "Epoch 798/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0838 - val_accuracy: 0.9833\n",
            "Epoch 799/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0380 - accuracy: 0.9867 - val_loss: 0.1034 - val_accuracy: 0.9800\n",
            "Epoch 800/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0420 - accuracy: 0.9881 - val_loss: 0.1013 - val_accuracy: 0.9900\n",
            "Epoch 801/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0369 - accuracy: 0.9848 - val_loss: 0.1509 - val_accuracy: 0.9867\n",
            "Epoch 802/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0494 - accuracy: 0.9837 - val_loss: 0.1064 - val_accuracy: 0.9833\n",
            "Epoch 803/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0511 - accuracy: 0.9841 - val_loss: 0.1100 - val_accuracy: 0.9833\n",
            "Epoch 804/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0381 - accuracy: 0.9848 - val_loss: 0.1215 - val_accuracy: 0.9800\n",
            "Epoch 805/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0276 - accuracy: 0.9915 - val_loss: 0.1203 - val_accuracy: 0.9833\n",
            "Epoch 806/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0336 - accuracy: 0.9867 - val_loss: 0.1143 - val_accuracy: 0.9800\n",
            "Epoch 807/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0371 - accuracy: 0.9889 - val_loss: 0.0850 - val_accuracy: 0.9867\n",
            "Epoch 808/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0374 - accuracy: 0.9863 - val_loss: 0.0921 - val_accuracy: 0.9800\n",
            "Epoch 809/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0293 - accuracy: 0.9907 - val_loss: 0.1211 - val_accuracy: 0.9867\n",
            "Epoch 810/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0413 - accuracy: 0.9859 - val_loss: 0.1038 - val_accuracy: 0.9833\n",
            "Epoch 811/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.0867 - val_accuracy: 0.9867\n",
            "Epoch 812/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0308 - accuracy: 0.9911 - val_loss: 0.0910 - val_accuracy: 0.9867\n",
            "Epoch 813/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0549 - accuracy: 0.9852 - val_loss: 0.0888 - val_accuracy: 0.9833\n",
            "Epoch 814/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0439 - accuracy: 0.9841 - val_loss: 0.1150 - val_accuracy: 0.9833\n",
            "Epoch 815/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0507 - accuracy: 0.9826 - val_loss: 0.0745 - val_accuracy: 0.9900\n",
            "Epoch 816/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0388 - accuracy: 0.9856 - val_loss: 0.0963 - val_accuracy: 0.9867\n",
            "Epoch 817/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0500 - accuracy: 0.9837 - val_loss: 0.1032 - val_accuracy: 0.9900\n",
            "Epoch 818/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0480 - accuracy: 0.9807 - val_loss: 0.0967 - val_accuracy: 0.9833\n",
            "Epoch 819/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0363 - accuracy: 0.9856 - val_loss: 0.1250 - val_accuracy: 0.9800\n",
            "Epoch 820/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0327 - accuracy: 0.9885 - val_loss: 0.0912 - val_accuracy: 0.9833\n",
            "Epoch 821/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0347 - accuracy: 0.9889 - val_loss: 0.0993 - val_accuracy: 0.9833\n",
            "Epoch 822/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0419 - accuracy: 0.9885 - val_loss: 0.1007 - val_accuracy: 0.9900\n",
            "Epoch 823/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0331 - accuracy: 0.9915 - val_loss: 0.1041 - val_accuracy: 0.9867\n",
            "Epoch 824/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0377 - accuracy: 0.9856 - val_loss: 0.0847 - val_accuracy: 0.9867\n",
            "Epoch 825/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0294 - accuracy: 0.9881 - val_loss: 0.0973 - val_accuracy: 0.9833\n",
            "Epoch 826/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0347 - accuracy: 0.9881 - val_loss: 0.1006 - val_accuracy: 0.9833\n",
            "Epoch 827/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0504 - accuracy: 0.9826 - val_loss: 0.1177 - val_accuracy: 0.9833\n",
            "Epoch 828/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0322 - accuracy: 0.9878 - val_loss: 0.0983 - val_accuracy: 0.9900\n",
            "Epoch 829/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.1051 - val_accuracy: 0.9833\n",
            "Epoch 830/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0322 - accuracy: 0.9893 - val_loss: 0.1130 - val_accuracy: 0.9867\n",
            "Epoch 831/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0343 - accuracy: 0.9863 - val_loss: 0.1083 - val_accuracy: 0.9900\n",
            "Epoch 832/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0405 - accuracy: 0.9867 - val_loss: 0.1130 - val_accuracy: 0.9833\n",
            "Epoch 833/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0378 - accuracy: 0.9844 - val_loss: 0.1323 - val_accuracy: 0.9867\n",
            "Epoch 834/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0433 - accuracy: 0.9841 - val_loss: 0.1218 - val_accuracy: 0.9867\n",
            "Epoch 835/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0406 - accuracy: 0.9881 - val_loss: 0.1022 - val_accuracy: 0.9800\n",
            "Epoch 836/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0359 - accuracy: 0.9881 - val_loss: 0.0978 - val_accuracy: 0.9867\n",
            "Epoch 837/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0373 - accuracy: 0.9893 - val_loss: 0.1158 - val_accuracy: 0.9833\n",
            "Epoch 838/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0378 - accuracy: 0.9885 - val_loss: 0.1163 - val_accuracy: 0.9833\n",
            "Epoch 839/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0385 - accuracy: 0.9893 - val_loss: 0.1109 - val_accuracy: 0.9833\n",
            "Epoch 840/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0370 - accuracy: 0.9874 - val_loss: 0.1143 - val_accuracy: 0.9900\n",
            "Epoch 841/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0332 - accuracy: 0.9885 - val_loss: 0.0949 - val_accuracy: 0.9900\n",
            "Epoch 842/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0306 - accuracy: 0.9896 - val_loss: 0.0948 - val_accuracy: 0.9867\n",
            "Epoch 843/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.0891 - val_accuracy: 0.9900\n",
            "Epoch 844/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0346 - accuracy: 0.9881 - val_loss: 0.1071 - val_accuracy: 0.9900\n",
            "Epoch 845/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0516 - accuracy: 0.9856 - val_loss: 0.1029 - val_accuracy: 0.9867\n",
            "Epoch 846/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0478 - accuracy: 0.9848 - val_loss: 0.0776 - val_accuracy: 0.9867\n",
            "Epoch 847/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0343 - accuracy: 0.9837 - val_loss: 0.1020 - val_accuracy: 0.9900\n",
            "Epoch 848/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0429 - accuracy: 0.9833 - val_loss: 0.1181 - val_accuracy: 0.9867\n",
            "Epoch 849/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0456 - accuracy: 0.9837 - val_loss: 0.1124 - val_accuracy: 0.9833\n",
            "Epoch 850/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0433 - accuracy: 0.9863 - val_loss: 0.0967 - val_accuracy: 0.9833\n",
            "Epoch 851/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0452 - accuracy: 0.9874 - val_loss: 0.0887 - val_accuracy: 0.9833\n",
            "Epoch 852/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0429 - accuracy: 0.9837 - val_loss: 0.0983 - val_accuracy: 0.9833\n",
            "Epoch 853/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.0981 - val_accuracy: 0.9867\n",
            "Epoch 854/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0431 - accuracy: 0.9841 - val_loss: 0.0877 - val_accuracy: 0.9833\n",
            "Epoch 855/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0362 - accuracy: 0.9893 - val_loss: 0.1004 - val_accuracy: 0.9800\n",
            "Epoch 856/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0302 - accuracy: 0.9896 - val_loss: 0.1039 - val_accuracy: 0.9833\n",
            "Epoch 857/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0366 - accuracy: 0.9881 - val_loss: 0.1144 - val_accuracy: 0.9833\n",
            "Epoch 858/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0353 - accuracy: 0.9870 - val_loss: 0.1058 - val_accuracy: 0.9833\n",
            "Epoch 859/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0426 - accuracy: 0.9870 - val_loss: 0.0998 - val_accuracy: 0.9833\n",
            "Epoch 860/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0329 - accuracy: 0.9881 - val_loss: 0.1034 - val_accuracy: 0.9800\n",
            "Epoch 861/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0399 - accuracy: 0.9856 - val_loss: 0.1010 - val_accuracy: 0.9867\n",
            "Epoch 862/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0350 - accuracy: 0.9889 - val_loss: 0.0963 - val_accuracy: 0.9833\n",
            "Epoch 863/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0353 - accuracy: 0.9893 - val_loss: 0.1074 - val_accuracy: 0.9833\n",
            "Epoch 864/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.0972 - val_accuracy: 0.9867\n",
            "Epoch 865/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0347 - accuracy: 0.9863 - val_loss: 0.0702 - val_accuracy: 0.9900\n",
            "Epoch 866/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0404 - accuracy: 0.9863 - val_loss: 0.0887 - val_accuracy: 0.9833\n",
            "Epoch 867/1000\n",
            "54/54 [==============================] - 27s 493ms/step - loss: 0.0305 - accuracy: 0.9881 - val_loss: 0.1107 - val_accuracy: 0.9867\n",
            "Epoch 868/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0440 - accuracy: 0.9878 - val_loss: 0.1085 - val_accuracy: 0.9867\n",
            "Epoch 869/1000\n",
            "54/54 [==============================] - 26s 491ms/step - loss: 0.0399 - accuracy: 0.9878 - val_loss: 0.0659 - val_accuracy: 0.9833\n",
            "Epoch 870/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0379 - accuracy: 0.9870 - val_loss: 0.0682 - val_accuracy: 0.9900\n",
            "Epoch 871/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0380 - accuracy: 0.9874 - val_loss: 0.0785 - val_accuracy: 0.9900\n",
            "Epoch 872/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0381 - accuracy: 0.9863 - val_loss: 0.0820 - val_accuracy: 0.9900\n",
            "Epoch 873/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0355 - accuracy: 0.9852 - val_loss: 0.0914 - val_accuracy: 0.9867\n",
            "Epoch 874/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0434 - accuracy: 0.9852 - val_loss: 0.0831 - val_accuracy: 0.9833\n",
            "Epoch 875/1000\n",
            "54/54 [==============================] - 27s 497ms/step - loss: 0.0445 - accuracy: 0.9852 - val_loss: 0.0953 - val_accuracy: 0.9900\n",
            "Epoch 876/1000\n",
            "54/54 [==============================] - 27s 496ms/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.0857 - val_accuracy: 0.9867\n",
            "Epoch 877/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0568 - accuracy: 0.9800 - val_loss: 0.0638 - val_accuracy: 0.9933\n",
            "Epoch 878/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0432 - accuracy: 0.9833 - val_loss: 0.0723 - val_accuracy: 0.9900\n",
            "Epoch 879/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0400 - accuracy: 0.9870 - val_loss: 0.0997 - val_accuracy: 0.9867\n",
            "Epoch 880/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0384 - accuracy: 0.9874 - val_loss: 0.0849 - val_accuracy: 0.9900\n",
            "Epoch 881/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.0770 - val_accuracy: 0.9867\n",
            "Epoch 882/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0438 - accuracy: 0.9848 - val_loss: 0.0769 - val_accuracy: 0.9833\n",
            "Epoch 883/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0378 - accuracy: 0.9848 - val_loss: 0.0693 - val_accuracy: 0.9867\n",
            "Epoch 884/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0278 - accuracy: 0.9926 - val_loss: 0.0877 - val_accuracy: 0.9900\n",
            "Epoch 885/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0325 - accuracy: 0.9900 - val_loss: 0.0910 - val_accuracy: 0.9867\n",
            "Epoch 886/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0377 - accuracy: 0.9878 - val_loss: 0.1025 - val_accuracy: 0.9867\n",
            "Epoch 887/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0451 - accuracy: 0.9841 - val_loss: 0.1073 - val_accuracy: 0.9867\n",
            "Epoch 888/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0406 - accuracy: 0.9863 - val_loss: 0.1010 - val_accuracy: 0.9900\n",
            "Epoch 889/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0347 - accuracy: 0.9856 - val_loss: 0.0835 - val_accuracy: 0.9867\n",
            "Epoch 890/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0446 - accuracy: 0.9870 - val_loss: 0.0872 - val_accuracy: 0.9833\n",
            "Epoch 891/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0338 - accuracy: 0.9915 - val_loss: 0.0878 - val_accuracy: 0.9833\n",
            "Epoch 892/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0249 - accuracy: 0.9915 - val_loss: 0.0975 - val_accuracy: 0.9833\n",
            "Epoch 893/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0426 - accuracy: 0.9878 - val_loss: 0.0900 - val_accuracy: 0.9900\n",
            "Epoch 894/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.1141 - val_accuracy: 0.9867\n",
            "Epoch 895/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0379 - accuracy: 0.9889 - val_loss: 0.1128 - val_accuracy: 0.9867\n",
            "Epoch 896/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0400 - accuracy: 0.9863 - val_loss: 0.0893 - val_accuracy: 0.9900\n",
            "Epoch 897/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0410 - accuracy: 0.9837 - val_loss: 0.0799 - val_accuracy: 0.9900\n",
            "Epoch 898/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0354 - accuracy: 0.9867 - val_loss: 0.0849 - val_accuracy: 0.9900\n",
            "Epoch 899/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0284 - accuracy: 0.9926 - val_loss: 0.1077 - val_accuracy: 0.9867\n",
            "Epoch 900/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0359 - accuracy: 0.9881 - val_loss: 0.1036 - val_accuracy: 0.9833\n",
            "Epoch 901/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0337 - accuracy: 0.9878 - val_loss: 0.1005 - val_accuracy: 0.9833\n",
            "Epoch 902/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0411 - accuracy: 0.9848 - val_loss: 0.1062 - val_accuracy: 0.9900\n",
            "Epoch 903/1000\n",
            "54/54 [==============================] - 26s 477ms/step - loss: 0.0503 - accuracy: 0.9848 - val_loss: 0.0844 - val_accuracy: 0.9867\n",
            "Epoch 904/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0407 - accuracy: 0.9852 - val_loss: 0.0963 - val_accuracy: 0.9867\n",
            "Epoch 905/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 0.0740 - val_accuracy: 0.9867\n",
            "Epoch 906/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0393 - accuracy: 0.9837 - val_loss: 0.0965 - val_accuracy: 0.9833\n",
            "Epoch 907/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0472 - accuracy: 0.9822 - val_loss: 0.1081 - val_accuracy: 0.9867\n",
            "Epoch 908/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.1130 - val_accuracy: 0.9800\n",
            "Epoch 909/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.1166 - val_accuracy: 0.9833\n",
            "Epoch 910/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.1187 - val_accuracy: 0.9833\n",
            "Epoch 911/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0393 - accuracy: 0.9885 - val_loss: 0.1007 - val_accuracy: 0.9833\n",
            "Epoch 912/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0448 - accuracy: 0.9870 - val_loss: 0.0922 - val_accuracy: 0.9833\n",
            "Epoch 913/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0573 - accuracy: 0.9793 - val_loss: 0.1222 - val_accuracy: 0.9767\n",
            "Epoch 914/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0432 - accuracy: 0.9867 - val_loss: 0.0753 - val_accuracy: 0.9867\n",
            "Epoch 915/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0415 - accuracy: 0.9878 - val_loss: 0.0752 - val_accuracy: 0.9867\n",
            "Epoch 916/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0326 - accuracy: 0.9889 - val_loss: 0.0767 - val_accuracy: 0.9867\n",
            "Epoch 917/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0247 - accuracy: 0.9937 - val_loss: 0.0905 - val_accuracy: 0.9900\n",
            "Epoch 918/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0306 - accuracy: 0.9889 - val_loss: 0.0848 - val_accuracy: 0.9867\n",
            "Epoch 919/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0303 - accuracy: 0.9896 - val_loss: 0.0825 - val_accuracy: 0.9833\n",
            "Epoch 920/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.1055 - val_accuracy: 0.9833\n",
            "Epoch 921/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0374 - accuracy: 0.9867 - val_loss: 0.1040 - val_accuracy: 0.9867\n",
            "Epoch 922/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0444 - accuracy: 0.9870 - val_loss: 0.0720 - val_accuracy: 0.9900\n",
            "Epoch 923/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0292 - accuracy: 0.9889 - val_loss: 0.0876 - val_accuracy: 0.9900\n",
            "Epoch 924/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0474 - accuracy: 0.9830 - val_loss: 0.0771 - val_accuracy: 0.9867\n",
            "Epoch 925/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0354 - accuracy: 0.9881 - val_loss: 0.0810 - val_accuracy: 0.9867\n",
            "Epoch 926/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0846 - val_accuracy: 0.9833\n",
            "Epoch 927/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0315 - accuracy: 0.9885 - val_loss: 0.0952 - val_accuracy: 0.9833\n",
            "Epoch 928/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0330 - accuracy: 0.9878 - val_loss: 0.0823 - val_accuracy: 0.9833\n",
            "Epoch 929/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0417 - accuracy: 0.9867 - val_loss: 0.0884 - val_accuracy: 0.9833\n",
            "Epoch 930/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0275 - accuracy: 0.9904 - val_loss: 0.1061 - val_accuracy: 0.9900\n",
            "Epoch 931/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 0.0801 - val_accuracy: 0.9867\n",
            "Epoch 932/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0304 - accuracy: 0.9922 - val_loss: 0.0982 - val_accuracy: 0.9833\n",
            "Epoch 933/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0435 - accuracy: 0.9859 - val_loss: 0.0971 - val_accuracy: 0.9833\n",
            "Epoch 934/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0429 - accuracy: 0.9859 - val_loss: 0.1050 - val_accuracy: 0.9900\n",
            "Epoch 935/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0371 - accuracy: 0.9867 - val_loss: 0.1059 - val_accuracy: 0.9833\n",
            "Epoch 936/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0307 - accuracy: 0.9885 - val_loss: 0.1290 - val_accuracy: 0.9867\n",
            "Epoch 937/1000\n",
            "54/54 [==============================] - 27s 498ms/step - loss: 0.0362 - accuracy: 0.9863 - val_loss: 0.1323 - val_accuracy: 0.9900\n",
            "Epoch 938/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0440 - accuracy: 0.9830 - val_loss: 0.1357 - val_accuracy: 0.9867\n",
            "Epoch 939/1000\n",
            "54/54 [==============================] - 27s 492ms/step - loss: 0.0404 - accuracy: 0.9852 - val_loss: 0.1146 - val_accuracy: 0.9833\n",
            "Epoch 940/1000\n",
            "54/54 [==============================] - 27s 499ms/step - loss: 0.0372 - accuracy: 0.9889 - val_loss: 0.1089 - val_accuracy: 0.9833\n",
            "Epoch 941/1000\n",
            "54/54 [==============================] - 27s 500ms/step - loss: 0.0314 - accuracy: 0.9889 - val_loss: 0.1069 - val_accuracy: 0.9833\n",
            "Epoch 942/1000\n",
            "54/54 [==============================] - 27s 502ms/step - loss: 0.0282 - accuracy: 0.9919 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
            "Epoch 943/1000\n",
            "54/54 [==============================] - 27s 506ms/step - loss: 0.0447 - accuracy: 0.9837 - val_loss: 0.0860 - val_accuracy: 0.9833\n",
            "Epoch 944/1000\n",
            "54/54 [==============================] - 27s 505ms/step - loss: 0.0337 - accuracy: 0.9867 - val_loss: 0.0816 - val_accuracy: 0.9833\n",
            "Epoch 945/1000\n",
            "54/54 [==============================] - 27s 501ms/step - loss: 0.0307 - accuracy: 0.9881 - val_loss: 0.0905 - val_accuracy: 0.9833\n",
            "Epoch 946/1000\n",
            "54/54 [==============================] - 27s 504ms/step - loss: 0.0354 - accuracy: 0.9870 - val_loss: 0.1006 - val_accuracy: 0.9867\n",
            "Epoch 947/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0290 - accuracy: 0.9900 - val_loss: 0.1111 - val_accuracy: 0.9867\n",
            "Epoch 948/1000\n",
            "54/54 [==============================] - 26s 489ms/step - loss: 0.0383 - accuracy: 0.9874 - val_loss: 0.0875 - val_accuracy: 0.9867\n",
            "Epoch 949/1000\n",
            "54/54 [==============================] - 27s 494ms/step - loss: 0.0463 - accuracy: 0.9822 - val_loss: 0.0882 - val_accuracy: 0.9867\n",
            "Epoch 950/1000\n",
            "54/54 [==============================] - 27s 491ms/step - loss: 0.0357 - accuracy: 0.9859 - val_loss: 0.1017 - val_accuracy: 0.9867\n",
            "Epoch 951/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.1138 - val_accuracy: 0.9867\n",
            "Epoch 952/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0369 - accuracy: 0.9844 - val_loss: 0.1287 - val_accuracy: 0.9867\n",
            "Epoch 953/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0360 - accuracy: 0.9874 - val_loss: 0.1172 - val_accuracy: 0.9867\n",
            "Epoch 954/1000\n",
            "54/54 [==============================] - 26s 488ms/step - loss: 0.0396 - accuracy: 0.9859 - val_loss: 0.1086 - val_accuracy: 0.9800\n",
            "Epoch 955/1000\n",
            "54/54 [==============================] - 26s 484ms/step - loss: 0.0400 - accuracy: 0.9874 - val_loss: 0.1147 - val_accuracy: 0.9867\n",
            "Epoch 956/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0390 - accuracy: 0.9889 - val_loss: 0.1226 - val_accuracy: 0.9800\n",
            "Epoch 957/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.1064 - val_accuracy: 0.9900\n",
            "Epoch 958/1000\n",
            "54/54 [==============================] - 26s 487ms/step - loss: 0.0471 - accuracy: 0.9837 - val_loss: 0.1066 - val_accuracy: 0.9800\n",
            "Epoch 959/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0376 - accuracy: 0.9870 - val_loss: 0.0950 - val_accuracy: 0.9833\n",
            "Epoch 960/1000\n",
            "54/54 [==============================] - 26s 486ms/step - loss: 0.0295 - accuracy: 0.9904 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
            "Epoch 961/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0338 - accuracy: 0.9885 - val_loss: 0.1021 - val_accuracy: 0.9833\n",
            "Epoch 962/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0441 - accuracy: 0.9867 - val_loss: 0.1375 - val_accuracy: 0.9800\n",
            "Epoch 963/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0299 - accuracy: 0.9893 - val_loss: 0.0833 - val_accuracy: 0.9867\n",
            "Epoch 964/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0373 - accuracy: 0.9893 - val_loss: 0.0994 - val_accuracy: 0.9800\n",
            "Epoch 965/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0342 - accuracy: 0.9874 - val_loss: 0.1114 - val_accuracy: 0.9900\n",
            "Epoch 966/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0451 - accuracy: 0.9837 - val_loss: 0.1028 - val_accuracy: 0.9833\n",
            "Epoch 967/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0357 - accuracy: 0.9874 - val_loss: 0.0977 - val_accuracy: 0.9833\n",
            "Epoch 968/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
            "Epoch 969/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0262 - accuracy: 0.9904 - val_loss: 0.1095 - val_accuracy: 0.9833\n",
            "Epoch 970/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0339 - accuracy: 0.9881 - val_loss: 0.0946 - val_accuracy: 0.9833\n",
            "Epoch 971/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0278 - accuracy: 0.9904 - val_loss: 0.1008 - val_accuracy: 0.9833\n",
            "Epoch 972/1000\n",
            "54/54 [==============================] - 26s 485ms/step - loss: 0.0393 - accuracy: 0.9881 - val_loss: 0.0908 - val_accuracy: 0.9867\n",
            "Epoch 973/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0322 - accuracy: 0.9900 - val_loss: 0.0980 - val_accuracy: 0.9833\n",
            "Epoch 974/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0348 - accuracy: 0.9885 - val_loss: 0.0885 - val_accuracy: 0.9867\n",
            "Epoch 975/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0322 - accuracy: 0.9919 - val_loss: 0.0885 - val_accuracy: 0.9867\n",
            "Epoch 976/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0311 - accuracy: 0.9885 - val_loss: 0.1066 - val_accuracy: 0.9833\n",
            "Epoch 977/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0392 - accuracy: 0.9907 - val_loss: 0.0842 - val_accuracy: 0.9833\n",
            "Epoch 978/1000\n",
            "54/54 [==============================] - 26s 476ms/step - loss: 0.0366 - accuracy: 0.9896 - val_loss: 0.1071 - val_accuracy: 0.9833\n",
            "Epoch 979/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0432 - accuracy: 0.9837 - val_loss: 0.1072 - val_accuracy: 0.9833\n",
            "Epoch 980/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0437 - accuracy: 0.9837 - val_loss: 0.0930 - val_accuracy: 0.9833\n",
            "Epoch 981/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0335 - accuracy: 0.9885 - val_loss: 0.0935 - val_accuracy: 0.9833\n",
            "Epoch 982/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0369 - accuracy: 0.9885 - val_loss: 0.0968 - val_accuracy: 0.9833\n",
            "Epoch 983/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0436 - accuracy: 0.9874 - val_loss: 0.0911 - val_accuracy: 0.9833\n",
            "Epoch 984/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0347 - accuracy: 0.9870 - val_loss: 0.0843 - val_accuracy: 0.9833\n",
            "Epoch 985/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0333 - accuracy: 0.9863 - val_loss: 0.0802 - val_accuracy: 0.9867\n",
            "Epoch 986/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0289 - accuracy: 0.9893 - val_loss: 0.0949 - val_accuracy: 0.9833\n",
            "Epoch 987/1000\n",
            "54/54 [==============================] - 26s 478ms/step - loss: 0.0392 - accuracy: 0.9874 - val_loss: 0.0912 - val_accuracy: 0.9833\n",
            "Epoch 988/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0482 - accuracy: 0.9856 - val_loss: 0.1085 - val_accuracy: 0.9800\n",
            "Epoch 989/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0452 - accuracy: 0.9844 - val_loss: 0.1021 - val_accuracy: 0.9833\n",
            "Epoch 990/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0418 - accuracy: 0.9856 - val_loss: 0.0906 - val_accuracy: 0.9833\n",
            "Epoch 991/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0388 - accuracy: 0.9859 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
            "Epoch 992/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.0913 - val_accuracy: 0.9867\n",
            "Epoch 993/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0370 - accuracy: 0.9859 - val_loss: 0.1103 - val_accuracy: 0.9867\n",
            "Epoch 994/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0316 - accuracy: 0.9856 - val_loss: 0.1045 - val_accuracy: 0.9833\n",
            "Epoch 995/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0406 - accuracy: 0.9870 - val_loss: 0.1119 - val_accuracy: 0.9867\n",
            "Epoch 996/1000\n",
            "54/54 [==============================] - 26s 479ms/step - loss: 0.0288 - accuracy: 0.9900 - val_loss: 0.1089 - val_accuracy: 0.9833\n",
            "Epoch 997/1000\n",
            "54/54 [==============================] - 26s 480ms/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 0.1017 - val_accuracy: 0.9833\n",
            "Epoch 998/1000\n",
            "54/54 [==============================] - 26s 483ms/step - loss: 0.0439 - accuracy: 0.9867 - val_loss: 0.0994 - val_accuracy: 0.9833\n",
            "Epoch 999/1000\n",
            "54/54 [==============================] - 26s 481ms/step - loss: 0.0430 - accuracy: 0.9844 - val_loss: 0.0794 - val_accuracy: 0.9867\n",
            "Epoch 1000/1000\n",
            "54/54 [==============================] - 26s 482ms/step - loss: 0.0338 - accuracy: 0.9881 - val_loss: 0.0941 - val_accuracy: 0.9867\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.99      0.98      0.99       150\n",
            "         yes       0.98      0.99      0.99       150\n",
            "\n",
            "    accuracy                           0.99       300\n",
            "   macro avg       0.99      0.99      0.99       300\n",
            "weighted avg       0.99      0.99      0.99       300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvNuAz0j2Dbz"
      },
      "source": [
        "#Saving the model that has been made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxeClOJ_z6c6",
        "outputId": "94871132-1489-44ac-c3f0-8ded85d3549c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"[INFO] saving mask detector model...\")\n",
        "model.save(\"tumor_detector.model\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] saving mask detector model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLkurcsOTOf9"
      },
      "source": [
        "# Plotting the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDkyy6391cWz",
        "outputId": "fa79fd65-3e65-4540-c407-15754a84ce63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"plot.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeXRV1dn48e+5506ZyRwgCUqYBBSFMEqZERXBCbVacQCtir/a1/Zdtlh80eUADrS2VFtUxNmqRa2oqMxDAUXCICDKGAKEhMzznc7+/XGTSy6ZbiADcJ/PWixyz/icQzjP3XufvbemlFIIIYQQgKm9AxBCCHH2kKQghBDCR5KCEEIIH0kKQgghfCQpCCGE8JGkIIQQwkeSggjY6tWr0TSNI0eONGs/TdN45513Wimq4DVq1Cjuueee9g5DnGckKZyHNE1r9M8FF1xwWscdNmwY2dnZdOrUqVn7ZWdnM2XKlNM6Z3NJAqrfAw88gK7rvPTSS+0dijjLSVI4D2VnZ/v+LF68GICMjAzfss2bN/tt73Q6Azqu1WolKSkJk6l5vzZJSUnY7fZm7SNaTnl5Oe+++y6PPvoor776anuHAwT+OyfaniSF81BSUpLvT0xMDADx8fG+ZQkJCfztb3/jtttuIyoqiqlTpwLwpz/9iYsuuojQ0FBSUlK4//77KS4u9h331Oqjms/Lli1jxIgRhIaG0rt3b5YuXeoXz6nf3jVN4+WXX2bq1KlERESQnJzMnDlz/PbJz8/npptuIiwsjMTERB577DHuvPNOxo0bd0b35s0336R3795YrVaSk5OZNWsWbrfbt379+vVcfvnlREREEBERQb9+/fj6669965955hm6du2KzWYjPj6eCRMmUFlZ2eD53nvvPQYPHkxUVBRxcXFMnDiRn3/+2bf+0KFDaJrGhx9+yDXXXENoaChdu3bljTfe8DtOZmYmV155JSEhIaSkpDB//vyAr/n999+ne/fuzJo1i8zMTL799ts623zwwQcMGDAAu91ObGwsV111FYWFhb71L730Er1798Zms5GQkMCNN97oW3fBBRfw1FNP+R3vnnvuYdSoUb7Po0aNYvr06Tz22GN07NiR1NTUgO4PQG5uLnfffTeJiYnY7XZ69uzJ66+/jlKKrl278swzz/htX15eTmRkJG+//XbA90icJEkhSD3xxBMMGzaMjIwM33/okJAQXnnlFXbv3s0bb7zB6tWreeihh5o81v/+7//y6KOPsn37dgYPHswtt9zi90Bp6PwjRoxg27ZtzJw5k0cffZQVK1b41t99991s376dzz//nJUrV3LkyBE+/fTTM7rmL774gmnTpjF16lR27tzJvHnzeOmll3jiiScAcLvdTJ48mcGDB5ORkUFGRgaPP/44oaGhAHz88cfMnTuXv/71r+zdu5dly5Zx1VVXNXpOh8PBrFmzyMjIYNmyZei6zsSJE+t8U/7jH//IHXfcwY4dO/jlL3/JPffc43s4KqW4/vrryc/PZ/Xq1SxZsoTPPvuMjIyMgK57wYIF3HXXXdhsNn75y1+yYMECv/WLFi3i9ttv57rrriMjI4NVq1Zx5ZVX4vF4AJg9ezZ/+MMfmDFjBj/88ANfffUV/fv3D+jctX344YecOHGCFStWsGzZsoDuT2VlJSNHjmT79u28++677N69m/nz5xMaGoqmadx7770sXLiQ2qP1/Otf/8JsNnPTTTc1O0YBKHFeW7VqlQJUVlaWbxmgpk2b1uS+H3/8sbJarcrj8dR7rJrPixcv9u1z/PhxBaivvvrK73xvv/223+ff/OY3fufq1auX+uMf/6iUUurnn39WgFq+fLlvvdPpVMnJyWrs2LGNxnzquWobPny4uummm/yWvfjii8putyuHw6EKCgoUoFatWlXv/n/+859V9+7dldPpbDSGxuTn5ytArV+/Ximl1MGDBxWg5s2b59vG7Xar8PBw9c9//lMppdSyZcsUoH766SffNrm5ucput6vp06c3er6tW7cqq9Wq8vLylFJKbdy4UYWGhqqioiLfNikpKerBBx+sd/+ysjJlt9vV888/3+A5unTpop588km/ZdOnT1cjR470fR45cqTq3r2773epIafen9dee03ZbDa/39/ajh8/riwWi1q2bJlv2ZAhQ9RDDz3U6HlEw6SkEKQGDRpUZ9nHH3/MiBEj6NSpE+Hh4fzqV7/C6XRy/PjxRo916aWX+n5OTExE13VycnIC3gegU6dOvn12794NwJAhQ3zrLRYL6enpjV9UE3bt2sWIESP8lo0cOZKqqir2799PdHQ099xzDxMmTOCqq65i7ty5/PTTT75tb775ZlwuF126dOGuu+7i7bffprS0tNFzbtu2jeuvv54LL7yQiIgIX7VJZmam33a174eu6yQkJPjdj7i4OHr06OHbJj4+np49ezZ5zQsWLOCaa64hNjYW8N7T5ORkX3Vebm4uWVlZXHHFFfXuv2vXLqqqqhpc3xwDBgyo0x7V1P3ZsmULvXv3Jjk5ud5jJiYmcu211/raSnbu3MmmTZu49957zzjeYCVJIUiFhYX5ff7222+56aabGDFiBJ988gkZGRn885//BJpuFLRarXWWGYbRrH00Tauzj6ZpjR6jNbz66qts2bKF8ePHs2bNGvr27eurbuncuTN79uzh9ddfJyEhgSeffJKePXuSlZVV77EqKiq44oor0DSNRYsW8d1337F582Y0TatzTwO5H81V08D86aefYjabfX/27t3bog3OJpPJr/oGwOVy1dnu1N+55tyfxtx///18+umn5OXl8dprrzF06FD69u17ehcjJCkIr/Xr1xMXF8dTTz3F4MGD6dGjR7P7I7SU3r17A7Bx40bfMrfbzZYtW87ouH369GHt2rV+y9asWUNISAhpaWm+ZX379uV3v/sdS5cuZfr06bzyyiu+dTabjSuvvJLnnnuOH374gYqKigbbOn788UdOnDjB008/zahRo7jooosoLCys8wBtSu/evcnLy2Pv3r2+ZXl5eX6lmPq8//77mM1mtm3b5vdn9erV7Nixg2+//ZaEhASSk5P55ptvGjy33W5vcD1AQkICx44d81u2devWJq8rkPszYMAAdu/e3ejv4pgxY0hNTWXBggW8/fbbUko4Q+b2DkCcHXr27MmJEydYuHAho0ePZv369bz88svtEkv37t2ZNGkSDz74IAsWLCA+Pp558+ZRUlISUOnh8OHDbNu2zW9Zp06dmDlzJpMmTWLu3LnccMMNbNu2jccff5zf//73WK1W9u3bx6uvvsqkSZNISUnh2LFjrFu3zteounDhQgzDYNCgQXTo0IEVK1ZQWlrqS2Kn6tKlCzabjfnz5/P73/+eQ4cO8cc//rHZJaCxY8fSr18/br/9dubPn4/VauUPf/gDFoul0f0WLFjA9ddfz8UXX1xn3ZAhQ1iwYAGDBw9m9uzZPPDAAyQmJjJlyhQMw2DVqlX88pe/JC4ujt///vc8/vjjhISEMH78eCorK/nyyy+ZOXMmAOPGjePll1/m+uuvp0uXLvzzn/8kMzPT9+ZbQwK5P7feeivPPfcckydP5rnnniMtLY0DBw6Ql5fHLbfcAnhLVb/+9a+ZNWsWISEhvuXiNLVzm4ZoZQ01NNfXGDtr1iyVkJCgQkND1VVXXaXee+89BaiDBw/We6z6jq2UUrquq0WLFjV4vvrOP3bsWHXnnXf6Pufl5akbb7xRhYSEqPj4ePXYY4+pKVOmqGuuuabR6wXq/TNnzhyllFJvvPGG6tWrl7JYLKpTp07q0UcfVS6XSyml1LFjx9T111+vOnfurKxWq+rYsaO65557fI2yixcvVkOHDlUdOnRQISEhqk+fPuq1115rNJ6PPvpIdevWTdlsNnXppZeq1atX+92fmobmdevW+e2XlpamZs+e7ft88OBBNX78eGWz2VTnzp3Viy++qEaOHNlgQ/PWrVvrNPjX9uKLL/o1OL/zzjvqkksuUVarVcXExKirr75aFRYWKqWUMgxDvfjii6pHjx7KYrGohIQENWXKFN+xSkpK1O233646dOig4uPj1ezZs+ttaK4v1qbuj1JKZWdnq6lTp6rY2Fhls9lUz549/dYrpdSJEyeUxWJRM2bMqPd6ReA0pWTmNXH283g89OrVi8mTJzNv3rz2DkecZXbt2kXfvn3Ztm0b/fr1a+9wzmlSfSTOSmvXriU3N5fLLruM0tJS/vKXv3Do0CHuuuuu9g5NnEUcDgd5eXnMnDmT0aNHS0JoAZIUxFnJ4/Hw1FNPsW/fPiwWC3379mXVqlX11o+L4PX+++8zbdo0+vTpw7///e/2Due8INVHQgghfOSVVCGEED6SFIQQQvic820Kp3aaCVRcXBx5eXktHM3ZTa45OMg1B4czuebG5kSRkoIQQggfSQpCCCF8JCkIIYTwkaQghBDCp00aml9++WUyMjKIioqqd4gCpRSLFi1i69at2Gw2ZsyYQdeuXdsiNCGEELW0SUlh1KhRPProow2u37p1K8ePH+dvf/sbv/71r3nttdfaIiwhhBCnaJOk0Lt3b8LDwxtc//333zNixAg0TaNHjx6Ul5c3OcevEEKIlndW9FMoKCggLi7O9zk2NpaCggKio6PrbLt8+XKWL18OwNy5c/32aw6z2Xza+56r5JqDg1zzSUqpM5rBr7i4mMLCQlJSulBW4iIq2orHo9B1jbJSFxarCZtN923vcHjI3F9O94sifOd1ODyYTBoWiwmPR6EBJr3xmDwe7+hDeiPbtda/81mRFJpj3LhxjBs3zvf5dDtvSGeXxq1duxabzcbgwYP9ltceKuvU/2yGYdSZg7f2fm63G7PZHNB/0o0bN5Kamkrnzp3r/Y/tcrnQdR1N0+o9bnZ2Nj/++CNTpkwhPz/fF29+fj7btm1j9OjRAKxatZpuaX3wGOXk5eVRXFxMnz79iAiPAaUREaX7nVcpxdq1ayktLcVqtTJmzBgMw4Tb5SYk1OI7j9vtRikTusk7a1yVwwOGjtnq5t///jcRERGEh0eSmJDMocyfOHToEBdckEbHjh1JT7+M3bt3k3kok9JSDw5HGaVlxcRERzN4yEB27drDoEEDyMjIID09ncxD2Wzb/j1xcTEMHTKcygqNzz5/F4B+l/QnJMTOpm83AJDW9SI6RIeyZcsWYmOSyC84TsekC8g+fgiAvhcNo2OnKHbuOMaoMUPZtHEzbqOA+IRYeqQNIDJao6Agj+XLl1NZWUX37j0ZNnQImmbC5TIIC7diGAZOh0IzucnJLmLTt5sxWzwcPXqYlE7pHMnOICoinp5pV5JXvImLL+lNYkICa9asIyqiG2gOso78yFVXj0fDgqNKY9uO9aQk96BTp0S2bt1BdnYWFoudPr1+weYt6+lzUT8y99pQysDpOcGx/K/p22cwB/fl0DGpG0dzvqVLl65cmDKQHT9kcDQnA4vFyqCBA+mW1o/M/Q7CI3QK8ss4nP0dkZFRnMg7wokT3nmyI8LjcDstRHaw4ayIpqRiNxZzDLrJzMUXX0TesUQSkqwUFudw+MhubF/asdndxMX0Zv/BDBSVhEWYCdV+gUmzER1nYAvR6BBj4vst/yUmfAAujqI8gKsrCg8aJlIvtBMVY8LpUHTtYaO4yMmRQx4sVjeXpnfG5S5r8v9SfRrrvNZmA+Ll5uby7LPP1tvQ/Morr9C7d2+GDx8OwG9/+1sef/zxeksKp5IezYGruebCwkJMJhNRUVF1tiksLGTdunUcOnQIgHvvvZfCwkLi4+PZv38/O3bs4Pjx41gsFkaPHk1oaCiaplFQUMD69esZP3485eXlrFu3josvvpjRo0fz448/smzZMt85HnroIb9zGoaioqKciooKdF3nyCE3a/77AQCTJk1iyZIlDB40itTUTpSVunG4Clm50nu88LBIyspLvPGMug7NVMLXX39d57p03cyV42/ji6/e8i0zmSwYRt25hGtEhfYlNt5G34t7kZdtxx6i8d2W1ZRV7vNtY7GEYXjAY5T7lsVEd6SgMLuxf4pzkm4KxWNUAQ3PHa1hRuFu45gqzugYGjpmPRzdZKfKlXPaxwmzXUC541Cj24TaUjEMJ1Wu4w1uY9YjcHtK0U0hdAjrR2nlT7jcJVgt0ThcJ59Z6f3HMmx4n9OK9axPChkZGXz11VfMnDmTvXv3smjRIubMmRPQcSUp+Nu4cSM///wzl112GZdccglw8ht8zTX/7W9/A+D//b//R84xN0rBkextKAXffrupReOxWcNxOP2/zZj1EJKS4km7YBhr//shuh6K23N633jaUqgthQpHVosfVzeF4DEqG91G0yyE27tSWumdlzkmYiAFpZsb3N6sRxBqS6GkYne96y16JC5Pie9zZOhFKOWmtHJvvdufjMNMVGgfisq3N7odBHZdACHWZCqdgc8HXvPQPBOpKd2oLA/hRMEPDW6jaRpKKTqEXUJR+Q7s1hhc7io8RgVhoR0wm2IpLttf774mk46JENxG/b/XFj2KEFsnSip+PK34dd3G9OnTsNsbn5K1Ie2eFF588UV2795NaWkpUVFR3Hzzzbjd3m8TV1xxBUopFi5cyPbt27FarcyYMcNvIvXGnG1JQSnF/PnzGT58uG9u39ocDgcLFixg4MCB9OvXj9DQUN+6srIy3nvvPbp27cq4ceP47LPPOHToENHR0dx4442EhoaSl5dHdnY2uikMm00jv+AEmzZtYvz48ezZs4esrJMPLZNJRykD0Lj44r7gjsPlKefHn74N6FpC7FFUVhWf8T2pER3en8KyjBY7XlTYxZg0M6C16HFraOgoPA2uN5ksdEsZR5X6kcOHD9XdX9PRNDBpFsLs3QkJK8WkRVFUmI/ZHIHdEs/Isd2JiNJYtWoVlw8fyjvvvAPAA/c/wD/++Q8A7rvvPrIOGOzeXsmh3HfQNI0br7uPI0cyOXR4B263h7z841x5xRSSOsaSnBJPYWEROzOq+GHPUgoKjzJp4i+JjIxCKQiLAIvFwpdfLONQ5k9MuGIKnTolEhGps2vXHrZnHCCv0FsaioiIYOzoiezYuZkDB/Zz8cX9uKzf5eQer0LT3GzYtBybqTuGVowtLJ+IiHBKSys4ejSTq6+6kcTEaLKzs9mzZx8jR4zBandjs9mAk1WAZrO5urpNsWXLFsxmMwMGDKCqqgpN01i3bh0lJSV06phCv0v7kndcJzpWQ2lVfPTRR5SXl9OnTx927dpF165dGTNmHMeyyvk+Yx1XXjUKZehYbVYqK0vZtGkTI0aMIDIyEqUUFeUOyitK2LRpE6GhoURERDBgwAAAdF0n+4iD/FyDXpdYMJu9te0VFRWEhoailMLpdGIYGl999Q1HjhwE4MEHH8TtdmN4dNyeSmw2G0oplKHjcnuTpFkPwVHlYfEn71BeXk58fDwnTpxg0qRJxMfHY7PZcDqdeDwGJpPGzp07MQyDgQMH4nK5sFqtJCUltcrYR+f8fApnU1LIyclh/fr1HD16FIBLL72U/v37Ex4ejsvlYuPGjaSkpLBkyRIAwsLCmD59OgCVlZW8+uqrvmOFhIRQWen/LSt9wGAyMjIwVMNVHtB0tUggwuwXkhD1C6qcuWQXfgWc/KacGn8Lh094q3cuSPgVhWVbKa7+RpoQNRowcLjyKHdk+koAXZPH4HaZOZzzDRZzBC73mX3TA7hy7K+JSzRz+GAhaze+T+fOKRw9Wv83+eTkFI4c8a4bOXIka9asAbxvxnW9oCcbNv6XgsJcRvxiNB2iI9i9vYKuXS8kNc3M999vYtu2rQB0TOhBdu7PXHLJJfTq1YukpCTA+2+vaSZMJo333nuPEHsI06bfg65reNyK8jKDyA7e9gmnw2DLxgouSQ8hLNy/zeLvf/87MTEx3HbbbeTm5qJpGvHx8d42GZciJ/cY4eHhdOjQwbePw+EgKyuLbt26Af6/206nk8zMTLp3717nnjS2Lj8/H4fD4Xt4LF68mKNHj3LdddeRmpra6L9LZWUlR48e9cXTFs6Gkn9xcTGlpaUkJye3yflaa0A8SQrNVFpaynvvvceQIUPYsGEDI0eOxGw2Y7fbWb16NUVFRXX2sVqtOJ3Oeo9ns9lwOBzNjqMhnWImYhgOjhctP+1jhNhi6JI0EUeVt+G2b38zsQk2PG4oK3Vit1t5/0Pvt9jrrrmPA3uL2PHTB0SExzN50k3ExOlkH3GRl+Mmv3gXZmsVv/jFLzAMRWlJGZFR4eQcLyU8wvtNuuYb2IIFCxqMqWPHjlxxxRWsXr2azMxMzGYzM2bM8K13OBy++7x69Wp++slbzdK3b1927tzJNddcw+eff07Pnj2ZMGECRUVFfP311wwaNIgLL7yQkpISVq1axZVXXun7JltbVVUVuq5jsVhwOp1YrdYGY/V4PCilfNfVHG63G03T0HW96Y0b0BoPyNzcXDZv3syECRNO67pa29mQFNqaJIUGtFVSKCsrY8eOHeTm5nL48OHTOmdLCLEmY7fGU1i2td71FybewQXdraxa798BsHPCUCqd2RQUHWrw2NEdoiksKiQmJpZbb72Ndd+U0qOvnU4pdR+Ar7zyCi6XiwcffBCAn376idTUVEJCQk772vbt20dSUhIul4uDBw8SGhpKamqqXxUbwIEDB4iNja23obzG0aNHsVqtdO3ale+++45evXqd0auJ5xJ5QAYHSQoNaO2kUFFRQXZ2Nl988UXAx7777rv58ssv6d69O+vXr29wu5SUC/CU90U3WSkozcBuTSLMlkJ+2fcYhpNK51HsliSSosdiKA+GUcXFl8WRebCAospNpKZ0Y9O3awkNiSEszMykyZMIDw8DYO2a9YSEhnDZZf0wDAOr1UpsbCxz586lsrKSadOmsWvXLkpKShg8eDBmsxmr1cqnn37KsGHDGv2lAW99MHjrps9m8rAIDnLNzSNJoR6B3NDy8nIWLlxY77qRI0fidrvp378/mqZx/PhxPvzwQ8xmMw888AA/7qgioaOF/XtKKHfu58QxO/ml3+P2lNI5ZhJWS93Xba0273vx+blu4pN0jhz9mVB7EgOGJPDdunLik8wMGXmyZ7hSih07dtC7d++AHs7yHyc4yDUHh9ZKCmdf5eBZoqSkhDfeeKPedTfeeCOdO3f2WxYTE0NiYiIDBw5jw8oyCvI87N/jwDuSSHdCbWC3dsTtKcVq9k8I8UlmzGaNvv1DsFo1du+oolsvGwOt3reXdF1j9NUR2O3+HcM0TaNfv34tdclCCCFJoT5KqToJwWazMWHCBE6cONFAljVz/XU3sW+Pg4K8+huOdZMZizmaiCid4ePCqSg3MAyIiPRvVOx7Wd16+fCI0294FEKIQElSqMepg/HZ7XZuuOEG4uLiuOCCC+rdZ9l/SjAa6OjZvbeNxE4WojroKAUm3fst/9TXEYUQor1JUjhFdnY2Bw96O6FomsaECRPo0aNHvdu6XArDUFRWf+Ovz/jJkdhDZC4jIcS5QZLCKT766CPfz9OmTSMsLKzBbb/+pJj6mun7DQxh724HlZUGVltwvAYphDg/SFKoxePxH9Lg1Pfjayil2LKxok5CGD42nOg47y1N7Vq3A5QQQpztJCnUUtMbuXfv3vTr16/Bzk4upyI76+QwEpePDScqWm907HMhhDgXSFKo5cSJEwBcdtllxMbG1ruNUoqDe0++XXTl9VFYrJIMhBDnB2kBrSU7OxuLxdLoPA7ZR1z8vOtkUpCEIIQ4n0hSqLZx40Z++OEH4uPj6509TBmK4kI3eTknJxAZMzGiLUMUQohWJ9VH1TZv9k5Y0lA/hP0/OfhxR5Xv81U3RGG2SClBCHF+kZIC3jGOwDvMcs0EG6cqLjr5ZlKXNKskBCHEeUlKCsB///tfALp161bnjaOCPDeZ+x3kZnvfNup2kY1eF9vbPEYhhGgLkhTwjr2fmppa74xSm9aU4aluRujZ106PPpIQhBDnr6CvPnK73ZSWltKxY8c665ShfAkBIDwi6G+XEOI8F/QlhYqKCgDCw8PrrKvpj3Dp4FAsFo3ETkF/u4QQ57mgf8rVDG1R37yzWQe98yondDRjs0kpQQhx/gv6pOB2e+uHak+UXlbqYdWXpQAkd7FIQhBCBI2gf9rVV1LIOXZyXKNQmfNACBFEgj4pVFV5O6TVLinUHtiuc5eze2J6IYRoSUGfFD777DPAv6Sgak2YI9NgCiGCSVAnhZqhssG/pOB0eidKuOLayDaPSQgh2lNQJ4Vvv/3W93NNSUEpxc+7qjCZwGYP6tsjhAhCQf3Uq106qBkZtbTYW3ck02gKIYJRUCeF2kNk1ySIslLv20iXDqp/Kk4hhDifBXVSqF1SiIjwzo1QVuotKXSIDfouHEKIICRJ4RRF+W7CI0xYZGhsIUQQCuqkcOoMa0opigo8hEfKa6hCiODUZnUk27ZtY9GiRRiGwdixY7nuuuv81ufl5fHSSy9RXl6OYRjcdttt9O/fv1VjqikpxMTEAJC534mjSuF2q1Y9rxBCnK3aJCkYhsHChQuZNWsWsbGxzJw5k/T0dJKTk33bLF68mKFDh3LFFVdw5MgR5syZ02pJQeXl4Diyn5r5dGoSVOZ+7wB4jiqjoV2FEOK81ibVR/v27SMpKYnExETMZjPDhg3zzYlcQ9M03zDWFRUVREdHt1o8ast/KXriYTxO7xhHYWFhKKV8bx71GyhvHgkhglOblBQKCgqIjY31fY6NjWXv3r1+29x000089dRTfPXVVzgcDh577LF6j7V8+XKWL18OwNy5c4mLi2t2POURkZQBNqsVk8lEfHw8LqeB4SkmfVgs3Xu2XkJqT2az+bTu17lMrjk4yDW34HFb/Iin6b///S+jRo1i0qRJ/Pzzz8yfP5958+bVaQweN24c48aN833Oy8tr9rmMSu8geOVlpZhMJvLy8qgo95YSXK4K8vI8Z3AlZ6+4uLjTul/nMrnm4CDX3DydOnVqcF2bVB/FxMSQn5/v+5yfn+9r3K2xcuVKhg4dCkCPHj1wuVyUlpa2TkDVDcyGx+NLOmUlNT2Zg/qFLCFEkGuTJ2BaWhrZ2dnk5ubidrvZsGED6enpftvExcWxc+dOAI4cOYLL5SIyssnJAv8AACAASURBVJUGpKvpvVxejlLeN40yD3gbmUNCJSkIIYJXm1Qf6brOtGnTePrppzEMg9GjR5OSksIHH3xAWloa6enp3HHHHSxYsIAvvvgCgBkzZqBprdSBzORNCvsPZ/kWedwKTYOoaOmjIIQIXm3WptC/f/86r5jecsstvp+Tk5N58skn2yaYenoyO6oU8UlnTROLEEK0i+CsK9F1PHhLIdHR0TgcBmUlHplQRwgR9IIyKWgmHXd1A3OfPn0oKzEwDKSkIIQIekGZFNB13NXtChaLBVf1TGsyh4IQItgFbVJwVScFs9mMy1n9OqpVkoIQIrgFZ1Iw+ZcUauZktkhSEEIEueBMCrWqj8xmM06H93VUs8yhIIQIcsGZFEw6zuqkoGFm/x4H9hCt9fpFCCHEOSI4k4Ku49AtABTmmVAKqiplDgUhhAjipOB9/dSk2wAZLlsIISBYk4KpVlJQVjQTpFxobeeghBCi/QVnUtB1nCYLJk3DMEyYzdKWIIQQEKxJwWzBYzKhaxoet7x1JIQQNYIzKYSEYqBh0sDtBrOMbiGEEEDQJoUwDE3DBDgdhvRkFkKIagEnhTfeeINDhw61YihtR7NYULqOSSkqKgxCwoIzNwohxKkCrjgxDIOnn36ayMhIfvGLX/CLX/yC2NjY1oytVSmLFU0pqiqVzLYmhBDVAk4K06ZN46677mLr1q2sW7eOjz/+mO7duzNixAgGDx6M3W5vzThb1Ir9RXwb1Z3O7nxQMhCeEELUaFYTq8lkYsCAAQwYMICsrCz+9re/8fLLL/Paa69x+eWXc/PNNxMTE9NasbaYKreiwmSlpg+zvH0khBBezUoKFRUVbNq0iXXr1pGZmcngwYOZPn06cXFxfP755zzzzDO88MILrRVriwm3mtBQoLzJQJd+CkIIATQjKcybN4/t27dz0UUXMX78eAYOHIjFYvGtv+OOO7jrrrtaI8YWF2HT0ZTCqC4qSOc1IYTwCjgpdO/enenTp9OhQ4d615tMJl599dUWC6w1hVt1TBio6pevpPpICCG8An7t5pJLLsHtdvsty8vL83tN1WaztVhgrSnCpqOhTiYF6bwmhBBAM5LC/Pnz8Xg8fsvcbjd///vfWzyo1hYTYkZDYeg2NE0RHqG3d0hCCHFWCDgp5OXlkZiY6LcsKSmJEydOtHhQrc1mNmFGoTQLYXZDGpqFEKJawEkhJiaGAwcO+C07cOAA0dHRLR5UW7CaAJMZi7OkvUMRQoizRsC16RMnTuT5559n8uTJJCYmkpOTw5IlS7jhhhtaM75WY9E1MHSszrL2DkUIIc4aASeFcePGERYWxsqVK8nPzyc2NpY77riDIUOGtGZ8rcZsAk3TsXgq2zsUIYQ4azTrvZuhQ4cydOjQ1oqlTeka6JiwuMvbOxQhhDhrNCspFBUVsW/fPkpLS1Hq5ET3Y8aMafHAWptJKTTNhMklSUEIIWoEnBS+++475s+fT8eOHcnKyiIlJYWsrCx69ep1TiYFTRlomDDcUn0khBA1Ak4KH3zwATNmzGDo0KHcfffdPPfcc6xatYqsrKzWjK/VeFwOrLoVo7K4vUMRQoizRsBJIS8vr057wsiRI/n1r3/NHXfc0eT+27ZtY9GiRRiGwdixY7nuuuvqbLNhwwY++ugjNE2jS5cu/Pa3vw00vGYxDAO3swo9zI7LJSUFIYSoEXBSiIyMpKioiA4dOhAfH8/PP/9MREQEhmE0ua9hGCxcuJBZs2YRGxvLzJkzSU9PJzk52bdNdnY2n376KU8++STh4eEUF7feN/jKSm8i0E12XA5pUxBCiBoBJ4WxY8eyZ88ehgwZwsSJE3niiSfQNI1rrrmmyX337dtHUlKSr0f0sGHD2Lx5s19SWLFiBRMmTCA8PByAqKio5l5LwKqqqgAwmWy4HeUolxPNYm218wkhxLki4KQwefJkTCZvB+iRI0fSp08fqqqq/B7sDSkoKPCbujM2Npa9e/f6bXPs2DEAHnvsMQzD4KabbuLSSy8NNLxmqSndaOi4DTeUFkNMfKucSwghziUBJQXDMJg6dSpvvPGGbw6FuLi4Fg3EMAyys7OZPXs2BQUFzJ49mxdeeIGwsDC/7ZYvX87y5csBmDt37mnFUVN9ZGDCZdLoYDZhaeHrORuZzeYW/3c728k1Bwe55hY8biAbmUwmOnXqRGlp6WlNtxkTE0N+fr7vc35+fp3jxMTE0L17d8xmMwkJCXTs2JHs7Gy6devmt924ceMYN26c73NeXl6z4ykoKADAo2k4TRaKso+hRcY2sde5Ly4u7rTu17lMrjk4yDU3T6dOnRpcF/CAeMOHD+fZZ59l9erV/PDDD+zcudP3pylpaWlkZ2eTm5uL2+1mw4YNpKen+20zaNAgdu3aBUBJSQnZ2dl1RmVtKTXVRwYmnCYLVEhjsxBCQDPaFL755hsAPvroI7/lmqY1OaeCrutMmzaNp59+GsMwGD16NCkpKXzwwQekpaWRnp5Ov3792L59Ow8//DAmk4nbb7+diIiI07ikptXMC2FUlxRUZTkyeLYQQjQjKbz00ktndKL+/fvTv39/v2W33HKL72dN07jzzju58847z+g8gahJCh40nLoFKita/ZxCCHEuCLj66HxysqRQXX1UIr2ahRACmlFSeOCBBxpc949//KNFgmkrNUlBmUyUR8Sijv7YzhEJIcTZIeCk8Jvf/Mbvc2FhIV9++SWXX355iwfV2moamk26icLQaDgSXG8tCCFEQwJOCr17966zrE+fPjz99NNcffXVLRpUa6spKZjNOkV6KFT3cBZCiGB3Rm0KZrOZ3NzcloqlzdQkBd2iU2Kyg0OSghBCQDOHzq7N4XCwdetWLrvsshYPqrXVJAWTRcep6eCQkVKFEAKakRRq90gGsNlsXHPNNYwYMaLFg2pt3bv34tCeaBwWHScmcFShlELTpLeCECK4BZwUZsyY0ZpxtCmLHoLNEovH4saJCaUUavlnaOOvbe/QhBCiXQXcpvDpp5+yb98+v2X79u3jP//5T4sH1dqcTu/80rpZw0DDremotV+3c1RCCNH+Ak4KX375ZZ1hspOTk/nyyy9bPKjW5nZ5k4K5egoFZ2Qs2OztGJEQQpwdAk4Kbrcbs9m/tslsNuN0Ols8qNZWU1KwWLyX7+5xMVTKoHhCCBFwUujatStff+1fxfLNN9/QtWvXFg+qtblqkoLV27DsDImQ8Y+EEIJmNDTfeeedPPXUU6xdu5bExERycnIoKirisccea834WkVN9ZHN5k0KubYOxFeUyxtIQoigF3BSSElJ4a9//StbtmwhPz+fwYMHM2DAAOz2c68uPqGjheiYSFS4t8polx5LH48bXE6w2to5OiGEaD8BJ4WCggKsVqvfWEdlZWUUFBSc1mxs7SkiSicuLpK8PCdWXaPSXJ0IKsolKQghglrAbQrPP/+8bxrLGgUFBbzwwgstHlRbCjGbqDJVv4Yk7QpCiCAXcFI4duwYqampfstSU1M5evRoiwfVluwWE1Umi/eDvIEkhAhyASeFyMhIjh8/7rfs+PHjrTZlZlsJMZuo1Kpr0QrzG99YCCHOcwEnhdGjRzNv3jy2bNnCkSNH+P7775k3bx5jxoxpzfhand1sospsB92M2pXR3uEIIUS7Crih+brrrsNsNvP222+Tn59PbGwsY8aMYdKkSa0ZX6uzW0xUOBUkdUaVyrScQojgFnBSMJlMTJ48mcmTJ/uWGYbB1q1b6d+/f6sE1xZCzBr5FQaEhsFOKSkIIYJbwEmhtszMTNasWcP69evxeDwsXLiwpeNqMyEWE1UuA/buBkDt3orW+9ybI0IIIVpCwEmhuLiYdevWsXbtWjIzM9E0jbvvvpvRo0e3Znytzm42UeU2Ti6Q11KFEEGsyYbmjRs3MnfuXO6//35Wr17NsGHD+Pvf/05kZCRDhgzBarW2RZytxm42UelWaNfeBoAyVDtHJIQQ7afJksKLL75IeHg4Dz/8MIMGDWqLmNpUiNmE21C4h4xB/897UCUlBSFE8GoyKTzwwAOsWbOGP//5z6SlpTF8+HCGDRt23gwcF1I9fHalbiccpPpICBHUmkwKo0aNYtSoUZw4cYI1a9bw1Vdf8dZbbwGwdetWRowYgckUcHeHs87JpGD1JoW8nHaNRwgh2lPADc3x8fFMmTKFKVOmsGfPHtasWcObb77J+++/z4IFC1ozxlYVZtEBqHADF/ZA7d7WvgEJIUQ7ajIp7Nixg969e/vNutarVy969erFtGnT2Lx5c6sG2NrCrN6SQrnLg9a1J2rDynaOSAgh2k+TSWHJkiX89a9/pWfPnvTv35/+/fv7hsq2WCwMGzas1YNsTWHW6pKC04CIKKgsR5UWo0VEtXNkQgjR9ppMCn/6059wOBz88MMPbN26lY8//piwsDAuu+wy+vfvT48ePc7pNoWw6jaFUqcHzN7RUtVn76H96oH2DEsIIdpFQG0KNpuN9PR00tPTATh8+DBbt27lX//6F0ePHqVPnz5MnDiR7t27t2qwrSEuzEKETeeHnArGDhuL+vci8HjaOywhhGgXpzXMRWpqKqmpqVx77bVUVFSwfft2KisrG91n27ZtLFq0CMMwGDt2LNddd129223atIk///nPzJkzh7S0tNMJr1nMJo3UKCsnyl1oEZEAqHXfoG67D6265CCEEMEi4KSwc+dOEhISSEhIoLCwkHfffReTycRtt93G0KFDG93XMAwWLlzIrFmziI2NZebMmaSnp5OcnOy3XWVlJUuXLm3zEke4Ved4mcv7oXMXOJoJ5WUQFd2mcQghRHsLuDFg4cKFvraDt956C4/Hg6ZpAb2Oum/fPpKSkkhMTMRsNjNs2LB631r64IMPuPbaa7FY2vYberhVp8zprTLSrrzRu1BmYRNCBKGASwoFBQXExcXh8XjYvn07L7/8Mmazmfvuuy+gfWNjY32fY2Nj2bt3r982Bw4cIC8vj/79+/PZZ581eKzly5ezfPlyAObOnUtcXFygl+DHbDb79o2PKqH8cClxcXE4EpMoAjrYrFhO89hnq9rXHCzkmoODXHMLHjfQDUNCQigqKiIrK4vk5GTsdjtutxu3233GQRiGwVtvvcWMGTOa3HbcuHGMGzfO9zkvL++0zhkXF+fbVzecVLkNsnNyMbu811N0PBstOuG0jn22qn3NwUKuOTjINTdPp06dGlwXcFK48sormTlzJm63m7vuuguAPXv20Llz5yb3jYmJIT//5PzH+fn5vr4OAFVVVWRlZfHEE08AUFRUxHPPPccjjzzSJo3N4dV9FcqcBh3sod6FUn0khAhCzZqOc9CgQZhMJpKSkgDvw/7+++9vct+0tDSys7PJzc0lJiaGDRs28NBDD/nWh4aG+k3U8/jjjzN16tQ2SQhQOyl46BDtreZSOcc4P4b8E0KIwDXrldTaRY6dO3diMpno3bt3k/vpus60adN4+umnMQyD0aNHk5KSwgcffEBaWpqv/0N7Ca8e6qLM6UGLj/LO17x/T7vGJIQQ7SHgpDB79mxuvfVWevXqxaeffsoXX3yByWRiwoQJ3HDDDU3uXzNERm233HJLvds+/vjjgYbVIiJs3pJCqaO601pSCuQcbdMYhBDibBDwK6lZWVn06NEDgBUrVjB79myefvppli1b1mrBtZXYUO8rsHkV3kZmLbEjnMhGGdKzWQgRXAJOCkp5p6k8fvw4AMnJycTFxVFefu43yHaw61hMGjk1HdgSO4PbjVr8ZvsGJoQQbSzg6qOePXvy+uuvU1hYyMCBAwFvgoiIiGi14NqKSdNICLeQW+5NClqnVBSgvvkUbprWvsEJIUQbCrik8OCDDxIaGkqXLl24+eabATh27BhXX311qwXXlhLCLOTWlBS69oRuFwEnS0hCCBEMAi4pREREcNttt/ktO7Xh+FyWEGZhf0EVAJqmoV02BLXvR++czaFh7RydEEK0jYCTgtvt5uOPP2bt2rUUFhYSHR3NiBEjuOGGG/xmZTtXRYfolDo8eAyFbtIgzDtiKmXFkhSEEEEj4Kf5O++8w/79+7n33nuJj4/nxIkTLF68mIqKCl8P53NZlN2MAkocHqJDzGidq9sV/rsC7fqp7R2eEEK0iYDbFDZt2sQjjzxCv3796NSpE/369eN///d/2bhxY2vG12ai7N6+CkVV1a+lXtAd+g1Crf0aZRjtGJkQQrSdZr+Ser6Kq+6rcKL6DSQA7eJ0KCuBovyGdhNCiPNKwNVHQ4cO5dlnn2XKlCm+0fkWL17c5AQ7bU0pRVVVFYZhoGkNj16Uk5ODw+HwfU60Gfy6XxRRupuKigrvsbpehPrlfWhuD1r1snPZqdfcGKUUJpMJu93e6H0UQpxfAk4Kt99+O4sXL2bhwoUUFhYSExPDsGHDWmTo7JZUVVWFxWJpsvHbbDaj67rvcyhwUScz4Tad0OpSgzIlgKcHhIZCSQHEd0Srtc+55tRrborb7aaqqoqQkJBWjEoIcTYJOCmYzWZuueUWv/GKnE4nU6dO5fbbb2+V4E6HYRin/TaURddweWq1H1TPNEdejvfv8lKI7HCGEZ47zGZzwCULIcT5IeA2hfqcjdUKZxKTxaThMmq1neinJJez8Hpb29n4byyEaD1nlBTONxZdw+1RGNWN6pqunywtQFAmBSFEcGmynmXnzp0Nrjvb2hPOlEX3PvTdhsJa/TMmHeSVVCFEkGgyKfzjH/9odP35NFm2pbpU4PIorDXtsbVLBwEkh+LiYj755JNmd+ibOnUqf//734mKimrWfv/zP//DuHHjuOaaa5q1nxBC1KfJpPDSSy+1RRxnhZqSgstTq10hLhHKSqG0CALoq1FSUsJbb71VJym43e5GG8Dffvvt04pZCCFa0rk/aFEjjH+9iso6WP86TavTIU8DklwGFl3DYzql/aCq0vt3997ov7y3wXM+88wzZGZmMn78eCwWCzabjaioKPbt28f69euZNm0ax44dw+FwMH36dN+bW4MHD2bp0qWUl5dz++23M2jQIL7//nuSkpJ4/fXXA3otdN26dTz55JN4PB769evHnDlzsNlsPPPMM3zzzTeYzWZGjBjB//3f/7FkyRL+8pe/YDKZiIyM5OOPP27y+EKI8995nRROh6Y1USAoLW50/0cffZSffvqJZcuWsWHDBu644w5WrlxJamoqAPPmzSM6OprKykomTpzI1VdfTUxMjN8xDh48yEsvvcTzzz/Pfffdx5dffsmNN97Y6Hmrqqp4+OGHffNeP/TQQ7z11lvceOONLF26lLVr12KxWMjP9/bOfvHFF3n33Xfp2LEjxcWNX5MQInic10nB1Mg3erPZXG9D+YnCKkIsJhLDrX7LldMBxw57f64oQwsNDyiGSy+91JcQAF5//XWWLl0KeOejOHjwYJ2kkJKSQt++fQG45JJLyMrKavI8+/fvJzU1lbS0NABuuukm3nzzTe6++25sNhu///3vmTBhAqNHjwYgPT2dhx9+mEmTJnHVVVcFdC1CiPOfvJJ6CpNJw1NfScFihcho78+FBQEfLzQ01Pfzhg0bWLduHUuWLGH58uX07du33s5hNpvN97Ou63g8pz9XtNls5osvvmDixIl88803/OpXvwLg2Wef5ZFHHuHYsWNcddVVFBQEfk1CiPPXeV1SOB1mTfNvaK6maRrExKE0DYoLUUrV27ErLCyMsrKyeo9dWlpKVFQUISEh7Nu3j4yMjBaLOy0tjaysLA4ePMiFF17I4sWLGTJkCOXl5VRWVjJ27FiGDh3qm0r10KFD9O/fn/79+7Nq1SqOHTtWp8QihAg+khROEWoxkVfhwm0ozKc2NgPoOqCguAAVFoFm8a9miomJYeDAgYwZMwa73e73yu6oUaN4++23GTlyJGlpaS06c53dbufPf/4z9913n6+heerUqRQVFTFt2jQcDgdKKWbPng3AU089xcGDB1FKMXz4cPr06dNisQghzl2aOsfHxD527Jjf54qKCr8qm4Y01KZQ6fJwtMRJxwgrYda6g8epspKTYyFRPe/COaKha25MoPfzbFUzom8wkWsODmdyzZ06dWpwnbQpnMKie29JhauBjmrndg4VQohGSfXRKWpGtyiuchMXaq7bbmC2tH1QeF913bx5s9+ye+65x2/UWiGEOFOSFE5ROwkY6mSS8K0PCaV2WUHlZkNMHForJ4tnnnmmVY8vhBAg1Uf1ig7x5sraU3P6iUs6+XNFGRTKdJ1CiPODJIV6hFq8t6XM6al/bmq7vY0jEkKItiFJoR4288nb4jbqSQraKbetsryVIxJCiLYhSaEeJk0jproKKbPI4Zt0x+fUxmfDqL9EIYQQ5xhJCg2oaVcA6vRw1kwm6BADHVMhurpzWo5/f4lAde/ecD+HrKwsxowZc1rHFUKI09Fmbx9t27aNRYsWYRgGY8eO5brrrvNb//nnn7NixQp0XScyMpIHHniA+Pj4tgqvDk3TCLPqlDs9eOqpQtI6xAKgXNVjF1VVoBxVaDZpbxBCnLvaJCkYhsHChQuZNWsWsbGxzJw5k/T0dJKTk33bXHDBBcydOxebzcY333zDO++8w8MPP3xG533t+xwOFlbVu06rZz6FOnErcLi98yvUDHlxYbSde9ITT25Ue+Kc7CyefvdDOnfu7Jtk54UXXsCkm9m0cQPFxcW43W4eeeQRJkyY0KxrqaqqYubMmezYsQNd15k9ezaXX345P/30E7/73e9wOp0opXjllVdISkrivvvuIzs7G8Mw+O1vf8u1117brPMJIYJTmySFffv2kZSURGKi92E6bNgwNm/e7JcUaoaKBm+Vyrp169oitEbVNB24DdBN4DEURVVuDKUw1ay0+U9+M3nIQGa/9E/uvPNONE3jP0uW8OzLr3PnXXcTGx1FQUEBkyZN4oorrqh3QL0ahlKUOU+OjvrGG2+gaRorVqxg37593Hrrraxbt463336b6dOnc8MNN+B0OvF4PKxcuZKkpCTfbG4lJSUte2OEEOetNkkKBQUFxMbG+j7Hxsayd+/eBrdfuXIll156ab3rli9fzvLlywGYO3dunTmic3JyfNNe3j+k85mGzp6cUgCSIu3klTlwGwrNpGPWTzbHGJ274DmaCUDftAvJzznO8e83URwSTnhEFDGx8Tz//LN8/923mEwmjh8/TmFhIQkJCQD1TtOZW+Ygr9yFobzrv//+e6ZPn47ZbKZXr16kpKSQmZnJwIED+etf/0pOTg4TJ06ka9eu9O3blyeffJI5c+Ywfvx4hgwZ4jtuY1OC1sdms53T83CbzeZzOv7TIdccHFrrms+6Hs1r167lwIEDPP744/WuHzduHOPGjfN9PnVAKIfDga7XHcjuVM0dHK7C4fb1ZHa73WjqZFI4tRJq4qiRfLF6DSfyCxh/1USWffkZ+Xl5LF26FIvFwuDBgykvL/edv744XG5vKUEphdvtRimFx+PxbVvz+dprr6Vfv36sWLGCW2+9lWeffZbhw4ezdOlSVq5cyZw5cxg+fDgPP/zwaQ2I53A4zumBxmSgtOAg19w87T4gXkxMjG8aSID8/Px6x+7fsWMHn3zyCY888ggWS/uMMXSqLh1sWHUTJQ63r8G5Truz2Qy6GUd0AoZmYvLY0Xy2fCVfrF7DmAlXU15aSmxcHJpuZsnyNRw5cqTJ855atTRo0CA++eQTwDvL2tGjR0lLSyMzM5MuXbowffp0JkyYwI8//sjx48cJCQnhxhtv5P777+eHH35o8DweQ9XbkC6ECE5tUlJIS0sjOzub3NxcYmJi2LBhAw899JDfNgcPHuTVV1/l0UcfJSoqqi3CCohFNxFl1zlRfnLU1Jr2aUMpTpS7iA0xozp14UiRA3tIHN3SoKDKRVxCEuHRcYybOJnZ/3M/Y8eOpdtFfbmga1qT561JCR5DoZTizjvvZObMmYwdOxZd15n7/DxsNhuf/Ocz/vPJx5jNZhISEvjNb37D9u3beeqpp9A0DYvFwpw5cxo8T01DfLfYkAa3EUIEjzabTyEjI4M333wTwzAYPXo0N9xwg2+S+fT0dJ588kkOHz5Mhw4dAG/R6A9/+EOTx23p+RQaUlDhoqDSu32nSCshZhOlTg+5ZS4ibDpRdjNHir2vp6aU55AVlui3f5JNUWzoVFYPyd3UQzivwkVR9flSomxYdI0ql0GoVaew0k1+hYuYEDMFlW7iQi10CGk6v9d3zfvyKxuNR+ZTOPfINQeH1qo+arM2hZqpH2urPezzY4891lahnJZQq+5LCsdKnETYdEodJ98OqkkIGnin7DyFKirEY4sEU/23vKDChc1s8k3sU/sISkFhpZvCSjedI604PN7EUun2/l3zublyy5yntZ8Q4vx11jU0n62sp4yhXTsh1P5ZAaUdksDp/6AutEXgrJUQlFKUOj043YoTh/dx/4O/8TuPyWzhpXcWVx9T+doxKt0GDb/I2jSlFE6PwmY2UVIr7txyFwlhZ0c7TkM8huLDnXlM6hlDuK3plwmEEM0nSSFAJk2ja7SdAw10hqut2Fn3m7vT5P/APVripKr6m35cajcWfrQE8FbjuA1FTpnTV9XkNryT/gA43arO0EuNqXIbeAxFmFXH4fZQUOGmuMpNageb33YlVW7i65tU6Czy3dEy/vVDPifK3Tw0tOMZH8/hNjCbNPT65uIWIkjJ2EfNYDJphFhO3rIo++nn1JqEACcf+DUOFVb5EgJ42xdqOGuNw1TTGuQxIL/C5ddD+3ipk9xyF0eKHWSXOskudXIwv8J3rvpeOGrOS0gOt4HDfXrVVqfLXX3tVS1wXqUUN3/wMy99e/yMjyXE+USSQjN5q1g0Qq068WEWOkdaW/T4tauiatR+ZdTpMXyJoWZxhctDYaXbb17pMqeHklrJptzpf9z6vhvnVbgCfuDe/fE+bv93wx0Qa+I+XuoMOHl8uDOPQ4VVwgbRAwAAIABJREFU5JQ5WXOwmPwKF1/8VBjQvs1Vcw9XHChuleMHs3Knhx3H2244+R9PVJBxpKjNzne+k+qjZrLoJtJibL5qlhCLTrfYEN9bPKfD7nFSpXuTS04Ajb81D1nPKS+OZZc6A361NLeeWeVKHR5KHR5So2xYzY1/Xyh3Nf2gf21LDl/+7P3P+p9f9Wp0W7eheHd7Hot3FXBZxzA2ZpVi1TWcHkWfhBA6Rlh9nQRLHR5cHgOL3vR3GqfbYFt2OZd2DGNnTgXHy5yMS+vgG0LELFVHLe6F9cfIyC7n7Ru7EXkGpelA/fGbw0DTv2MiMFJSOA311bt3Os0SQ0JVIUmVeYRSt4TQlPo6nQX6hnFj395zGpqGtJHznspQypcQmhOP02OQWVRV/bP3PL/98hA3f/Czb9sdORU8tbrhDoCGUny0M49jJU7mrzvI7JVZHCys4k/LDzN/k7e66ES5txRlM3v/LZVSZBY5Ao63vXy4M4/vjpS2dxiNqrmPzjboFHn3x/ta/RzBRpJCCwm16Jg0jdKSEj751zsAdLCbSYnyb9Ct/dZMF1MFEa5ydGXwu/vvpjSAgetMTTQEOz3qjCf8OfUMH+7MI+NYGVVug/0F/u0d4J1v4r7/7OezPQW+ZZuymvfgql0ldqy0/qS09OeTVUnbjlf4rcsudfqS1aFCB+9sz+ORbzI5WODdrrjKP+k+s8abVEyaxpc/F/J/K7N46IuD/Jjr3V4p1SJtF821Yn8R3zZy797dnsfTa462YUTNs/pgMfmVNUOxBL6foRSHi5uXlJVSvtfEzwVbjpaRX1H/7/aSPQW8tiWnjSOq33ldfbQzo4KSovq/gQcydHZ9Ijvo9O1ff2cuQynKSkv4zwfv8uvpd/saou1mE2VVTu9gdNWntOgalg6xqGLvg/SVF18k1x7Z8HlxoVsslBlgNFKocHgMzKYze13z1Lzz/9s77/AoqrWB/2Y2m7LZ9AYEQkgIXTqigICA2Ci2qygWFLGgIHpF0OsVPhGwXASlCCLCFfFKExAUkY7SIRSlQyihpW0Skuwm2d053x+TbHazmwJSJJnf8/CwmTmzc86cnfOet5z3zNunLpBpX9vI9rO5TO5Vz+X8wgPpXMy1Mmt3Kl1jA/HTy+R5iMAqj5zCijWlg2meTXRZFhsv/ZhE70YhPNMygtdXnlK/s8DOnrOqz2DXuVyXa4J8dWQX2IkN9mHGzpKX8WKulcaRqq9h8raLTO8TR80Ab9YnZdM00kCa2co7q8/wyd11aRB+9VeBf16kyXgyhVztdaaFdoUj6RZuifKvsKwQAotNwaAv+7eVZMpn4pYLjr/ddiwshx8OmJi7L42J98YSF1q5PUkslym0Zyemcmu0kaZR138xZnJ2Ae9vOEv72kbe6VLb7fxXu1MBeL5NlNu5602VFgrXG18vmS8/+4TzZ8/wSO970ev1+Pj4EBgYxNHjx5j74xreGvIi586fx15YyPPPD6R/p/YA9OjTh2nzf8RiNvPW4IG0b96MnX8eIDwyinGfzSDCqg7MmQGuP6jli75n+eL52KxWouvU5aNPJ+EdZMSUkc6EMf/mwtlkAN54932atWzNLz8uYf5/v0KSJOIaNOTdcRPc2mGxKpgsVkJ8vfgjpcRhuP2sOrCm5pbMdtLyrMz/oySv1VOLj9Mkwo9uceWnKskttOPnJXPClI8sSfzzl1OX8aRVijWD5EvqDHPLmRzurOf5vs6C7uejmZzJVn03Rm/XQU4ALyw7QUpRG89kFRBu0DNpqzrYRfqrr8z+i+ZyhUKhXUFCQq+7Mp/Fb6cucUes6yShrEHQahckZxd4HEytdgUBeHvwv3y1K5VVx7OY0quem0ZbmoUHMpi3L93FT3Ap38aELRd4/faaBPt5OYSx495FKVoqE+Z8OF3V0I5mWKgb7FOpMGFzBX6ts9kF1C5ql00RLD1kYukh0w3xPaw54R7QMGpdMtn5NibdV8/DFTeOKi0UyprRw+WnuagMNQO8Gf3uvxh08jirV69my5YtPP3006xbt46YmBgKbQqTPp1AaGgoFouF+++/n/tuaURIUBASgpqWDJLw49yZUzz/3kg+GfkmL/77/9j/00Ka9LzT7X4+XjKde9xN70f6AfDV5E9ZtOB7+jz2FJ9/+D4t29zK2ElfYLfbsZjNnDx+lLkzpzL1mwUEh4RyKbtsm7/JbMNktpGe5/6MnCN2TmW6q/wH0yxus/pp2y8yuH0N9l7II6/Qzse/n6d5DQP7S5mBLof+C49RL8SH7vGqIMgw23ij1MBUjPMA4qwZ2BT3gSXFSejZFIHF6rzIr+h5VDBm/eP7o4QbvJh4b+wVOVv/s/m8m1AorX2l5lo5mZnPngt5rDyWxZd944gyuvq2Xv4xiewCOwv7NXS7xwmT6rspbQ4s5tfjWdwabSTYz4vfTqmmTZPF5mjPymNZ7L2Qx4ojmTzZ0n2XRKtd8MKyJBqE+zK8U+XS2H+xI4UvdqS4DdynMvNZfiSTwbfWcAiM0s9j59lccgvtdKkXyM5zuYzbeI63OtWiY91Aj1F9V5v9F/M4lVVAn0buyT6LzUbOsm7vBXXCNWXbBbfyN5IqLRSuNzpZcovaadmyJTExMQB4e8nMnj2blStXAmrepqSMLNoUJQD0tRdQ15JFnehomjZIAMVO64Q4Us4nu90ryEdHiDmDP44f5l+TPyM35xIWs5lbO94BQOKOrbwz9hO1XjodxoAAVi1fQu9evQgOUX+0gUHBV9TOLWdKbN6VVeFXHc/inoRgRq0ractfEQjF9z6YZqFpZMXmgLIGhdIDS2lfyKmsAo9hx4pQs8t6mtEW+yLSzTaeWnycW2sb+Vcpk8H5S4UYfXQElrMy264Ivk5MpWu9QBLC/Fw2XQJVcBxJtzj2E88tVChtfEgzlz3xKY5eS81T83rdVicARQisdkGmxcbU7RfZGOnHe3fWcWhWztuV7ywyye0+n0uHmAC37195NIvUPCupeVaGd/Jch3ybUqR5uj5Ha1Hqlo2nLtEhJoBJWy9wMrOAzrGBNI7ww1snu63v+aDIT2S2Ko4ovpOZBaTmZZDp5HtYdSyLu+oHcTwjn1m7UxnVrTY+OhlB5aPRSvf9vH1pLPhT1Zg9CYXi31mOB7Pqag9axI1EEwrXGOdkclu2bOG3335j+fLl+Pn58cgjj1Doa4CoaNB5gcEINgUf75JBSCfL5NtLBoPgwhyyvAMIN6eDOY9R745gzGczaNqwIT/9uISdO7Y7yob56SlAptApN9LVXrBcev1DeVyr6J6FBzIqLFOWUCit0RSbyIr56Wim42V3Zt6+dNacyObLvvEuO/Gl5loZtOyES9kdTt+ZZMpHAG+sPEWYnxdfP1Tfca50VNfYjWfZfV6diS9+vKGLUBBCOAbO4gGvdNdWZNMvvt8nv6tJJec/1oCZu1JcTB1Z+Xa+21+SdM1iVbDaBY98f6SkTZkFjPEQDbbqeIkmeiozn2E/n2Jq7ziM3jLncwppHGHg083n2X42l4QwV9PXxVwrSw+ZWHMiGy9ZcqzJeW+tOqmYeG8su897Xgvx5a4STVCSYM6eNJfz03ZcZPWJLAJ9dBxOt5B4Po/VJ7I5kGJm0eMNMVlsZFlsZfo2dp3LZcyGs3x4VwyNIw0U2BSPvxFnivvOZFZ3blxfxvqYI+kW4kN9yxROvxzLRJYketa/sgldZdCEwlXG39+f3Nxcj+dycnIICgrCz8+P48ePk5iYCIDkZwBJQgqPQvIzgiyDrwHy3WfSYQXZhBWU/KDMeWbCwiPxyTOxavkSQqPU7Icd27Tlx5lT6P3S61gK7ciFFjrdfhsjhr3CPY8+Q1BwCJeys65YWyhm+s7yIyYebRbmeGF+OnptFqJVhrKc1BVRnsM8JdfK3gt5jFqXTJRRT+1A7zIHqu1nc9h06hK/n85xmBAySkXOlE5s6PxdD//vCE+1KDHRHE6zEFBKyziTXUCowYvivbis9vKFQr7N9fzprAI327cs4SKMvtyZwpud3DNsVuRSXn0iGwH8cDDDcY+F/Ro4hPCxDNf0Ma+uOOn4nFtop/QYWdp/URZlTfyPZeRTrNSnm60OU86yQya+TlSdvnMequ/QwtRrLOy7YMZaZHJ8Z80Z7ooPJibYVZPMtymcv1RIXKgvZqudw2mWkmSaOYXM3JVSZrj2W6vUHRy/fSSBCZvP0yjcj37Nw/lmTyo7zuWSXKSxtahh4FptNKcJhatMaGgo7dq1o1u3bvj6+rpsl9e1a1fmzp1Lly5diI+Pd8sa60CSkGpEIy5WvBnPGy88z0v9HyYyKIAGLVqTa7ZQUypg7GsvM+KjCXzX5x6EJPPRG0Po0KwJr77yCq89+wSyTkfzW5oxesx4smwSvl4yBm8ZUxnmhuZRBvanXL65p2f9YIdQKP3i/93x18sVLtIbW2SySMm1uvgiSjNjR4pDCDgrBN/uTSM60Js744IqjNiau69kxmuxKfiWMlUWR/6sHRzBrnO5/FyGEBZC8MWOFLcFjGke1qdISPg43ed0dgHf7U9zK5dZQWjooTT1t+MsdCprPpy5K7VS5Tzx/R9lz+CLLZ9pTn6zYoEAqlktxM8Lq12gk2H4L6cRwF1FPixFuGpDxfx4yMS8/el82DOGJQdNDsHXLS6I9UnZLgKhQZgvRz28F/9ac4bTWQXsuZBHn8YhLD5ocjl/MNVC09gKm39FXLf9FK4V12s/hRuBKCyA8+pqTaLrQtpFKCzbBJOjN5DpHUidvBSkMuZuws+fJK8QAOK98iGzyDRQsw54+zicj8VE5l4kNyQai01h1LpkhxnmpXZRGPQyn25xdZLFhfiQVOR8rmHUM6NvPOlmKwOXuJpUOsYEsPmM53j8+xsE81OpmVSdIG+G3laT4UUzqWLCDV6kexBk8aG+bm1x5uO76zpmZWWREOZ73QTZsv6NHGaJyvDWHbVYn5TNznPumkn7usFsP+36/H54vCGypGonH24697cQ0E0j/TiQWnkNrkUNA/vKECRPtqnNt7vLf3avtK9xWbmuPuhRh5oB3gxccoJucYGsS6p4HRFAyxoG9l40M/S2GkzZftExCfiidxxjN57l7KWSrAVf9I5j+KpT5JYzIXi8eTj/2++6b0KzKAMz+rW+ebfj1LgyJG8fpNgE9Z/eG8IiS04ajKD3LvksyQQE+BOTd7FMgQAgWfIIz88iMj+zRCAAXEiG0+6rQ2VTGrUCvYkP9eXbRxII9lVNFnfH+NI5qkTRnJK3lm8ers/Yu2JY/HhD7k0IZtSddQAIN+hZ8FgDIgxq+SijnrfuiGZZ/0Z8/WA8Xz8Yz7OtI2hd05/ZD9VnUNsSd+m/u6oO2lujjTQI92NZ/0bck1Bi8nI2ocx9uD6vFWVPrRvsHmJ5r9N1PjqJXg1DynxOxWWKqX0FK9b9vSv/es1JTHURCC/fGuWWrt2ZBX9kcNJD5BfgJhAAhq86zeh1yQxccuK6CoQHGrs7XYu5HIEAOPrWE0+3c4/9d+ahJqF0iwvi7vrBtKjhedL4TKkIql+PZTsmM5UVCKBmQAZYdjjTRSusGaB3y3wQZvAiLqT8dRmlBUKTCD/+TDGzZP+1iVrShMJNwjvvvEPPXr3pOfAleg4aTM/+TzP/t61Qpx5SZE2IiUMKDIbgopdQkiCihsfvCrLmEmD1bPsOLMwlqDCX8PwsovJNYErD/sZTiDx1Vv+fe2IZ3bkmYshjKNM/5IHGoYQWZFNr5yqCfL0w6HV4yRIvtgim5vnDiBRVk/Pxkvnqwfos69+IL/uWbEcaZtATZtDzQOMw3msfTIg1xyWuvXUtf97oUJMnnOzpxbptu2gjwUXhkQlhvgT6etEtLohl/RvR3cM6ifudhICXTqJPo7KFQq0Ab55qWSKEi9cb1AqoWDg0KHKaGirIH+XMkkMl5oGnWkZwT0KIi9mmmAn3xAJqVJQnDQk829FPmPLdVoG3i6540dpfpU5Qxc/rnS6VC1ctL1LL4F3+gs1nWkXiJUsMbl+DV9t7Fi59G4fyTKuS39mm05UXBM6mvOKIL+fAirpBar604olRMT5eMsPviHY42l9t7/mddUYu6uAgv2uz/4kmFG4Sxo0bx+rVq1m9dq36/+rV9OvXD0mn/sgcA2lQqKpBhEUh+QeoZqHS6Mt+USMKsggvyCLImovRWjSI5GSjDOuPsnIRoSP602LLQvX4oX08k/QTX20dC4AycwLKykWIvdtQXn0UZcK7KO++VOk2Km8OQPnnMwD0iA/i7c7RSEKhS70gl2iM4hn4vQnB9G8RTpCvjrc7uw4sTSP90MsSj7as5XRdycBh9NY5Vuf2iFcFycR7Yx3nv+gTR6OIksVpDzUJA+CTu+sSUqQtPehhFrywXwP6Fh1/vLm7JzCoEpsDPdJUvdcwp5lx8xoGPr03tlKDbGVTDjlrZMXUDfZhwWMNmHx/Pf73aALvdy/5/VzJOryaResm6pZaHFdslwcIdXLm9qwfxIBW7mseQE1GObV3yUKvpU+UrL3wtEDuk7vrevyeSKOe59uUCPwWNQz0ahiCTpbwuwxB7lo3yaW/SuNV9PCct83t30L9fQT6qBmXQV1pv7BfA5oXaTMta5YI7q8eiGfJEw0dKeRDrpFQ0BzNVQxJklT/Q/HfPr4Qm4DIt0CR41qKros4VX7aa0+IH75R/1/zY8mxVUtKPu/YCDvcI1GEKR3x+6+IfTvgTBLy5Png7YP47Vek+EZItWPVgjbV0WmfNIpXBwyF9LMoL45A6tsfqVMPpGB1sHwsOIfYtiG0ruWPJEl883CC6/1sVrh4joX9GhAREYFBsqLXSRiLhEnDcD+HhjGtd5xjxznv5OOU9Up0jg2kc9Fiso/vjiW7wEZCmB93xAbyxspTeMmwqF9DJEmiU91A2tcOQK+T6B4fzP6LecQG+2Dw1nEkzcI7a85U6nm3jTbSISaALIuN0XfWQSe7p2apF+LDs60jHaGaf5UJ99RFr5MdmzCFOQ1i3z3agMecEhMW80LbKL7clcKd9QKJD/V1pGwAiA/zJdJfz3NtIl3WqNxVP9gRn6+TJGoHenP2UiEvtqvBskMmt3sUUzvQh2m947BYFSRJ4uO76zoc5JPvr0egr443V54izWwjwEfH1F71sHmQkt3jg/hqdyqyBO93j3Ecr0iePt8mktgQH1YcyWRbci4v3xrFFztSQIhyzX0Nw1VNoEOdAObtS2d0tzq0chrwn2sdSbCvjta1jHjJEmO6x/Dr8SxurxPgSFFfLDjCirSNAB8voPzklVeCJhSqCZKvn/qD9y6asUXXBVkHyUmuBf0D1H+pqtlHuqUNfP/X7q2MeM717yEle3MLQP7sf0gGJ1PGgT0ow58tKbNsHmLZPADkIf9GP3kMnQDenQh149WB0pyH5G9Uyy+cjVi3AvmjWRARwYNFs3yAyb3qEeWvR/lpAVKdekQ3b6deYy1EP/MjuP1fRX9bUUY8R43bRhJbw9UUFWnUE2lUX9D4UF8WP97QLZ2Dc3qL5jVK2ubldPyRpmEsclpj0TDcF30p28+IO1w1IEmSeO32miRnF5BTYOfpVpEE+ujoFhfEulKx716yhE0R/LNjLWoHeruFcUYZvR0Dekm9XWfKoU7mDmcTybL+jeg77zAAIX6q9pNbaOf2mADSzTaWFg3svl4yMx+Ix5mPetZ12fo1yFfHxPtisStqnYu1wiG31WDJQRNnLxUy7x8lgt95MWHDcD8aFqUbcQgyg540sw2DXi5zIyw/L5lmUQY3E2JFGYB7Fy1Miw325f4G+cSG+PLFjhTVdFqkicYG+3CqyHT0n3vqYrMLR0r72kE+LH2ioZtmE+Gv58V2rqaj4rUIcx6q75KccfCtNWgbbaR+hD/p6Veesr8sNKFQjfCKicdWNNOUikxIok6caqSXgCwThIQjybLjuFToeX8H6dnXELM/uyr1Ul57HOmBJytXdvKYks8fvI488mOUT94Gux3p4WegsBBx9AAAYuMv2PwN2GdMQO43CCmyJjFBPgghUJZ+iwB0M4u0nv07CSlQbci+XjJcyoScbKatfhv5i8Xl1kkdxMqeJYp8M5w7gxTfyOEPiSzIoqnOh0VO5T7qWbdSeYK6xQWpkWl6b0f5126vycNNQ5m1K5XEopj7iffFYrMLj4uw/tlRNavd3zCEHvFBLqnJnSk2sdUv+o53OkdzuGgg+mfHWoT5eTl2Iwzy9SLcoOfZ1pEsPWRycdQDvNWpFil5VhpF+Lmkbg8zFAmIIsvafQ1CCPTR0aVeILfWDiDDbHXLUVUeb3eO5s9Uc7k7I0qSxNgeMW7HQw2VGxIDfHQOYT+obSTtoo2EGfS80zmaxpEGhv9yikYRfiR42N/kcre8dV4rAWqm5Ypyi/0VtJDUasSVtNlsNuP76w+InxYg9X4csfx/SG06It33D5Qxw6BRc+SnX0VsWgWXshBb1iIPHYXIyUbMnlTm98pvjkNs34D47de/2qzLwxgAdjtYVH+J/OUyxNb1jromh9fDe8SHRA0v0WakW7sgD/onorAA8f1MpLv6Ik4eRWrXGfIt4GcAnc7xsotTx8CcBwlNkfR67J+Nhj8TkSfP57RF4rWfT9H14i5ei8giuc8ghv50ktY1/RnVzYP/xwMi34Iy5DGkXv2Q+z7hcm7RnxmO9QzfdjYSUKckKqd4Zu8pIdziAxk0ifSjcYQBkXwSAoORgtRZdLrZSoC3zqPju5htyTk0r2FwCJG0PCs+XnKZzmEhBHP2pNExJuCqZJsNDw+/4vDM0vVKPJ/H+xvOUjfYx+Esfq9rbfQ6yUXru9H8lTaXF5KqCYUbTEJCAseOXb59/0q4UqHgp9MhNvyM1KOPurBOVgcHkbgVmrRE8i15qYXdjqTTIWw2lJcfUg8GBIGX3hECK/V5Arl3P4TFjDK039Vp3LUmKATiGsKebSXHvLyg6HlKPR9Eatke5eORJedDwpFa347Y8LMqiGQZ+Y0P2DpnHi1MR/FVrEjPDuN8045EB3q7zCCF3Q7ZJggKVZ/n2VMo389E6nIP4ks1pxXBoeg+meNW1eDQME5s2kTwZ28jPTIAvH2Rut5LhsVGXqHiMVzXGfugPhAQhO7TuRU+FnH+DKRdRGpxq+vxotQsku6vpXEX584gzpxAvt09IaQzV0soFHMhp5Bwg55jyekcOp/Fwx0SKrxGpF0EUzpSw2ZXrR7loQmFMihPKGzatIm0NPfVl3Dl+ylERETQuXPny69oGdwMQqEyQrYi7FPHwt7tSM8MQe50l+O4uJQJJ4+j/L4a9m5DenQgYsEsAOSx00Gnh7MnUaZ88Jfr8LclJh6pTj3kAUOxf/4+kp8BsWOTaxmDEczu6VOk2++EhreoAjckAqltJ4LN2Zjeet61YEITOHZQddrf3g3SL6KsXobc9V5IaAZ6LyRZh0i7iPLOC0CJaU0oCkgS4pcfICgYuUN3tdz3M2H/TrUe/V9WgwG89IikIyjjh6v39fZGfu9zpKha6vuWmY4U6jm6yBP2QX1c6lIWIbLAZBNgs6IMfRzpqcHIHXuUWV4oCiSfRKobX2YZAPu/XoTUC8hfLqvQ7GN/6UGw2yus69XiWgkFzadwlRk3bhy1atViwIABAEyYMAGdTseWLVvIzs7GZrPx1ltvcffdd1f4XXl5eTz77LMer1u4cCEzZswAoHHjxkyePJm0tDRGjhzJ6dPqSt3x48fTrl27a9PQy0R+eABKegpS4xYux6XAEGjRDl2LdgiLGcnPgCLLiE2rkCLVH64IDQdjANJt3SA8CimqJsry7yFJTcomD/s/lEmjoPXtRZFVx5GffhVl+AAkXwPSkH+jfDEO+ZmhiNxLiP9OVtdzZJUd5eJW/9dGo3w2+qo9DxfOnECcOYH91DE4d9pzBIwHgQAgtq6HretL/p7zGR5bdeyget7JaQ+g7Nvh+CwPH4cy/aOSc/O/QvrHsygvPojUvgti+0YA7HOnOSLFHPed94W6+LHdHSgT3ys5UViohiW37qDuDrV3O9K9j6jfZUqD1rcjP/4CYsHXUL8xUpd71bb6GsAprbny4/8gPQVpwBDE5rVIDZohFeX5En8mkv7ZaKTn/4kU3wjsNsScz1EO7EF+YTj2Ec9BeA10w8eV1HfHJsSsT9Vrbu1c9oCfWrRAzGIGg7vpSPl6EuLQXuRnh6naIEWZCLy8ELs2I7XtiORh0ytRkA+FhUgBgYizJ8EYhBRc9kI/sXc7KHak1h3KLHO1qNKaQnlcK/PRn3/+yahRo1i8WHVOdu3alXnz5hEYGEhAQAAmk4nevXvz+++/I0lSuZqCzWbDYrG4XXf06FEGDhzIjz/+SGhoKJmZmYSEhPDSSy/Rpk0bBg0ahN1uJy8vj8DAkpz8N1JTuBaIvBzVlu9rUM0VTqYtAHH6BKH14shUnMwyQiA2rERq1d4lwglA6vsEYtl3bveRJ3wDAUHqYOqlL4rQMsL5M+rLek4VwlLX+5CatkLZ+Is6QOb8vVIiXxFR0ZByGdt/+vhBwVWKiPHxhQLX1dfSbV0R2zY4/pbfHIfy80I4uEc1bwqBWLu85Pzw8WogAiC/MQaRZUK+/U6UX5ciFn5d8r0DXlPNoF5eap/arEh33O24lkbNkWrUVk2BMXHIDw+A+EYorz7qVm35/6aomlK+Bal3P6T4xoiDe1VhZzAijvyhpqu5kIxu5o8ObYjoupCegvzpXCRvH1UbWzRb1b6KNEd50ndq6HdmBhGDXicjo+IMwZ6otuaj8riWPoUuXbowf/58MjIyeOedd1i0aBGjR49m+/btSJJEUlISW7duJTIyslyhYLVaPV63YsUKUlNTGTlypEv5W265hV27duHj49lmXNWEQmUoT8VWfvsVqX5jlPdeAS8vdF/8gPhzNyQ0haQjKJ/+G6Bc04EQAuWFvoC7iUMcP4jykWsfyTNJ7HawAAAQYUlEQVSWgKIgflnsIoDkoe+hfP7+5TWufmOkhKaIlUUxTIHBcKnsjZOqPLViSnKFlUer21x9Q1cZqecDiF+XVq6wt49bPjPpnodBKC5rgDwR9vk8svzc97GoDJr56DrTq1cvfvrpJ1JTU+nTpw8//PADGRkZrFy5Er1eT/v27SkoqHhvgSu9TqNyyHf0VP8f96U6KwWkZm3Uk41bIE9ZCEIp15YsSRLy2BkeExVK9Zsgj5kGFgtE1YTMDNWUIOvg/scQZ0/B7i3IIz5Uy344SzVrmfPA1w+xd1uJUxmgWWukNh3h5DHwNyI/9DQA9mMH4Pgh5IGvq1pK4lakNh3VKKLU80j3PIz4pSSsVp62GBQFZeYnqv/CaebtQiktQepyD1Kr21VTHUBMPJw5gdTnCaRGzREH96hmkIgolPlfIT/6PMqUMZ6/u/g7H35GdVKnp6L8vACOHyq3PACNW8Chfe7HKyMQwE0gyK/8C2XqWM9ldV7qIL9ykefzHhAbVla6rKffjXNflYft7ClIuKXy96okmlC4BvTp04fhw4djMplYvHgxy5cvJzw8HL1ez+bNmzl7tnKZMHNycjxe17FjRwYOHMgLL7zgYj7q1KkT33zzTZnmIw3PSGXkiJLK0LjcykWWnd5AquGUqM1gLDkuSehectUipLAiB2xAUZ/VikEAUud7kJq0hFbtVaHi5KgHkJ97HbFuBTRqjq5JK0INfmTkmZGshWAtRPIPQDz0NKRdUENN9eq6AN2r7wKovoyL51S/yZK5SE1bQUYa0nPDEMu+Raz7GQosSD0fQIqs5VhsKISAw/uh4S1IsoyU0MRRJ13RokD5ix8g+SRYclEmjgJvH+R/T0KqUSrfUc06yPUSUKaNU/0fDZohvzkWCiyq38THFzH7M6TbuyENGArmXMT+nXDuDBj8Ecv/57DpA8jvTkSZPclh2gOgZXvkex5GbN+IWP+T+syfflXVDD0gj/hQjdyKicN+cK9qEvT2hsJCdVMse1HkWf+XEPOmqxc1aw1/Jnr8Po+07gCJW8o+X68BnFTXkMiv/5/6DIu5RpYOzXx0jejevTshISEsWrQIk8nEM888g9lspnnz5iQmJvLtt99Sp06dcs1H5V23YMECpk+fjizLNGvWjEmTJpGWlsZbb73FmTNnkGWZ8ePH07Zt27/U5qpsProZEBmpEBpxWQueLrfNwpwLWSakWu6LuRxlFLtHh2ml7yEEnDmhRlpV0BaRlQF+/mqKFufjBQVlCmq/jT+T++10aHUbUpuOyO27qPdMOYdY8yNS+64QWx9J742w2RA7NqkOZi91XqxsW48Um6D6DS6eU6OknIIilG3rEbMmIo/7Uo3OCg1HanALxNZH7t4bZftGpBa3IuZ8jti9WTXtNWgGNhvi1yVI/V5Aqh2L2PCzqsFJEvKbY5GCQhC5l1C+nYYUHqVqbdmZSO27IHXvjVSvAeLcGZSlc5GffxNOH1c/v/pvImLqaiGpnvi7CoW/I5pQqB5U1zanpaVd9mrhK0H5dSlSs9YehagwpSHW/6yG/hYJHGGzInlVLnmdyMuB1ItI9SpeF6GFpGpoaGiUw/UQCAByzwfKrkNohJpuxflYJQUCoGY2rndlzuOrxXUTCnv37mX27NkoikL37t154AHXB2u1WpkyZQpJSUkEBAQwbNgwIiMjy/i2qsWhQ4cYOnSoyzEfHx9WrFhxg2qkoaFRXbkuQkFRFGbNmsW7775LWFgYb7/9Nm3btqV27RIn3Lp16/D392fy5Mls3ryZefPm8frrr1/2vW5Ga1jjxo1ZvXr1ja6GR27G56mhoXHlXJdNdo4fP06NGjWIiorCy8uLDh06sHPnTpcyu3btomvXrgDcdttt/Pnnn1c0IMmyXO18BdcKm82GLGv7MGloVCeui6ZgMpkICyvJaR8WFuYWceNcRqfTYTAYyMnJcQupXLNmDWvWrAHgww8/JDzcdXcrIQQmk6lCwaAoSrWbBV9um/V6PVFRUdfNVnst8PLycvuNVHW0NlcPrlWbbzpHc48ePejRoyTRVVned10F2Rmra4TGZYUqCnHFy+j/Lmj9XD3Q2nx5lBd9dF1sA6GhoS6DS0ZGBqGhoWWWsdvtmM1mAgJurBdeQ0NDo7pxXYRCfHw8Fy5cIDU1FZvNxpYtW1wWVQG0adOGDRs2ALBt2zaaNm16U5stNDQ0NG5Grov5SKfT8dxzzzF27FgUReHOO++kTp06zJ8/n/j4eNq2bUu3bt2YMmUKQ4YMwWg0MmzYsOtRNQ0NDQ0NJ276Fc0aGhoaGlePahtvWDrtdHVAa3P1QGtz9eBatbnaCgUNDQ0NDXc0oaChoaGh4UA3evTo0Te6EjeKuLi4G12F647W5uqB1ubqwbVos+Zo1tDQ0NBwoJmPNDQ0NDQcaEJBQ0NDQ8PBTZf76GpQ0d4ONyvp6elMnTqVrKwsJEmiR48e3HfffeTm5jJx4kTS0tKIiIjg9ddfx2g0IoRg9uzZ7NmzBx8fHwYPHnxT2mUVRWHkyJGEhoYycuRIUlNTmTRpEjk5OcTFxTFkyBC8vLyqzJ4deXl5TJ8+neTkZCRJ4uWXX6ZWrVpVuo9XrFjBunXrkCSJOnXqMHjwYLKysqpcP0+bNo3ExESCgoKYMGECwBW9vxs2bOCHH34A4KGHHnJkoK4Uoppht9vFq6++Ki5evCisVqt48803RXJy8o2u1lXBZDKJEydOCCGEMJvNYujQoSI5OVnMnTtXLFmyRAghxJIlS8TcuXOFEELs3r1bjB07ViiKIo4cOSLefvvtG1b3v8Ly5cvFpEmTxPjx44UQQkyYMEH8/vvvQgghZsyYIVatWiWEEOKXX34RM2bMEEII8fvvv4tPP/30xlT4LzJ58mSxZs0aIYQQVqtV5ObmVuk+zsjIEIMHDxYFBQVCCLV/169fXyX7+cCBA+LEiRPijTfecBy73L7NyckRr7zyisjJyXH5XFmqnfmoMns73KyEhIQ4Zgp+fn5ER0djMpnYuXMnXbp0AaBLly6O9u7atYvOnTsjSRINGjQgLy+PzMzMG1b/KyEjI4PExES6d+8OqJldDxw4wG233QZA165dXdp7NfbsuJGYzWYOHTpEt27dADV9sr+/f5XuY1C1wcLCQux2O4WFhQQHB1fJfm7SpAlGo9Hl2OX27d69e2nevDlGoxGj0Ujz5s3Zu3dvpetQ7cxHldnboSqQmprKyZMnqV+/PtnZ2YSEhAAQHBxMdnY2oD4L53zsYWFhmEwmR9mbgTlz5vDkk09isVgAyMnJwWAwOFKnh4aGYjKZgMrv2fF3JjU1lcDAQKZNm8bp06eJi4tjwIABVbqPQ0ND6d27Ny+//DLe3t60aNGCuLi4Kt3Pzlxu35Ye45yfTWWodppCdSA/P58JEyYwYMAADAaDyzlJkqpM9tndu3cTFBR0U9rIrxS73c7Jkyfp2bMnH3/8MT4+PixdutSlTFXqY1Bt6jt37mTq1KnMmDGD/Pz8y5r5ViWuR99WO02hMns73MzYbDYmTJjAHXfcQfv27QEICgoiMzOTkJAQMjMzHTOm0NBQl006brZnceTIEXbt2sWePXsoLCzEYrEwZ84czGYzdrsdnU6HyWRytKm478PCwm7aPTvCwsIICwsjISEBUM0jS5curbJ9DPDHH38QGRnpaFP79u05cuRIle5nZy63b0NDQzl48KDjuMlkokmTJpW+X7XTFCqzt8PNihCC6dOnEx0dTa9evRzH27Zty8aNGwHYuHEj7dq1cxzftGkTQgiOHj2KwWC4qcwKTzzxBNOnT2fq1KkMGzaMZs2aMXToUJo2bcq2bdsANQqjuH+rwp4dwcHBhIWFcf78eUAdMGvXrl1l+xjUHcaOHTtGQUEBQghHm6tyPztzuX3bsmVL9u3bR25uLrm5uezbt4+WLVtW+n7VckVzYmIi//3vfx17Ozz00EM3ukpXhcOHD/Pee+8RExPjeAkef/xxEhISmDhxIunp6W4hbbNmzWLfvn14e3szePBg4uPjb3ArrowDBw6wfPlyRo4cSUpKCpMmTSI3N5d69eoxZMgQ9Ho9hYWFTJkyhZMnTzr27IiKirrRVb9sTp06xfTp07HZbERGRjJ48GCEEFW6jxcsWMCWLVvQ6XTExsby0ksvYTKZqlw/T5o0iYMHD5KTk0NQUBCPPvoo7dq1u+y+XbduHUuWLAHUkNQ777yz0nWolkJBQ0NDQ8Mz1c58pKGhoaFRNppQ0NDQ0NBwoAkFDQ0NDQ0HmlDQ0NDQ0HCgCQUNDQ0NDQeaUNDQuE48+uijXLx48UZXQ0OjXKrdimYNDYBXXnmFrKwsZLlkXtS1a1cGDhx4A2vlmVWrVpGRkcETTzzBqFGjeO6556hbt+6NrpZGFUUTChrVlhEjRtC8efMbXY0KSUpKonXr1iiKwrlz56hdu/aNrpJGFUYTChoapdiwYQNr164lNjaWTZs2ERISwsCBA7nlllsANZfMzJkzOXz4MEajkb59+9KjRw9ATfG8dOlS1q9fT3Z2NjVr1mT48OGObJb79+9n3LhxXLp0iU6dOjFw4MAKUzAkJSXxyCOPcP78eSIiIhyZQTU0rgWaUNDQ8MCxY8do3749s2bNYseOHfznP/9h6tSpGI1GPvvsM+rUqcOMGTM4f/48Y8aMoUaNGjRr1owVK1awefNm3n77bWrWrMnp06fx8fFxfG9iYiLjx4/HYrEwYsQI2rZt6zEvjdVqZdCgQQghyM/PZ/jw4dhsNhRFYcCAAfTp06fKpGfR+HuhCQWNassnn3ziMut+8sknHTP+oKAg7r//fiRJokOHDixfvpzExESaNGnC4cOHGTlyJN7e3sTGxtK9e3c2btxIs2bNWLt2LU8++SS1atUCIDY21uWeDzzwAP7+/vj7+9O0aVNOnTrlUSjo9XrmzJnD2rVrSU5OZsCAAXzwwQf069eP+vXrX7uHolHt0YSCRrVl+PDhZfoUQkNDXcw6ERERmEwmMjMzMRqN+Pn5Oc6Fh4dz4sQJQE1fXF7yteDgYMdnHx8f8vPzPZabNGkSe/fupaCgAL1ez/r168nPz+f48ePUrFmT8ePHX1ZbNTQqiyYUNDQ8YDKZEEI4BEN6ejpt27YlJCSE3NxcLBaLQzCkp6c7cvmHhYWRkpJCTEzMX7r/sGHDUBSFF154gS+//JLdu3ezdetWhg4d+tcapqFRAdo6BQ0ND2RnZ7Ny5UpsNhtbt27l3LlztGrVivDwcBo2bMh3331HYWEhp0+fZv369dxxxx0AdO/enfnz53PhwgWEEJw+fZqcnJwrqsO5c+eIiopClmVOnjx5U6a81rj50DQFjWrLRx995LJOoXnz5gwfPhyAhIQELly4wMCBAwkODuaNN95w7N712muvMXPmTF588UWMRiP/+Mc/HGaoXr16YbVa+eCDD8jJySE6Opo333zziuqXlJREvXr1HJ/79u37V5qroVEptP0UNDRKURySOmbMmBtdFQ2N645mPtLQ0NDQcKAJBQ0NDQ0NB5r5SENDQ0PDgaYpaGhoaGg40ISChoaGhoYDTShoaGhoaDjQhIKGhoaGhgNNKGhoaGhoOPh/E7yYJQBVSbYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJfy3P2sTvEH"
      },
      "source": [
        "# Main Brain Tumour Detection Code from MRI scan\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHgYGBk1ViYy",
        "outputId": "3ebb2166-aaba-4704-bbce-8ae0c70603e7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "model_path = r\"/content/drive/My Drive/brain_tumor_dataset/tumor_detector.model\"\n",
        "tumorNet = load_model(model_path)\n",
        "\n",
        "x = files.upload()\n",
        "path = ''\n",
        "for key, val in x.items():\n",
        "  path=key\n",
        "\n",
        "img = load_img(path, target_size=(224, 224))\n",
        "img = img_to_array(img)\n",
        "img = preprocess_input(img)\n",
        "I = [img]\n",
        "I = np.array(I, dtype=\"float32\")\n",
        "[[no,yes]] = tumorNet.predict(I, batch_size = 20)\n",
        "if yes>no:\n",
        "  print(\"Tumor detected, with probabilty of \" + str(yes*100) + \"%\")\n",
        "else:\n",
        "  print(\"No tumor detected, with probabilty of \" + str(no*100) + \"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40b07a21-4f7c-4bde-adab-f4b290659a2e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40b07a21-4f7c-4bde-adab-f4b290659a2e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving pred14.jpg to pred14.jpg\n",
            "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd180968b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Tumor detected, with probabilty of 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}